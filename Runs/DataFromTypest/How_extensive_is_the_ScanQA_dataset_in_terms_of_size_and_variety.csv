Title,Link,Abstract
(1) ScanQA: 3D Question Answering for Spatial Scene Understanding,https://typeset.io/papers/scanqa-3d-question-answering-for-spatial-scene-understanding-zp7mo391,"Abstract: We propose a new 3D spatial understanding task for 3D question answering (3D-QA). In the 3D-QA task, models receive visual information from the entire 3D scene of a rich RGB-D indoor scan and answer given textual questions about the 3D scene. Unlike the 2D-question answering of visual question answering, the conventional 2D-QA models suffer from problems with spatial understanding of object alignment and directions and fail in object localization from the textual questions in 3D-QA. We propose a baseline model for 3D-QA, called the ScanQA <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> https://github.com/ATR-DBI/ScanQA, which learns a fused descriptor from 3D object proposals and encoded sentence embeddings. This learned descriptor correlates language expressions with the underlying geometric features of the 3D scan and facilitates the regression of 3D bounding boxes to determine the described objects in textual questions. We collected human-edited question-answer pairs with free-form answers grounded in 3D objects in each 3D scene. Our new ScanQA dataset contains over 41k question-answer pairs from 800 indoor scenes obtained from the ScanNet dataset. To the best of our knowledge, ScanQA is the first large-scale effort to perform object-grounded question answering in 3D environments."
(2) Dataset,https://typeset.io/papers/dataset-1iqfskpj6c,Abstract: Dataset for 'Quantitative assessment of visual cortex function with fMRI at 7 Tesla – test-retest variability'
(3) A Large Dataset of Object Scans,https://typeset.io/papers/a-large-dataset-of-object-scans-37fe62ontr,"Abstract: We have created a dataset of more than ten thousand 3D scans of real objects. To create the dataset, we recruited 70 operators, equipped them with consumer-grade mobile 3D scanning setups, and paid them to scan objects in their environments. The operators scanned objects of their choosing, outside the laboratory and without direct supervision by computer vision professionals. The result is a large and diverse collection of object scans: from shoes, mugs, and toys to grand pianos, construction vehicles, and large outdoor sculptures. We worked with an attorney to ensure that data acquisition did not violate privacy constraints. The acquired data was irrevocably placed in the public domain and is available freely at this http URL ."
(4) A multimodal fMRI dataset unifying naturalistic processes with a rich array of experimental tasks,https://typeset.io/papers/a-multimodal-fmri-dataset-unifying-naturalistic-processes-2itvoopsqg,"Abstract: Cognitive neuroscience has advanced significantly due to the availability of openly shared datasets. Large sample sizes, large amounts of data per person, and diversity in tasks and data types are all desirable, but are difficult to achieve in a single dataset. Here, we present an open dataset with N = 101 participants and 6 hours of scanning per participant, with 6 multifaceted cognitive tasks including 2 hours of naturalistic movie viewing. This datasets' combination of ample sample size, extensive data per participant, more than 600 hours worth of data, and a wide range of experimental conditions - including cognitive, affective, social, and somatic/interoceptive tasks - positions it uniquely for probing important questions in cognitive neuroscience."
(5) dataset,https://typeset.io/papers/dataset-38m63rini0,"Abstract: We release a Question Answering dataset containing +5000 questions specifically taking spatial context information into account, i.e. visual features of a surrounding target object, or the user's location and moving direction -- SpatialQuestions. The data was collected in a big-scale user study with Amazon's Mechanical Turk platform involving over 400 crowdworkers and annotated semi-automatically in the post-processing. <br>"
(6) Analysis of Data for SCAN Project,https://typeset.io/papers/analysis-of-data-for-scan-project-ahph5ia5on,"Abstract: According to the SCAN Project’s grant agreement, the Consortium must carry out 500 questionnaires and 100 semi-structured, qualitative interviews. This paper proposes the methodology enacted to formulate and present the indicators regarding the state of knowledge of the users on the project topics."
(7) Efqa,https://typeset.io/papers/efqa-2t7s9lsd6f,"Abstract: we consider the FigureQA dataset, extend the variation of the figures and question types in the dataset and explore how the complexity of figures and increasing the variation of questions will impact the results of a conventional FQA system. Accordingly, we extend the dataset to the five-figure variants by adding the Gaussian distribution and adding eight question types to the twenty-three dataset variants"
(8) QScored: A Large Dataset of Code Smells and Quality Metrics,https://typeset.io/papers/qscored-a-large-dataset-of-code-smells-and-quality-metrics-3gzthy0wc3,"Abstract: The dataset contains code quality information of more than 86 thousand GitHub repositories containing more than 1.1 billion lines of code mainly written in C# and Java. The code quality information contains detected 7 kinds of architecture smells, 19 kinds of design smells, and 11 kinds of implementation smells, and 27 commonly used code quality metrics computed at project, package, class, and method levels."
(9) A Mighty Dataset for Stress-Testing Question Answering Systems,https://typeset.io/papers/a-mighty-dataset-for-stress-testing-question-answering-55rzfu65gx,"Abstract: The general goal of semantic question answering systems is to provide correct answers to natural language queries, given a number of structured datasets. The increasing broad deployment of question answering (QA) systems in everyday life requires a comparable and reliable rating of how well QA systems perform and how scalable they are. In order to achieve this, we developed a massive dataset of more than 2 million natural language questions and their SPARQL queries for the DBpedia dataset. We combined natural language processing and linked open data to automatically generate this large amount of valid question-query pairs. Our aim is to assist the benchmarking or scoring of QA systems in terms of answering questions in a range of languages, retrieving answers from heterogeneous sources or answering massive amounts of questions within a limited time. This dataset represents an ideal choice for stress-testing systems’ scalability, speed and correctness. As such it has already been included into the Large-scale QA task of the Question Answering Over Linked Data (QALD) Challenge and the HOBBIT project Question Answering Benchmark."
(10) emrQA: A Large Corpus for Question Answering on Electronic Medical Records,https://typeset.io/papers/emrqa-a-large-corpus-for-question-answering-on-electronic-13wluvvis0,"Abstract: We propose a novel methodology to generate domain-specific large-scale question answering (QA) datasets by re-purposing existing annotations for other NLP tasks. We demonstrate an instance of this methodology in generating a large-scale QA dataset for electronic medical records by leveraging existing expert annotations on clinical notes for various NLP tasks from the community shared i2b2 datasets. The resulting corpus (emrQA) has 1 million question-logical form and 400,000+ question-answer evidence pairs. We characterize the dataset and explore its learning potential by training baseline models for question to logical form and question to answer mapping."
