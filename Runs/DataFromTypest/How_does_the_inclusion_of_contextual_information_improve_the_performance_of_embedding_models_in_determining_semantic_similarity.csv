Title,Link,Abstract
(1) A comparative analysis of embedding models for patent similarity,https://typeset.io/papers/a-comparative-analysis-of-embedding-models-for-patent-5fo5v5jjrn,"Abstract: This paper makes two contributions to the field of text-based patent similarity. First, it compares the performance of different kinds of patent-specific pretrained embedding models, namely static word embeddings (such as word2vec and doc2vec models) and contextual word embeddings (such as transformers based models), on the task of patent similarity calculation. Second, it compares specifically the performance of Sentence Transformers (SBERT) architectures with different training phases on the patent similarity task. To assess the models' performance, we use information about patent interferences, a phenomenon in which two or more patent claims belonging to different patent applications are proven to be overlapping by patent examiners. Therefore, we use these interferences cases as a proxy for maximum similarity between two patents, treating them as ground-truth to evaluate the performance of the different embedding models. Our results point out that, first, Patent SBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer architecture proposed in this research, outperforms the current state-of-the-art in patent similarity. Second, they show that, in some cases, large static models performances are still comparable to contextual ones when trained on extensive data; thus, we believe that the superiority in the performance of contextual embeddings may not be related to the actual architecture but rather to the way the training phase is performed."
(2) Optimization of Natural Language Understanding with Contextual Embeddings,https://typeset.io/papers/optimization-of-natural-language-understanding-with-1wu66ddfst,"Abstract: Natural Language Understanding (NLU) has recently made considerable progress, but there is still an immediate need to improve its performance. To this end, researchers have addressed the issue by introducing contextual embedding’s, which enable the NLU model to map words to their contextual meanings rather than just looking at their individual meanings. Contextual embedding’s enable the model to capture the nuances of words in the various contexts they are used in, allowing for better understanding and performance. Two methods— feature engineering and transfer learning—have been employed to further improve performance. With feature engineering, transformed features are used to obtain improved accuracy and faster training times whereas transfer learning uses pre-trained models to reduce the computational power required for training. This approach has resulted in improved accuracy in the various language understanding tasks. Furthermore, the innovative use of contextual embedding’s in combination with various optimization methods has resulted in a much more reliable and accurate NLU model."
(3) Context Embeddings for Efficient Answer Generation in RAG,https://typeset.io/papers/context-embeddings-for-efficient-answer-generation-in-rag-22ekqtwpin,"Abstract: Retrieval-Augmented Generation (RAG) allows overcoming the limited knowledge of LLMs by extending the input with external information. As a consequence, the contextual inputs to the model become much longer which slows down decoding time directly translating to the time a user has to wait for an answer. We address this challenge by presenting COCOM, an effective context compression method, reducing long contexts to only a handful of Context Embeddings speeding up the generation time by a large margin. Our method allows for different compression rates trading off decoding time for answer quality. Compared to earlier methods, COCOM allows for handling multiple contexts more effectively, significantly reducing decoding time for long inputs. Our method demonstrates a speed-up of up to 5.69
×
while achieving higher performance compared to existing efficient context compression methods."
(4) PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic Search,https://typeset.io/papers/pic-a-phrase-in-context-dataset-for-phrase-understanding-and-106kf7j835,"Abstract: While contextualized word embeddings have been a de-facto standard, learning contextualized phrase embeddings is less explored and being hindered by the lack of a human-annotated benchmark that tests machine understanding of phrase semantics given a context sentence or paragraph (instead of phrases alone). To fill this gap, we propose PiC—a dataset of ∼28K of noun phrases accompanied by their contextual Wikipedia pages and a suite of three tasks for training and evaluating phrase embeddings. Training on PiC improves ranking-models’ accuracy and remarkably pushes span selection (SS) models (i.e., predicting the start and end index of the target phrase) near human accuracy, which is 95% Exact Match (EM) on semantic search given a query phrase and a passage. Interestingly, we find evidence that such impressive performance is because the SS models learn to better capture the common meaning of a phrase regardless of its actual context. SotA models perform poorly in distinguishing two senses of the same phrase in two contexts (∼60% EM) and in estimating the similarity between two different phrases in the same context (∼70% EM)."
(5) An Improved Sentence Embeddings based Information Retrieval Technique using Query Reformulation,https://typeset.io/papers/an-improved-sentence-embeddings-based-information-retrieval-2azcdbti,"Abstract: Information retrieval (IR) is a field of study that focuses on gathering, organizing, storing, analyzing, and accessing information. It is used in all applications where searching for information is required through the internet. Selection and ranking of relevant web documents to satisfy users’ information need in response to users’ queries is the most important task associated with IR. To better comprehend the user’s intent and create a more efficient query, the initial query has been reformulated by adding new terms. The semantic text-similarity techniques proposed by researchers in the past demand a significant amount of trained labeled data as well as user intervention; both of them are scarce, hard to capture, and difficult to maintain. In general, these techniques ignored contextual information as well as word order, which causes the problems like data sparsity and latitudinal explosion. Deep learning techniques are now being utilized to find text similarities. Various sentence embedding models exist today that can use vectors to represent complete sentences and their semantics. This helps the search engine to comprehend the context, intent, and different aspects of the user text. So, in this paper, a sentence embedding model based query reformulation (QR) has been proposed for improving document ranking performance using a universal sentence encoder (USE) and a cosine similarity measure. Four standard datasets CACM, CISI, ADI, and Medline are used to perform all the experiments. The outcomes demonstrate the superior performance of the USE-based QR system over the SBERT sentence embedding model by 4.48 %, 5.97%, and 7.2% and 2.1% for ADI, CISI, CACM, and Medline datasets respectively."
(6) Contextual word embeddings for tabular data search and integration,https://typeset.io/papers/contextual-word-embeddings-for-tabular-data-search-and-17rleoj0,"Abstract: Abstract This paper presents a new approach to retrieve and further integrate tabular datasets (collections of rows and columns) using union and join operations. In this work, both processes were carried out using a similarity measure based on contextual word embeddings, which allows finding semantically similar tables and overcome the recall problem of lexical approaches based on string similarity. This work is the first attempt to use contextual word embeddings in the whole pipeline of table search and integration, including for the first time their use in the join operation. A comprehensive analysis of their performance was carried out on both retrieving and integrating tabular datasets, comparing them with context-free models. Column headings and cell values were used as contextual information and their impact on each task was evaluated. The results revealed that contextual models significantly outperform context-free models and a traditional weighting schema in ad hoc table retrieval. In the data integration task, contextual models also improved the results on union operation compared to context-free approaches."
(7) PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic Search,https://typeset.io/papers/pic-a-phrase-in-context-dataset-for-phrase-understanding-and-am56o8zs,"Abstract: While contextualized word embeddings have been a de-facto standard, learning contextualized phrase embeddings is less explored and being hindered by the lack of a human-annotated benchmark that tests machine understanding of phrase semantics given a context sentence or paragraph (instead of phrases alone). To fill this gap, we propose PiC -- a dataset of ~28K of noun phrases accompanied by their contextual Wikipedia pages and a suite of three tasks for training and evaluating phrase embeddings. Training on PiC improves ranking models' accuracy and remarkably pushes span-selection (SS) models (i.e., predicting the start and end index of the target phrase) near-human accuracy, which is 95% Exact Match (EM) on semantic search given a query phrase and a passage. Interestingly, we find evidence that such impressive performance is because the SS models learn to better capture the common meaning of a phrase regardless of its actual context. SotA models perform poorly in distinguishing two senses of the same phrase in two contexts (~60% EM) and in estimating the similarity between two different phrases in the same context (~70% EM)."
(8) A Study on Knowledge Embedding Method for Extending Contextual Information of Words,https://typeset.io/papers/a-study-on-knowledge-embedding-method-for-extending-3rmr9zdy,Abstract: 사전 학습 언어 모델은 대용량 텍스트 데이터를 학습에 사용하여 다양한 자연어 처리 분야에서 우수한 성능을 보인다. 단어의 문맥 정보를 반영한 임베딩 기법들은 사전 학습 모델의 성능에 중요한 역할을 한다. 그렇지만 문맥에 출현한 명시적 단어들의 문맥 정보만 임베딩에 사용되고 있으므로 단어들이 가질 수 있는 다양한 관계 정보를 동시에 임베딩에 반영하기 위한 임베딩 기법이 필요하다. 본 논문에서는 지식 임베딩 기법을 통해 문장에 출현한 개체가 트리플에서 주어 개체인 경우뿐만 아니라 목적어 개체인 경우까지 포함시켜 출현 개체가 가질 수 있는 지식 정보를 확장하여 임베딩에 반영하는 확장 방법을 제안한다. 제안 모델의 성능 평가결과 기존 지식 기반 사전 학습 모델인 CoLAKE보다 우수한 성능을 보인다. 따라서 지식 임베딩을 통한 사전학습 모델의 성능 향상을 통해 응용 분야 문제 해결에 도움을 줄 것으로 기대한다.
(9) Incorporating Contextual Information in Prediction Based Word Embedding Models,https://typeset.io/papers/incorporating-contextual-information-in-prediction-based-1avt6d3q,"Abstract: A clustering-based language model is proposed for analyzing the performance of context sensitive word embedding models that uses contextual information. The construction of text readability and prediction models faces several shortcomings due to the complex nature and structure of language. The language structure is complicated all the more by words having vastly different meanings and interpretation based on the context in which it is used. This paper aims to resolve this issue by first clustering the sentences based on similarity and then performing word embedding separately on each of these clusters to obtain an enhanced outcome. This would serve to embed the same word separately in varying context as an improvement over the standard existing word embedding models provided by various prediction based models. Comparing the two approaches, our results have showed that clustering improves the performance of the model and discriminate the contextual information based on sense which leads to more accurate representation in vector form."
(10) A Comparative Analysis of Text Embedding Models for Bug Report Semantic Similarity,https://typeset.io/papers/a-comparative-analysis-of-text-embedding-models-for-bug-2p0821e9as,"Abstract: Bug reports are an essential aspect of software development, and it is crucial to identify and resolve them quickly to ensure the consistent functioning of software systems. Retrieving similar bug reports from an existing database can help reduce the time and effort required to resolve bugs. In this paper, we compared the effectiveness of semantic textual similarity methods for retrieving similar bug reports based on a similarity score. We explored several embedding models such as TF-IDF (Baseline), FastText, Gensim, BERT, and ADA. We used the Software Defects Data containing bug reports for various software projects to evaluate the performance of these models. Our experimental results showed that BERT generally outperformed the rest of the models regarding recall, followed by ADA, Gensim, FastText, and TFIDF. Our study provides insights into the effectiveness of different embedding methods for retrieving similar bug reports and highlights the impact of selecting the appropriate one for this task. Our code is available on GitHub."
