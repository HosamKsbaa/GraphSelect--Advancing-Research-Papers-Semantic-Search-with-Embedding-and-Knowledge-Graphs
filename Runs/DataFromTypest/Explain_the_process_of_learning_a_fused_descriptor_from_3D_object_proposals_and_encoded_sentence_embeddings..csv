Title,Link,Abstract
(1) 3D Object Detection Using Multiple-Frame Proposal Features Fusion,https://typeset.io/papers/3d-object-detection-using-multiple-frame-proposal-features-30hst4o435,"Abstract: Object detection is important in many applications, such as autonomous driving. While 2D images lack depth information and are sensitive to environmental conditions, 3D point clouds can provide accurate depth information and a more descriptive environment. However, sparsity is always a challenge in single-frame point cloud object detection. This paper introduces a two-stage proposal-based feature fusion method for object detection using multiple frames. The proposed method, called proposal features fusion (PFF), utilizes a cosine-similarity approach to associate proposals from multiple frames and employs an attention weighted fusion (AWF) module to merge features from these proposals. It allows for feature fusion specific to individual objects and offers lower computational complexity while achieving higher precision. The experimental results on the nuScenes dataset demonstrate the effectiveness of our approach, achieving an mAP of 46.7%, which is 1.3% higher than the state-of-the-art 3D object detection method."
(2) 3D Object Detection Using Multiple-Frame Proposal Features Fusion,https://typeset.io/papers/3d-object-detection-using-multiple-frame-proposal-features-59q0z9fwq2,"Abstract: Object detection is important in many applications, such as autonomous driving. While 2D images lack depth information and are sensitive to environmental conditions, 3D point clouds can provide accurate depth information and a more descriptive environment. However, sparsity is always a challenge in single-frame point cloud object detection. This paper introduces a two-stage proposal-based feature fusion method for object detection using multiple frames. The proposed method, called proposal features fusion (PFF), utilizes a cosine-similarity approach to associate proposals from multiple frames and employs an attention weighted fusion (AWF) module to merge features from these proposals. It allows for feature fusion specific to individual objects and offers lower computational complexity while achieving higher precision. The experimental results on the nuScenes dataset demonstrate the effectiveness of our approach, achieving an mAP of 46.7%, which is 1.3% higher than the state-of-the-art 3D object detection method."
(3) Multi-scale and multi-level shape descriptor learning via a hybrid fusion network,https://typeset.io/papers/multi-scale-and-multi-level-shape-descriptor-learning-via-a-25wq9y59,"Abstract: Discriminative and informative 3D shape descriptors are of fundamental significance to computer graphics applications, especially in the fields of geometry modeling and shape analysis. 3D shape descriptors, which reveal extrinsic/intrinsic properties of 3D shapes, have been well studied for decades and proved to be useful and effective in various analysis and synthesis tasks. Nonetheless, existing descriptors are mainly founded upon certain local differential attributes or global shape spectra, and certain combinations of both types. Conventional descriptors are typically customized for specific tasks with priori domain knowledge, which severely prevents their applications from widespread use. Recently, neural networks, benefiting from their powerful data-driven capability for general feature extraction from raw data without any domain knowledge, have achieved great success in many areas including shape analysis. In this paper, we present a novel hybrid fusion network (HFN) that learns multi-scale and multi-level shape representations via uniformly integrating a traditional region-based descriptor with modern neural networks. On one hand, we exploit the spectral graph wavelets (SGWs) to extract the shapes’ local-to-global features. On the other hand, the shapes are fed into a convolutional neural network to generate multi-level features simultaneously. Then a hierarchical fusion network learns a general and unified representation from these two different types of features which capture multi-scale and multi-level properties of the underlying shapes. Extensive experiments and comprehensive comparisons demonstrate our HFN can achieve better performance in common shape analysis tasks, such as shape retrieval and recognition, and the learned hybrid descriptor is robust, informative, and discriminative with more potential for widespread applications."
(4) Learning to fuse local geometric features for 3D rigid data matching,https://typeset.io/papers/learning-to-fuse-local-geometric-features-for-3d-rigid-data-4omob9j2nr,About: This article is published in Information Fusion. The article was published on 01 Sep 2020. and is currently open access.
(5) Self-Supervised Multi-View Learning via Auto-Encoding 3D Transformations,https://typeset.io/papers/self-supervised-multi-view-learning-via-auto-encoding-3d-1j27h4jm,"Abstract: 3D object representation learning is a fundamental challenge in computer vision to infer about the 3D world. Recent advances in deep learning have shown their efficiency in 3D object recognition, among which view-based methods have performed best so far. However, feature learning of multiple views in existing methods is mostly performed in a supervised fashion, which often requires a large amount of data labels with high costs. In contrast, self-supervised learning aims to learn multi-view feature representations without involving labeled data. To this end, we propose a novel self-supervised framework to learn Multi-View Transformation Equivariant Representations (MV-TER), exploring the equivariant transformations of a 3D object and its projected multiple views that we derive. Specifically, we perform a 3D transformation on a 3D object, and obtain multiple views before and after the transformation via projection. Then, we train a representation encoding module to capture the intrinsic 3D object representation by decoding 3D transformation parameters from the fused feature representations of multiple views before and after the transformation. Experimental results demonstrate that the proposed MV-TER significantly outperforms the state-of-the-art view-based approaches in 3D object classification and retrieval tasks, and show the generalization to real-world datasets. The code is available at: https://github.com/gyshgx868/mvter."
(6) Learning the Global Descriptor for 3-D Object Recognition Based on Multiple Views Decomposition,https://typeset.io/papers/learning-the-global-descriptor-for-3-d-object-recognition-8b8zj46a,"Abstract: The key point of view based strategies for the analysis of 3D object is to obtain a global descriptor from a collection of its rendered views on 2D images. The views are always redundantly sampled as to ensure the completeness of the information. In this paper, we bring new insight into the study of multi-view object recognition, which models an object as a View Mixture Model (VMM). We argue that each object represented by the multiple views can be decomposed into just a few latent views. Based on the VMM, we introduce a decomposition module to mine the representations of these latent views for the construction of a compact and comprehensive descriptor. After that, we further propose a view alignment module to ensure the descriptor is robust to the variation of view permutation. We evaluate our method on the ModelNet-40, ModelNet-10 and ShapeNetCore55 datasets. The experimental results show that our method can learn efficient and comprehensive representation for 3D objects, and achieves state-of-the-art performance on both the 3D object classification and retrieval tasks. Lastly, experiments are conducted for benchmarking various popular CNN backbones on the 3D object recognition task, with a view to achieving fair comparisons and promoting the future research in this area. Codes for our paper are released: &#x201C;<uri>https://github.com/hjjpku/multi_view_sort</uri> &#x201D;."
(7) Learning Disentangled Representation for Multi-View 3D Object Recognition,https://typeset.io/papers/learning-disentangled-representation-for-multi-view-3d-1jkm20gy,"Abstract: 3D object recognition is a hot research topic. Particularly, view-based methods, which represent a 3D object with a collection of its rendered views on the 2D domain, play an important role in this field. Currently, view-based researches tend to aggregate information from multiple views via pooling based strategies to endow the models with the characteristic of view permutation invariance, at the cost of inevitable loss of useful features. In this paper, we introduce a new method that learns a more comprehensive descriptor for a 3D object from its views while successfully keeping its robustness to the variation of view permutation. Our method disentangles the information in the set of multi-view images into a global category-related feature and a set of view-permutation related features. To unbind these two parts, an encode-decoder based disentangling architecture is proposed, which barely bring extra computations compared to the baseline model. Systematic experiments are conducted for this new method to demonstrates the effectiveness and the competitive performance based on ModelNet40, ModelNet10, and ShapeNetCore55 datasets. Codes for our paper will be released soon on “ <uri xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">https://github.com/hjjpku/multi_view_sort</uri> ”."
(8) Learning Discriminative and Generative Shape Embeddings for Three-Dimensional Shape Retrieval,https://typeset.io/papers/learning-discriminative-and-generative-shape-embeddings-for-58ar94nh3d,"Abstract: As an important solution for 3D shape retrieval, a multi-view shape descriptor has achieved impressive performance. One crucial part of view-based shape descriptors is to interpret 3D structures through various 2D observations. Most existing methods like MVCNN believe that a strong classification model trained with deep learning, can often provide an efficient shape embedding for 3D shape retrieval. However, these methods pay much attention to discriminative models and none of them necessarily incorporate the underlying 3D properties of the objects from 2D images. In this paper, we present a novel encoder-decoder recurrent feature aggregation network (ERFA-Net) to address this problem. Aiming at emphasizing the 3D properties of 3D shapes in the fusion of multiple view features, 3D properties prediction tasks are introduced into the 3D shape retrieval. Specifically, an image sequence of the shape is recurrently aggregated into a discriminative shape embedding based on LSTM network, and then this latent shape embedding is trained to predict the original voxel grids and estimate images of unseen viewpoints. This generation task gives an effective supervision which makes the network exploit 3D properties of shapes through various 2D images. Our method achieves the state-of-the-art performance for 3D shape retrieval, on two large-scale 3D shape datasets, ModelNet and ShapeNetCore55. Extensive experiments show that the proposed 3D representation performs robust discrimination against view occlusion, and strong generation ability for various 3D shape tasks."
(9) 3D Object Detection Based on Multi-scale Feature Fusion and Contrastive Learning,https://typeset.io/papers/3d-object-detection-based-on-multi-scale-feature-fusion-and-35qeu99h,"Abstract: 3D object detection plays an increasingly important role in the understanding of real natural scenes. In recent years, the method based on Hough voting has attracted more and more attention because of its compact model and high efficiency. However, the current voting strategy only uses poor local proposal information, which is not conducive to model optimization and performance improvement. A 3D object detection model based on multi-scale feature fusion and contrastive learning is proposed in this paper. The proposal stage focuses on voting to generate proposals, including the coordinates of proposals and the corresponding feature vectors. In order to obtain the local structure information, then calculate the multi-scale attention, and trace the multi-scale features of the proposal, an additional proposed coding contrast branch is introduced, which uses the proposed feature coding contrast loss to jointly optimize the feature representation and multi-scale attention modules. We have obtained competitive results on two large datasets SUN RGBD and ScanNet, which shows the effectiveness of our method."
(10) 3D Object Detection Based on Multi-scale Feature Fusion and Contrastive Learning,https://typeset.io/papers/3d-object-detection-based-on-multi-scale-feature-fusion-and-2gcinqcm,"Abstract: 3D object detection plays an increasingly important role in the understanding of real natural scenes. In recent years, the method based on Hough voting has attracted more and more attention because of its compact model and high efficiency. However, the current voting strategy only uses poor local proposal information, which is not conducive to model optimization and performance improvement. A 3D object detection model based on multi-scale feature fusion and contrastive learning is proposed in this paper. The proposal stage focuses on voting to generate proposals, including the coordinates of proposals and the corresponding feature vectors. In order to obtain the local structure information, then calculate the multi-scale attention, and trace the multi-scale features of the proposal, an additional proposed coding contrast branch is introduced, which uses the proposed feature coding contrast loss to jointly optimize the feature representation and multi-scale attention modules. We have obtained competitive results on two large datasets SUN RGBD and ScanNet, which shows the effectiveness of our method."
