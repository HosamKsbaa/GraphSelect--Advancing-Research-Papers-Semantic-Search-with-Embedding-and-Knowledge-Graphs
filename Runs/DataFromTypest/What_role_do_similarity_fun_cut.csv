Title,Link,Abstract
(1) Multi-Objective Optimal Threshold Selection for Similarity Functions in Siamese Networks for Semantic Textual Similarity Tasks,https://typeset.io/papers/multi-objective-optimal-threshold-selection-for-similarity-1hsoulnt78,"Abstract: This paper presents a comparative study of fundamental similarity functions for Siamese networks in semantic textual similarity (STS) tasks. We evaluate various similarity functions using the STS Benchmark dataset, analyzing their performance and stability. Additionally, we introduce a multi-objective approach for optimal threshold selection. Our findings provide insights into the effectiveness of different similarity functions and offer a straightforward method for threshold selection optimization, contributing to the advancement of Siamese network architectures in STS applications."
(2) Improving The Performance of Semantic Text Similarity Tasks on Short Text Pairs,https://typeset.io/papers/improving-the-performance-of-semantic-text-similarity-tasks-33hin276,"Abstract: Training semantic similarity model to detect duplicate text pairs is a challenging task as almost all of datasets are imbalanced, by data nature positive samples are fewer than negative samples, this issue can easily lead to model bias. Using traditional pairwise loss functions like pairwise binary cross entropy or Contrastive loss on imbalanced data may lead to model bias, however triplet loss showed improved performance compared to other loss functions. In triplet loss-based models data is fed to the model as follow: anchor sentence, positive sentence and negative sentence. The original data is permutated to follow the input structure. The default structure of training samples data is 363,861 training samples (90% of the data) distributed as 134,336 positive samples and 229,524 negative samples. The triplet structured data helped to generate much larger amount of balanced training samples 456,219. The test results showed higher accuracy and f1 scores in testing. We fine-tunned RoBERTa pre trained model using Triplet loss approach, testing showed better results. The best model scored 89.51 F1 score, and 91.45 Accuracy compared to 86.74 F1 score and 87.45 Accuracy in the second-best Contrastive loss-based BERT model."
(3) Improving The Performance of Semantic Text Similarity Tasks on Short Text Pairs,https://typeset.io/papers/improving-the-performance-of-semantic-text-similarity-tasks-1k3hqyfg,"Abstract: Training semantic similarity model to detect duplicate text pairs is a challenging task as almost all of datasets are imbalanced, by data nature positive samples are fewer than negative samples, this issue can easily lead to model bias. Using traditional pairwise loss functions like pairwise binary cross entropy or Contrastive loss on imbalanced data may lead to model bias, however triplet loss showed improved performance compared to other loss functions. In triplet loss-based models data is fed to the model as follow: anchor sentence, positive sentence and negative sentence. The original data is permutated to follow the input structure. The default structure of training samples data is 363,861 training samples (90% of the data) distributed as 134,336 positive samples and 229,524 negative samples. The triplet structured data helped to generate much larger amount of balanced training samples 456,219. The test results showed higher accuracy and f1 scores in testing. We fine-tunned RoBERTa pre trained model using Triplet loss approach, testing showed better results. The best model scored 89.51 F1 score, and 91.45 Accuracy compared to 86.74 F1 score and 87.45 Accuracy in the second-best Contrastive loss-based BERT model."
(4) Features of Semantic Similarity Assessment,https://typeset.io/papers/features-of-semantic-similarity-assessment-2igj8roz,"Abstract: Semantic similarity is a fundamental concept in computational linguistics. The models used for the representation of text have a major role in similarity computation. The text with multilingual and multimodal components shows the need for computing similarity based on different characteristics of text. This chapter studies various aspects of semantic similarity of linguistic units, cross-level similarity, semantic models, and similarity measures. One of the main motivations of this chapter is to analyze semantic similarity models such as geometric models, feature-based models, graph-based models, vector space models, and formal concept analysis models. In addition, a composite summary score based on words and hashtags is applied for the tweet summarization task which is effective when compared with other measures."
(5) Semantic Similarity Measures,https://typeset.io/papers/semantic-similarity-measures-4p62vm1maq,"Abstract: To enable artificial systems to converse and adapt to users’ needs and desires, which is a fundamental aspect of phenotropic interaction, understanding the semantics of exchanged information is vital. Semantic similarity enables the estimation of the closeness of meaning between various elements. This is key for the processing of information in a human-like manner, allowing to automatically extend knowledge to unknown concepts and perceptions. Semantic similarity measures play a key role in the computing with words pipeline’s precisiation phase and the identification of relationships between concepts in approximate reasoning. Modeling perceived commonalities between elements relies on similarity estimation, which inherently possesses a subjective component. Therefore, similarity cannot be defined crisply but requires a fuzzy nuance to account for the subjectivity and imprecision of natural language. In this chapter, a novel semantic similarity measure taking into account these aspects is defined, covering the specific case of similarity between scalar terms, that is generally not well handled by state-of-the-art methods. In a final evaluation task, the developed measure not only results more accurate than state-of-the-art methods but reaches also a performance similar to that of humans in a semantic sorting task."
(6) Comparative Evaluation of Semantic Similarity Upon Sentential Text of Varied (Generic) Lengths,https://typeset.io/papers/comparative-evaluation-of-semantic-similarity-upon-1b8f1q7r,"Abstract: AbstractSemantic similarity plays a vital role in natural language processing, information retrieval, text mining, question and answering system, text-related research, biomedical informatics, and plagiarism checking. Measuring similarity between sentences means estimating the degree of closeness in meaning between these sentences. Very recently, NLP research has observed much-inclined shift from word-lexicon and syntactic feature-based techniques to structural feature-based techniques of exploring NLP semantics. Rather, no stone is unturned by NLP community working on computation of text semantic similarity to formulate supporting feature spaces, say, word-to-word co-occurrences, lexical corpora and natural language grammar rule-based word order vectors as well as semantically parsed structures. The most popularly used WordNet lexical dictionary was exploited as hierarchical structures (semantic trees) to fetch all combinations of noun and verb families of word phrases, predominantly occupying free texts. In this paper, the computation of text semantic similarity is addressed by devising a novel set of generic similarity metrics based on both, word-sense of the phrases constituting the text as well as the grammatical layout and sequencing of these word phrases forming text with sensible meaning. The experiments performed on benchmark datasets created by Li and group explore semantically parsed dependency structures in feature extraction step and achieve very promising values of semantic closeness within sentence pairs. The model’s performance is indicated achieving highest value of Pearson’s correlation coefficient (0.89) with mean-human similarity scores against those obtained through closely competent structured approach models. However, these very promising results are contributed by word-sense similarity component of sentential similarity measure. A future scope of work is also incepted that shall also improve upon dependency (grammatical relations) component to raise the text similarity performance metric.KeywordsNatural language processingSemantic similarityStructured approachSimilarity matrixLexical dictionary"
(7) Semantic Similarity for Text Comparison between Textual Documents or Sentences,https://typeset.io/papers/semantic-similarity-for-text-comparison-between-textual-58niim2vud,"Abstract: The inclusion of semantic information into any similarity metric boosts its effectiveness and gives findings that are interpretable by humans for further inquiry. A similarity calculation approach only centered on word properties inside the text sometimes provides less precise findings. This document gives three methods that aims to focus on textual terms and incorporate semantic information into their feature vectors, thereby computing semantic similarities. These strategies are founded in both corpus and knowledge-based approaches, namely: cosine similarity using tf-idf vectors, cosine similarity employing word embeddings, and soft cosine similarity utilizing word embeddings. Among these three, cosine similarity utilizing tf-idf vectors is deemed the most proficient in spotting similarities among succinct texts. The texts found by this technology are easily comprehensible and can be easily employed in other applications for important information retrieval."
(8) An Ontology-Based Semantic Similarity Computation Model,https://typeset.io/papers/an-ontology-based-semantic-similarity-computation-model-44sn5c03m5,"Abstract: The semantic similarity computation is very important in many applications that need to process text data. Because ontology can provide explicit concept specification, we establish an ontology-based semantic similarity computation model. This model quantifies the semantic similarity between concepts considering the semantic distance, concept level and the overlapping degree between sets of the hypernyms and hyponyms. The association between the semantic distance and concept level is also presented so that the number of parameters is reduced. The final experiment results demonstrate that this model is effective, and the accuracy is improved without any dependency on corpus."
(9) New similarity functions,https://typeset.io/papers/new-similarity-functions-fxz97ndtyl,Abstract not found or could not extract.
(10) Semantic similarity on constraints datasets: A latent approach,https://typeset.io/papers/semantic-similarity-on-constraints-datasets-a-latent-1w07sbbivb,"Abstract: The technological world has grown by incorporating billions of small sensing devices, collecting and sharing large amounts of diversified data over the new generation of wireless and mobile networks. Semantic similarity models have been used as a means to organize and optimize devices in constrained environments such as IoT, edge computing, and 5G and next-generation networks. In this paper, we reviewed the commonly used semantic similarity models, discussed the limitations of our previous model, and explored latent space methods (through matrix factorization) as a way to reduce noise and correct the model profiles with no additional data. Our solution was evaluated on two datasets: Miller-Charles and IoT semantic datasets. The improved model achieved a correlation of 0.62 and 0.53 respectively (which represents an improvement of 0.21 and 0.13 for each dataset)."
