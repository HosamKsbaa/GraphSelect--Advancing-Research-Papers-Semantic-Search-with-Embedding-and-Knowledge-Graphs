Title,Link,Abstract
(1) Dataset,https://typeset.io/papers/dataset-4ebw2cp5oh,"Abstract: The dataset includes information on the use of e-mail reply suggestions from a socio-cultural perspective of some language-related variables. It also aimed to correlate how users perceive the use of e-mail reply suggestions to some variables: job, specialization, age, and gender. In this descriptive-inferential survey design, a self-developed questionnaire was distributed online to collect the data from a convenient sample of e-mail users. A questionnaire was used to collect data after checking the issues of validity and reliability indications. The collected data were analyzed using SPSS software."
(2) HQA-Data: A historical question answer generation dataset from previous multi perspective conversation,https://typeset.io/papers/hqa-data-a-historical-question-answer-generation-dataset-uk7hbalm,About: This article is published in Data in Brief. The article was published on 01 May 2023. and is currently open access. The article focuses on the topics: Medicine & Computer science.
(3) Quora Question Answer Dataset,https://typeset.io/papers/quora-question-answer-dataset-vkbbwfrr4i,"Abstract: We report on a progressing work for compiling Quora Question Answer dataset. Quora dataset is composed of questions which are posed in Quora Question Answering site. It is the only dataset which provides sentence-level and word-level answers at the same time. Moreover, the questions in the dataset are authentic which is much more realistic for Question Answering systems. We test the performance of a state-of-the-art Question Answering system on the dataset and compare it with human performance to establish an upper bound."
(4) dataset,https://typeset.io/papers/dataset-38m63rini0,"Abstract: We release a Question Answering dataset containing +5000 questions specifically taking spatial context information into account, i.e. visual features of a surrounding target object, or the user's location and moving direction -- SpatialQuestions. The data was collected in a big-scale user study with Amazon's Mechanical Turk platform involving over 400 crowdworkers and annotated semi-automatically in the post-processing. <br>"
(5) CodeQA: A Question Answering Dataset for Source Code Comprehension,https://typeset.io/papers/codeqa-a-question-answering-dataset-for-source-code-464wmg63an,"Abstract: We propose CodeQA, a free-form question answering dataset for the purpose of source code comprehension: given a code snippet and a question, a textual answer is required to be generated. CodeQA contains a Java dataset with 119,778 question-answer pairs and a Python dataset with 70,085 question-answer pairs. To obtain natural and faithful questions and answers, we implement syntactic rules and semantic analysis to transform code comments into question-answer pairs. We present the construction process and conduct systematic analysis of our dataset. Experiment results achieved by several neural baselines on our dataset are shown and discussed. While research on question-answering and machine reading comprehension develops rapidly, few prior work has drawn attention to code question answering. This new dataset can serve as a useful research benchmark for source code comprehension."
(6) CodeQA: A Question Answering Dataset for Source Code Comprehension.,https://typeset.io/papers/codeqa-a-question-answering-dataset-for-source-code-cbl35evmu9,"Abstract: We propose CodeQA, a free-form question answering dataset for the purpose of source code comprehension: given a code snippet and a question, a textual answer is required to be generated. CodeQA contains a Java dataset with 119,778 question-answer pairs and a Python dataset with 70,085 question-answer pairs. To obtain natural and faithful questions and answers, we implement syntactic rules and semantic analysis to transform code comments into question-answer pairs. We present the construction process and conduct systematic analysis of our dataset. Experiment results achieved by several neural baselines on our dataset are shown and discussed. While research on question-answering and machine reading comprehension develops rapidly, few prior work has drawn attention to code question answering. This new dataset can serve as a useful research benchmark for source code comprehension."
(7) Data for question answering: the case of why,https://typeset.io/papers/data-for-question-answering-the-case-of-why-4xd3q8rgpg,"Abstract: For research and development of an approach for automatically answering why-questions (why-QA) a data collection was created The data set was obtained by way of elicitation and comprises a total of 395 why-questions For each question, the data set includes the source document and one or two user-formulated answers In addition, for a subset of the questions, user-formulated paraphrases are available All question-answer pairs have been annotated with information on topic and semantic answer type The resulting data set is of importance not only for our research, but we expect it to contribute to and stimulate other research in the field of why-QA"
(8) Clotho-AQA dataset,https://typeset.io/papers/clotho-aqa-dataset-2ljrzl4gi8,"Abstract: Clotho-AQA is an audio question-answering dataset consisting of 1991 audio samples taken from Clotho dataset [1]. Each audio sample has 6 associated questions collected through crowdsourcing. For each question, the answers are provided by three different annotators making a total of 35,838 question-answer pairs. For each audio sample, 4 questions are designed to be answered with 'yes' or 'no', while the remaining two questions are designed to be answered in a single word. More details about the data collection process and data splitting process can be found in our following paper. <em><strong>S. Lipping, P. Sudarsanam, K. Drossos, T. Virtanen ‘Clotho-AQA: A Crowdsourced Dataset for Audio Question Answering.’ </strong></em>The paper is available online at 2204.09634.pdf (arxiv.org) If you use the Clotho-AQA dataset, please cite the paper mentioned above. A sample baseline model to use the Clotho-AQA dataset can be found at partha2409/AquaNet (github.com) To use the dataset, • Download and extract <strong>‘audio_files.zip’</strong>. This contains all the 1991 audio samples in the dataset. • Download <strong>‘clotho_aqa_train.csv’, ‘clotho_aqa_val.csv’, </strong>and <strong>‘clotho_aqa_test.csv’</strong>. These files contain the train, validation, and test splits, respectively. They contain the audio file name, questions, answers, and confidence scores provided by the annotators. <strong>License:</strong> The audio files in the archive ‘audio_files.zip’ are under the corresponding licenses (mostly CreativeCommons with attribution) of Freesound [2] platform, mentioned explicitly in the CSV file <strong>’clotho_aqa_metadata.csv’</strong> for each of the audio files. That is, each audio file in the archive is listed in the CSV file with meta-data. The meta-data for each file are: • File name • Keywords • URL for the original audio file • Start and ending samples for the excerpt that is used in the Clotho dataset • Uploader/user in the Freesound platform (manufacturer) • Link to the license of the file. The questions and answers in the files: • clotho_aqa_train.csv • clotho_aqa_val.csv • clotho_aqa_test.csv are under the MIT license, described in the LICENSE file. <strong>References:</strong> [1] K. Drossos, S. Lipping and T. Virtanen, ""Clotho: An Audio Captioning Dataset,"" IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Barcelona, Spain, 2020, pp. 736- 740, doi: 10.1109/ICASSP40776.2020.9052990. [2] Frederic Font, Gerard Roma, and Xavier Serra. 2013. Freesound technical demo. In Proceedings of the 21st ACM international conference on Multimedia (MM '13). ACM, New York, NY, USA, 411-412. DOI: https://doi.org/10.1145/2502081.2502245"
(9) SearchQA: A New Q&amp;A Dataset Augmented with Context from a Search Engine,https://typeset.io/papers/searchqa-a-new-q-a-dataset-augmented-with-context-from-a-323fua2u0k,"Abstract: We publicly release a new large-scale dataset, called SearchQA, for machine comprehension, or question-answering. Unlike recently released datasets, such as DeepMind CNN/DailyMail and SQuAD, the proposed SearchQA was constructed to reflect a full pipeline of general question-answering. That is, we start not from an existing article and generate a question-answer pair, but start from an existing question-answer pair, crawled from J! Archive, and augment it with text snippets retrieved by Google. Following this approach, we built SearchQA, which consists of more than 140k question-answer pairs with each pair having 49.6 snippets on average. Each question-answer-context tuple of the SearchQA comes with additional meta-data such as the snippet's URL, which we believe will be valuable resources for future research. We conduct human evaluation as well as test two baseline methods, one simple word selection and the other deep learning based, on the SearchQA. We show that there is a meaningful gap between the human and machine performances. This suggests that the proposed dataset could well serve as a benchmark for question-answering."
(10) SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine,https://typeset.io/papers/searchqa-a-new-q-a-dataset-augmented-with-context-from-a-sw7k3tud5o,"Abstract: We publicly release a new large-scale dataset, called SearchQA, for machine comprehension, or question-answering. Unlike recently released datasets, such as DeepMind CNN/DailyMail and SQuAD, the proposed SearchQA was constructed to reflect a full pipeline of general question-answering. That is, we start not from an existing article and generate a question-answer pair, but start from an existing question-answer pair, crawled from J! Archive, and augment it with text snippets retrieved by Google. Following this approach, we built SearchQA, which consists of more than 140k question-answer pairs with each pair having 49.6 snippets on average. Each question-answer-context tuple of the SearchQA comes with additional meta-data such as the snippet's URL, which we believe will be valuable resources for future research. We conduct human evaluation as well as test two baseline methods, one simple word selection and the other deep learning based, on the SearchQA. We show that there is a meaningful gap between the human and machine performances. This suggests that the proposed dataset could well serve as a benchmark for question-answering."
