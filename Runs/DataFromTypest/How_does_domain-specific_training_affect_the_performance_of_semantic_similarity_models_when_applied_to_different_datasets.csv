Title,Link,Abstract
(1) Efficient Domain Adaptation of Sentence Embeddings using Adapters,https://typeset.io/papers/efficient-domain-adaptation-of-sentence-embeddings-using-2oe71pg9,"Abstract: Sentence embeddings enable us to capture the semantic similarity of short texts. Most sentence embedding models are trained for general semantic textual similarity (STS) tasks. Therefore, to use sentence embeddings in a particular domain, the model must be adapted to it in order to achieve good results. Usually, this is done by fine-tuning the entire sentence embedding model for the domain of interest. While this approach yields state-of-the-art results, all of the model's weights are updated during fine-tuning, making this method resource-intensive. Therefore, instead of fine-tuning entire sentence embedding models for each target domain individually, we propose to train lightweight adapters. These domain-specific adapters do not require fine-tuning all underlying sentence embedding model parameters. Instead, we only train a small number of additional parameters while keeping the weights of the underlying sentence embedding model fixed. Training domain-specific adapters allows always using the same base model and only exchanging the domain-specific adapters to adapt sentence embeddings to a specific domain. We show that using adapters for parameter-efficient domain adaptation of sentence embeddings yields competitive performance within 1% of a domain-adapted, entirely fine-tuned sentence embedding model while only training approximately 3.6% of the parameters."
(2) Efficient data selection employing Semantic Similarity-based Graph Structures for model training,https://typeset.io/papers/efficient-data-selection-employing-semantic-similarity-based-2q6q72gyu7,"Abstract: Recent developments in natural language processing (NLP) have highlighted the need for substantial amounts of data for models to capture textual information accurately. This raises concerns regarding the computational resources and time required for training such models. This paper introduces Semantics for data SAliency in Model performance Estimation (SeSaME). It is an efficient data sampling mechanism solely based on textual information without passing the data through a compute-heavy model or other intensive pre-processing transformations. The application of this approach is demonstrated in the use case of low-resource automated speech recognition (ASR) models, which excessively rely on text-to-speech (TTS) calls when using augmented data. SeSaME learns to categorize new incoming data points into speech recognition difficulty buckets by employing semantic similarity-based graph structures and discrete ASR information from homophilous neighbourhoods through message passing. The results indicate reliable projections of ASR performance, with a 93% accuracy increase when using the proposed method compared to random predictions, bringing non-trivial information on the impact of textual representations in speech models. Furthermore, a series of experiments show both the benefits and challenges of using the ASR information on incoming data to fine-tune the model. We report a 7% drop in validation loss compared to random sampling, 7% WER drop with non-local aggregation when evaluating against a highly difficult dataset, and 1.8% WER drop with local aggregation and high semantic similarity between datasets."
(3) A synthetic data approach for domain generalization of NLI models,https://typeset.io/papers/a-synthetic-data-approach-for-domain-generalization-of-nli-245e8kvdly,"Abstract: Natural Language Inference (NLI) remains an important benchmark task for LLMs. NLI datasets are a springboard for transfer learning to other semantic tasks, and NLI models are standard tools for identifying the faithfulness of model-generated text. There are several large scale NLI datasets today, and models have improved greatly by hill-climbing on these collections. Yet their realistic performance on out-of-distribution/domain data is less well-understood. We present an in-depth exploration of the problem of domain generalization of NLI models. We demonstrate a new approach for generating synthetic NLI data in diverse domains and lengths, so far not covered by existing training sets. The resulting examples have meaningful premises, the hypotheses are formed in creative ways rather than simple edits to a few premise tokens, and the labels have high accuracy. We show that models trained on this data (685K synthetic examples) have the best generalization to completely new downstream test settings. On the TRUE benchmark, a T5-small model trained with our data improves around 7% on average compared to training on the best alternative dataset. The improvements are more pronounced for smaller models, while still meaningful on a T5 XXL model. We also demonstrate gains on test sets when in-domain training data is augmented with our domain-general synthetic data."
(4) Estimating Semantic Similarity between In-Domain and Out-of-Domain Samples,https://typeset.io/papers/estimating-semantic-similarity-between-in-domain-and-out-of-3lqsv89n,"Abstract: Prior work typically describes out-of-domain (OOD) or out-of-distribution (OODist) samples as those that originate from dataset(s) or source(s) different from the training set but for the same task. When compared to in-domain (ID) samples, the models have been known to usually perform poorer on OOD samples, although this observation is not consistent. Another thread of research has focused on OOD detection, albeit mostly using supervised approaches. In this work, we first consolidate and present a systematic analysis of multiple definitions of OOD and OODist as discussed in prior literature. Then, we analyze the performance of a model under ID and OOD/OODist settings in a principled way. Finally, we seek to identify an unsupervised method for reliably identifying OOD/OODist samples without using a trained model. The results of our extensive evaluation using 12 datasets from 4 different tasks suggest the promising potential of unsupervised metrics in this task."
(5) Estimating Semantic Similarity between In-Domain and Out-of-Domain Samples,https://typeset.io/papers/estimating-semantic-similarity-between-in-domain-and-out-of-qe6pxj0x,"Abstract: Prior work typically describes out-of-domain (OOD) or out-of-distribution (OODist) samples as those that originate from dataset(s) or source(s) different from the training set but for the same task. When compared to in-domain (ID) samples, the models have been known to usually perform poorer on OOD samples, although this observation is not consistent. Another thread of research has focused on OOD detection, albeit mostly using supervised approaches. In this work, we first consolidate and present a systematic analysis of multiple definitions of OOD and OODist as discussed in prior literature. Then, we analyze the performance of a model under ID and OOD/OODist settings in a principled way. Finally, we seek to identify an unsupervised method for reliably identifying OOD/OODist samples without using a trained model. The results of our extensive evaluation using 12 datasets from 4 different tasks suggest the promising potential of unsupervised metrics in this task."
(6) SynCDR : Training Cross Domain Retrieval Models with Synthetic Data,https://typeset.io/papers/syncdr-training-cross-domain-retrieval-models-with-synthetic-1ogqrxvzbm,"Abstract: In cross-domain retrieval, a model is required to identify images from the same semantic category across two visual domains. For instance, given a sketch of an object, a model needs to retrieve a real image of it from an online store's catalog. A standard approach for such a problem is learning a feature space of images where Euclidean distances reflect similarity. Even without human annotations, which may be expensive to acquire, prior methods function reasonably well using unlabeled images for training. Our problem constraint takes this further to scenarios where the two domains do not necessarily share any common categories in training data. This can occur when the two domains in question come from different versions of some biometric sensor recording identities of different people. We posit a simple solution, which is to generate synthetic data to fill in these missing category examples across domains. This, we do via category preserving translation of images from one visual domain to another. We compare approaches specifically trained for this translation for a pair of domains, as well as those that can use large-scale pre-trained text-to-image diffusion models via prompts, and find that the latter can generate better replacement synthetic data, leading to more accurate cross-domain retrieval models. Code for our work is available at https://github.com/samarth4149/SynCDR ."
(7) A Cross-Domain Semantic Similarity Measure and Multi-Source Domain Adaptation in Sentiment Analysis,https://typeset.io/papers/a-cross-domain-semantic-similarity-measure-and-multi-source-1d6lu4ir,"Abstract: Domain adaptation becomes crucial when there is a lack of labelled data in various domains. The accuracy of traditional machine learning models degrades largely if they are trained on one domain (called the source or training domain) and classify the data of a different domain (called the target domain or test domain, which is different from the source domain). The machine needs to train on a corresponding domain to improve the classification accuracy, but labelling each new domain is a complex and time-consuming task. Hence, the domain adaptation technique is required to solve the issue of data labeling. The similarity measure plays a vital role in selecting important pivot features from the target domain that match source domains. This research article has introduced an enhanced cross-entropy measure for matching the normalized frequency distribution of different domains and found an important domain-specific feature set. In addition, the technique of using enhanced cross entropy measures is proposed in the multi-source domain adaptation model to effectively classify the target domain data. The result shows that there is an improvement of 3.66% to 9.09% using our approach."
(8) Learning Domain‐specific Semantic Representation from Weakly Supervised Data to Improve Research Dataset Retrieval,https://typeset.io/papers/learning-domain-specific-semantic-representation-from-weakly-29uufvgr,"Abstract: Along with the development of the data‐driven research paradigm, there are exponentially increasing datasets, which bring challenges to researchers in the efficient retrieval of relevant datasets. Previous studies mainly focused on query expansion methods based on sparse retrieval models to improve the accuracy and recall in retrieval. We investigated the use of semantically rich information to retrieve relevant datasets and the benefits of using domain‐specific dense vector representation as opposed to general representation. First, we used pairs of metadata fields that have semantic relevance to construct the domain‐specific weakly supervised training data. Then, a pre‐trained transformer‐based deep learning model is fine‐tuned on the training data using the contrastive learning method. Finally, dense vector representations of the queries and datasets are obtained based on the fine‐tuned model. The relevance of a dataset to a query is measured by the similarity between the vectors. To evaluate the performance of the proposed model, we collected 104,683 datasets from 13 research data repositories, recruited volunteers to design research‐oriented queries, and annotated the retrieval results. The experimental results show that compared with the domain‐independent fine‐tuned model, our proposed method can improve the NDCG@10 score by about 5%."
(9) Estimating Semantic Similarity between In-Domain and Out-of-Domain Samples,https://typeset.io/papers/estimating-semantic-similarity-between-in-domain-and-out-of-tgx7uv8sl6,"Abstract: Prior work typically describes out-of-domain (OOD) or out-of-distribution (OODist) samples as those that originate from dataset(s) or source(s) different from the training set but for the same task.When compared to in-domain (ID) samples, the models have been known to usually perform poorer on OOD samples, although this observation is not consistent.Another thread of research has focused on OOD detection, albeit mostly using supervised approaches.In this work, we first consolidate and present a systematic analysis of multiple definitions of OOD and OODist as discussed in prior literature.Then, we analyze the performance of a model under ID and OOD/OODist settings in a principled way.Finally, we seek to identify an unsupervised method for reliably identifying OOD/OODist samples without using a trained model.The results of our extensive evaluation using 12 datasets from 4 different tasks suggest the promising potential of unsupervised metrics in this task."
(10) Improving The Performance of Semantic Text Similarity Tasks on Short Text Pairs,https://typeset.io/papers/improving-the-performance-of-semantic-text-similarity-tasks-33hin276,"Abstract: Training semantic similarity model to detect duplicate text pairs is a challenging task as almost all of datasets are imbalanced, by data nature positive samples are fewer than negative samples, this issue can easily lead to model bias. Using traditional pairwise loss functions like pairwise binary cross entropy or Contrastive loss on imbalanced data may lead to model bias, however triplet loss showed improved performance compared to other loss functions. In triplet loss-based models data is fed to the model as follow: anchor sentence, positive sentence and negative sentence. The original data is permutated to follow the input structure. The default structure of training samples data is 363,861 training samples (90% of the data) distributed as 134,336 positive samples and 229,524 negative samples. The triplet structured data helped to generate much larger amount of balanced training samples 456,219. The test results showed higher accuracy and f1 scores in testing. We fine-tunned RoBERTa pre trained model using Triplet loss approach, testing showed better results. The best model scored 89.51 F1 score, and 91.45 Accuracy compared to 86.74 F1 score and 87.45 Accuracy in the second-best Contrastive loss-based BERT model."
