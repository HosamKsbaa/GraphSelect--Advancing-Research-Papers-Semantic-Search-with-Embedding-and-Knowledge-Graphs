Title,Link,Abstract
(1) ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs,https://typeset.io/papers/oda-observation-driven-agent-for-integrating-llms-and-1b0fp4875d,"Abstract: The integration of Large Language Models (LLMs) and knowledge graphs (KGs) has achieved remarkable success in various natural language processing tasks. However, existing methodologies that integrate LLMs and KGs often navigate the task-solving process solely based on the LLM's analysis of the question, overlooking the rich cognitive potential inherent in the vast knowledge encapsulated in KGs. To address this, we introduce Observation-Driven Agent (ODA), a novel AI agent framework tailored for tasks involving KGs. ODA incorporates KG reasoning abilities via global observation that enhances reasoning capabilities through a cyclical paradigm of observation, action, and reflection. Confronting the exponential explosion of knowledge during observation, we innovatively design a recursive observation mechanism. Subsequently, we integrate the observed knowledge into the action and reflection modules. Through extensive experiments, ODA demonstrates state-of-the-art performance on several datasets, notably achieving accuracy improvements of 12.87% and 8.9%."
(2) Boosting Domain Knowledge Understanding of LLMs through Fine-Tuning with a Novel KNN Algorithm,https://typeset.io/papers/boosting-domain-knowledge-understanding-of-llms-through-fine-1t5ea6xy4y,"Abstract: <title>Abstract</title> Transformer-based architectures have revolutionized natural language processing, demonstrating exceptional capabilities in generating and understanding human-like text. Despite these advancements, challenges persist in enabling models to effectively interpret and generate domain-specific knowledge with high accuracy. The novel integration of a k-nearest neighbour (KNN) algorithm with the GPT-Neo model represents a significant enhancement, specifically designed to improve the model's performance in specialized fields through capturing semantic similarities and contextual nuances inherent in domain-specific data. The research involved fine-tuning the GPT-Neo model with the KNN algorithm, resulting in substantial improvements in accuracy, coherence, and contextual understanding, as evidenced through higher F1, BLEU, and ROUGE scores, alongside reduced perplexity. Ablation studies highlighted the contributions of dynamic weighting schemes, custom distance metrics, and hybrid embeddings to the overall performance gains. The implications of this approach extend to various applications requiring precise domain knowledge interpretation, such as medical diagnostics, legal analysis, and technical support. Future work aims to explore broader domain datasets, integrate additional machine learning algorithms, and further refine the training methodologies to enhance model capabilities."
(3) KGLens: A Parameterized Knowledge Graph Solution to Assess What an LLM Does and Doesn't Know,https://typeset.io/papers/kglens-a-parameterized-knowledge-graph-solution-to-assess-4l6wyo1mgp,"Abstract: Current approaches to evaluating large language models (LLMs) with pre-existing Knowledge Graphs (KG) mostly ignore the structure of the KG and make arbitrary choices of which part of the graph to evaluate. In this paper, we introduce KGLens, a method to evaluate LLMs by generating natural language questions from a KG in a structure aware manner so that we can characterize its performance on a more aggregated level. KGLens uses a parameterized KG, where each edge is augmented with a beta distribution that guides how to sample edges from the KG for QA testing. As the evaluation proceeds, different edges of the parameterized KG are sampled and assessed appropriately, converging to a more global picture of the performance of the LLMs on the KG as a whole. In our experiments, we construct three domain-specific KGs for knowledge assessment, comprising over 19,000 edges, 700 relations, and 21,000 entities. The results demonstrate that KGLens can not only assess overall performance but also provide topic, temporal, and relation analyses of LLMs. This showcases the adaptability and customizability of KGLens, emphasizing its ability to focus the evaluation based on specific criteria."
(4) An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration,https://typeset.io/papers/an-enhanced-prompt-based-llm-reasoning-scheme-via-knowledge-4cayssj0km,"Abstract: While Large Language Models (LLMs) demonstrate exceptional performance in a multitude of Natural Language Processing (NLP) tasks, they encounter challenges in practical applications, including issues with hallucinations, inadequate knowledge updating, and limited transparency in the reasoning process. To overcome these limitations, this study innovatively proposes a collaborative training-free reasoning scheme involving tight cooperation between Knowledge Graph (KG) and LLMs. This scheme first involves using LLMs to iteratively explore KG, selectively retrieving a task-relevant knowledge subgraph to support reasoning. The LLMs are then guided to further combine inherent implicit knowledge to reason on the subgraph while explicitly elucidating the reasoning process. Through such a cooperative approach, our scheme achieves more reliable knowledge-based reasoning and facilitates the tracing of the reasoning results. Experimental results show that our scheme significantly progressed across multiple datasets, notably achieving over a 10% improvement on the QALD10 dataset compared to the best baseline and the fine-tuned state-of-the-art (SOTA) work. Building on this success, this study hopes to offer a valuable reference for future research in the fusion of KG and LLMs, thereby enhancing LLMs' proficiency in solving complex issues."
"(5) A Preliminary Roadmap for LLMs as Assistants in Exploring, Analyzing, and Visualizing Knowledge Graphs",https://typeset.io/papers/a-preliminary-roadmap-for-llms-as-assistants-in-exploring-5219q398qy,"Abstract: We present a mixed-methods study to explore how large language models (LLMs) can assist users in the visual exploration and analysis of knowledge graphs (KGs). We surveyed and interviewed 20 professionals from industry, government laboratories, and academia who regularly work with KGs and LLMs, either collaboratively or concurrently. Our findings show that participants overwhelmingly want an LLM to facilitate data retrieval from KGs through joint query construction, to identify interesting relationships in the KG through multi-turn conversation, and to create on-demand visualizations from the KG that enhance their trust in the LLM's outputs. To interact with an LLM, participants strongly prefer a chat-based 'widget,' built on top of their regular analysis workflows, with the ability to guide the LLM using their interactions with a visualization. When viewing an LLM's outputs, participants similarly prefer a combination of annotated visuals (e.g., subgraphs or tables extracted from the KG) alongside summarizing text. However, participants also expressed concerns about an LLM's ability to maintain semantic intent when translating natural language questions into KG queries, the risk of an LLM 'hallucinating' false data from the KG, and the difficulties of engineering a 'perfect prompt.' From the analysis of our interviews, we contribute a preliminary roadmap for the design of LLM-driven knowledge graph exploration systems and outline future opportunities in this emergent design space."
(6) Making Large Language Models Perform Better in Knowledge Graph Completion,https://typeset.io/papers/making-large-language-models-perform-better-in-knowledge-3v4vmmkpyj,"Abstract: Large language model (LLM) based knowledge graph completion (KGC) aims to predict the missing triples in the KGs with LLMs and enrich the KGs to become better web infrastructure, which can benefit a lot of web-based automatic services. However, research about LLM-based KGC is limited and lacks effective utilization of LLM's inference capabilities, which ignores the important structural information in KGs and prevents LLMs from acquiring accurate factual knowledge. In this paper, we discuss how to incorporate the helpful KG structural information into the LLMs, aiming to achieve structrual-aware reasoning in the LLMs. We first transfer the existing LLM paradigms to structural-aware settings and further propose a knowledge prefix adapter (KoPA) to fulfill this stated goal. KoPA employs structural embedding pre-training to capture the structural information of entities and relations in the KG. Then KoPA informs the LLMs of the knowledge prefix adapter which projects the structural embeddings into the textual space and obtains virtual knowledge tokens as a prefix of the input prompt. We conduct comprehensive experiments on these structural-aware LLM-based KGC methods and provide an in-depth analysis comparing how the introduction of structural information would be better for LLM's knowledge reasoning ability. Our code is released at https://github.com/zjukg/KoPA."
(7) From human experts to machines: An LLM supported approach to ontology and knowledge graph construction,https://typeset.io/papers/from-human-experts-to-machines-an-llm-supported-approach-to-3f0rjdi6v5,"Abstract: The conventional process of building Ontologies and Knowledge Graphs (KGs) heavily relies on human domain experts to define entities and relationship types, establish hierarchies, maintain relevance to the domain, fill the ABox (or populate with instances), and ensure data quality (including amongst others accuracy and completeness). On the other hand, Large Language Models (LLMs) have recently gained popularity for their ability to understand and generate human-like natural language, offering promising ways to automate aspects of this process. This work explores the (semi-)automatic construction of KGs facilitated by open-source LLMs. Our pipeline involves formulating competency questions (CQs), developing an ontology (TBox) based on these CQs, constructing KGs using the developed ontology, and evaluating the resultant KG with minimal to no involvement of human experts. We showcase the feasibility of our semi-automated pipeline by creating a KG on deep learning methodologies by exploiting scholarly publications. To evaluate the answers generated via Retrieval-Augmented-Generation (RAG) as well as the KG concepts automatically extracted using LLMs, we design a judge LLM, which rates the generated content based on ground truth. Our findings suggest that employing LLMs could potentially reduce the human effort involved in the construction of KGs, although a human-in-the-loop approach is recommended to evaluate automatically generated KGs."
(8) Towards Safe LLMs Integration,https://typeset.io/papers/towards-safe-llms-integration-2suznyu9zs,"Abstract: Abstract LLMs face a critical vulnerability known as sandbox breakout, where attackers bypass the system designers’ limitations to prevent malicious access to the resources for which the LLM agent is a user interface. Thus, they can access the system and potentially steal data, change the interaction with other users, or inject malicious code or contents into underlying databases. Therefore, it is essential to identify and address vulnerabilities that could be exploited to break out of the sandbox. These vulnerabilities could exist in the sandbox, the operating system, or the LLM’s software dependencies. To mitigate the risk of LLM sandbox breakout, robust security measures, such as regular model updates, automated model red-teaming, testing, and access control policies, must be implemented. In addition, sandboxing should be enforced at multiple levels to reduce the attack surface and prevent attackers from accessing critical systems. By implementing these measures, the risk of LLM sandbox breakout can be significantly reduced, and the security and reliability of LLM-based applications can be improved."
(9) LLM-assisted Knowledge Graph Engineering: Experiments with ChatGPT,https://typeset.io/papers/llm-assisted-knowledge-graph-engineering-experiments-with-eb2ah3id2b,"Abstract: Zusammenfassung Knowledge Graphs (KG) provide us with a structured, flexible, transparent, cross-system, and collaborative way of organizing our knowledge and data across various domains in society and industrial as well as scientific disciplines. KGs surpass any other form of representation in terms of effectiveness. However, Knowledge Graph Engineering (KGE) requires in-depth experiences of graph structures, web technologies, existing models and vocabularies, rule sets, logic, as well as best practices. It also demands a significant amount of work. Considering the advancements in large language models (LLMs) and their interfaces and applications in recent years, we have conducted comprehensive experiments with ChatGPT to explore its potential in supporting KGE. In this paper, we present a selection of these experiments and their results to demonstrate how ChatGPT can assist us in the development and management of KGs. Zusammenfassung. Wissensgraphen (englisch Knowledge Graphs , KGs), bieten uns eine strukturierte, flexible, transparente, systemübergreifende und kollaborative Möglichkeit, unser Wissen und unsere Daten über verschiedene Bereiche der Gesellschaft und der industriellen sowie wissenschaftlichen Disziplinen hinweg zu organisieren. KGs übertreffen jede andere Form der Repräsentation in Bezug auf die Effektivität. Die Entwicklung von Wissensgraphen (englisch Knowledge Graph Engineering , KGE) erfordert jedoch fundierte Erfahrungen mit Graphstrukturen, Webtechnologien, bestehenden Modellen und Vokabularen, Regelwerken, Logik sowie Best Practices. Es erfordert auch einen erheblichen Arbeitsaufwand. In Anbetracht der Fortschritte bei großen Sprachmodellen (englisch Large Language Modells , LLMs) und ihren Schnittstellen und Anwendungen in den letzten Jahren haben wir umfassende Experimente mit ChatGPT durchgeführt, um sein Potenzial zur Unterstützung von KGE zu untersuchen. In diesem Artikel stellen wir eine Auswahl dieser Experimente und ihre Ergebnisse vor, um zu zeigen, wie ChatGPT uns bei der Entwicklung und Verwaltung von KGs unterstützen kann."
(10) KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases,https://typeset.io/papers/knowledgpt-enhancing-large-language-models-with-retrieval-3vmerrxtrp,"Abstract: Large language models (LLMs) have demonstrated impressive impact in the field of natural language processing, but they still struggle with several issues regarding, such as completeness, timeliness, faithfulness and adaptability. While recent efforts have focuses on connecting LLMs with external knowledge sources, the integration of knowledge bases (KBs) remains understudied and faces several challenges. In this paper, we introduce KnowledGPT, a comprehensive framework to bridge LLMs with various knowledge bases, facilitating both the retrieval and storage of knowledge. The retrieval process employs the program of thought prompting, which generates search language for KBs in code format with pre-defined functions for KB operations. Besides retrieval, KnowledGPT offers the capability to store knowledge in a personalized KB, catering to individual user demands. With extensive experiments, we show that by integrating LLMs with KBs, KnowledGPT properly answers a broader range of questions requiring world knowledge compared with vanilla LLMs, utilizing both knowledge existing in widely-known KBs and extracted into personalized KBs."
