Title,Link,Abstract
(1) A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems,https://typeset.io/papers/a-tale-of-trust-and-accuracy-base-vs-instruct-llms-in-rag-29h4y2y9po,"Abstract: Retrieval Augmented Generation (RAG) represents a significant advancement in artificial intelligence combining a retrieval phase with a generative phase, with the latter typically being powered by large language models (LLMs). The current common practices in RAG involve using ""instructed"" LLMs, which are fine-tuned with supervised training to enhance their ability to follow instructions and are aligned with human preferences using state-of-the-art techniques. Contrary to popular belief, our study demonstrates that base models outperform their instructed counterparts in RAG tasks by 20% on average under our experimental settings. This finding challenges the prevailing assumptions about the superiority of instructed LLMs in RAG applications. Further investigations reveal a more nuanced situation, questioning fundamental aspects of RAG and suggesting the need for broader discussions on the topic; or, as Fromm would have it, ""Seldom is a glance at the statistics enough to understand the meaning of the figures""."
(2) Unveiling the Synergy: Retrieval Augmented Generation (RAG) Meets Knowledge Graphs,https://typeset.io/papers/unveiling-the-synergy-retrieval-augmented-generation-rag-d00ez2e12x,"Abstract: &lt;strong&gt; Tools and Platform for Integration of Knowledge Graph with RAG pipelines. &lt;/strong&gt; Authors Aland Astudillo (ORCID: 0009-0008-8672-3168) Aishwarya Nambissan (ORCID: 0009-0003-3823-6609) Many users of chatbots such as ChatGPT, have encountered the problem of receiving inappropriate or incompatible responses. There are several reasons why this might&amp;nbsp;happen."
(3) Validating and constructing behavioral models for simulation and projection using automated knowledge extraction,https://typeset.io/papers/validating-and-constructing-behavioral-models-for-simulation-2l3d700tfs,"Abstract: Human behavior may be one of the most challenging phenomena to model and validate. This paper proposes a method for automatically extracting and compiling evidence on human behavior determinants into a knowledge graph. The method (1) extracts associations of behavior determinants and choice options in relation to study groups and moderators from published studies using Natural Language Processing and Deep Learning, (2) synthesizes the extracted evidence into a knowledge graph, and (3) sub-selects the model components and relationships that are relevant and robust. The method can be used to either (4a) construct a structurally valid simulation model before proceeding with calibration or (4b) to validate the structure of existing simulation models. To demonstrate the feasibility of the method, we discuss an example implementation with mode of transport as behavior choice. We find that including non-frequently studied significant behavior determinants drastically improves the model's explanatory power in comparison to only including frequently studied variables. The paper serves as a proof-of-concept which can be reused, extended or adapted for various purposes."
(4) Knowledge Verification From Data.,https://typeset.io/papers/knowledge-verification-from-data-10afapuu,"Abstract: Knowledge verification is an important task in the quality management of knowledge graphs (KGs). Knowledge is a summary of facts and events based on human cognition and experience. Due to the nature of knowledge, most knowledge quality (KQ) management methods are designed by human experts or the characteristics of existing knowledge, which may be limited by human cognition and the quality of existing knowledge. Numerical data contain a wealth of potential information that may be helpful in verifying knowledge, which is rarely explored. However, due to the implicit representation of numerical data to facts as well as the noise in the data, it is challenging to use data to verify the knowledge. Therefore, this article proposes a knowledge verification method, which discovers the correlation and causality from numerical data to validate knowledge and then evaluate the quality of knowledge. Moreover, to address the impact of noise, the method integrates multisource knowledge to jointly evaluate the KQ. Specifically, an iterative update method is designed to update KQ by utilizing the consistency between multisource knowledge while designing knowledge verification factors based on data causality and correlation to manage update process. The method is validated with multiple datasets, and the results demonstrate that the proposed method could evaluate KQ more accurately and has strong robustness to noise in the data."
(5) KGValidator: A Framework for Automatic Validation of Knowledge Graph Construction,https://typeset.io/papers/kgvalidator-a-framework-for-automatic-validation-of-4qefeo3n26,"Abstract: This study explores the use of Large Language Models (LLMs) for automatic evaluation of knowledge graph (KG) completion models. Historically, validating information in KGs has been a challenging task, requiring large-scale human annotation at prohibitive cost. With the emergence of general-purpose generative AI and LLMs, it is now plausible that human-in-the-loop validation could be replaced by a generative agent. We introduce a framework for consistency and validation when using generative models to validate knowledge graphs. Our framework is based upon recent open-source developments for structural and semantic validation of LLM outputs, and upon flexible approaches to fact checking and verification, supported by the capacity to reference external knowledge sources of any kind. The design is easy to adapt and extend, and can be used to verify any kind of graph-structured data through a combination of model-intrinsic knowledge, user-supplied context, and agents capable of external knowledge retrieval."
(6) Creating and validating a scholarly knowledge graph using natural language processing and microtask crowdsourcing,https://typeset.io/papers/creating-and-validating-a-scholarly-knowledge-graph-using-1kbrw2ex,"Abstract: Abstract Due to the growing number of scholarly publications, finding relevant articles becomes increasingly difficult. Scholarly knowledge graphs can be used to organize the scholarly knowledge presented within those publications and represent them in machine-readable formats. Natural language processing (NLP) provides scalable methods to automatically extract knowledge from articles and populate scholarly knowledge graphs. However, NLP extraction is generally not sufficiently accurate and, thus, fails to generate high granularity quality data. In this work, we present TinyGenius, a methodology to validate NLP-extracted scholarly knowledge statements using microtasks performed with crowdsourcing. TinyGenius is employed to populate a paper-centric knowledge graph, using five distinct NLP methods. We extend our previous work of the TinyGenius methodology in various ways. Specifically, we discuss the NLP tasks in more detail and include an explanation of the data model. Moreover, we present a user evaluation where participants validate the generated NLP statements. The results indicate that employing microtasks for statement validation is a promising approach despite the varying participant agreement for different microtasks."
(7) Efficient Knowledge Graph Validation via Cross-Graph Representation Learning.,https://typeset.io/papers/efficient-knowledge-graph-validation-via-cross-graph-514d0p130s,"Abstract: Recent advances in information extraction have motivated the automatic construction of huge Knowledge Graphs (KGs) by mining from large-scale text corpus. However, noisy facts are unavoidably introduced into KGs that could be caused by automatic extraction. To validate the correctness of facts (i.e., triplets) inside a KG, one possible approach is to map the triplets into vector representations by capturing the semantic meanings of facts. Although many representation learning approaches have been developed for knowledge graphs, these methods are not effective for validation. They usually assume that facts are correct, and thus may overfit noisy facts and fail to detect such facts. Towards effective KG validation, we propose to leverage an external human-curated KG as auxiliary information source to help detect the errors in a target KG. The external KG is built upon human-curated knowledge repositories and tends to have high precision. On the other hand, although the target KG built by information extraction from texts has low precision, it can cover new or domain-specific facts that are not in any human-curated repositories. To tackle this challenging task, we propose a cross-graph representation learning framework, i.e., CrossVal, which can leverage an external KG to validate the facts in the target KG efficiently. This is achieved by embedding triplets based on their semantic meanings, drawing cross-KG negative samples and estimating a confidence score for each triplet based on its degree of correctness. We evaluate the proposed framework on datasets across different domains. Experimental results show that the proposed framework achieves the best performance compared with the state-of-the-art methods on large-scale KGs."
(8) Knowledge Verification From Data,https://typeset.io/papers/knowledge-verification-from-data-2x4y4m3c,"Abstract: Knowledge verification is an important task in the quality management of knowledge graphs (KGs). Knowledge is a summary of facts and events based on human cognition and experience. Due to the nature of knowledge, most knowledge quality (KQ) management methods are designed by human experts or the characteristics of existing knowledge, which may be limited by human cognition and the quality of existing knowledge. Numerical data contain a wealth of potential information that may be helpful in verifying knowledge, which is rarely explored. However, due to the implicit representation of numerical data to facts as well as the noise in the data, it is challenging to use data to verify the knowledge. Therefore, this article proposes a knowledge verification method, which discovers the correlation and causality from numerical data to validate knowledge and then evaluate the quality of knowledge. Moreover, to address the impact of noise, the method integrates multisource knowledge to jointly evaluate the KQ. Specifically, an iterative update method is designed to update KQ by utilizing the consistency between multisource knowledge while designing knowledge verification factors based on data causality and correlation to manage update process. The method is validated with multiple datasets, and the results demonstrate that the proposed method could evaluate KQ more accurately and has strong robustness to noise in the data."
(9) Knowledge graph verification and updating method based on block chain distributed double consensus,https://typeset.io/papers/knowledge-graph-verification-and-updating-method-based-on-5fts1fowbs,"Abstract: The invention discloses a knowledge graph verification and updating method based on block chain distributed double consensus, which comprises the following steps: all machine nodes i perform local newly-added knowledge mining and match with the latest knowledge graph of an original block chain to find out different knowledge to form a knowledge list; all the machine nodes share the knowledge listto other machine nodes to form a total task table Si; the machine nodes broadcast the entity relationship pair k in the node Sp to other machine nodes in sequence and are used for verifying whether the entity relationship pair k corresponding to each number n of a machine node p is provided by a machine node i or not; after the machine node p receives f + 1 Confire messages, local verification isinitiated on the entity relationship pair k in the consensus Sp; and when the machine node p receives more than f + 1 local-complet messages, updating the latest knowledge graph, performing virtual currency rewarding on the providing node i of the entity relationship pair k, and writing the newly added knowledge graph and the transaction into the blockchain at the same time."
(10) Efficient Knowledge Graph Validation via Cross-Graph Representation Learning,https://typeset.io/papers/efficient-knowledge-graph-validation-via-cross-graph-37t7f3zkph,"Abstract: Recent advances in information extraction have motivated the automatic construction of huge Knowledge Graphs (KGs) by mining from large-scale text corpus. However, noisy facts are unavoidably introduced into KGs that could be caused by automatic extraction. To validate the correctness of facts (i.e., triplets) inside a KG, one possible approach is to map the triplets into vector representations by capturing the semantic meanings of facts. Although many representation learning approaches have been developed for knowledge graphs, these methods are not effective for validation. They usually assume that facts are correct, and thus may overfit noisy facts and fail to detect such facts. Towards effective KG validation, we propose to leverage an external human-curated KG as auxiliary information source to help detect the errors in a target KG. The external KG is built upon human-curated knowledge repositories and tends to have high precision. On the other hand, although the target KG built by information extraction from texts has low precision, it can cover new or domain-specific facts that are not in any human-curated repositories. To leverage external KG for validation, one intuitive approach is to find matched triplets between the target KG and the external KG. However, this approach can only apply to the small portion of triplets that are covered by both external and target KGs and is not useful for the validation of majority of the triplets. To tackle this challenging task, we propose a cross-graph representation learning framework, i.e., CrossVal, which can leverage an external KG to validate the facts in the target KG efficiently. This is achieved by embedding triplets based on their semantic meanings, drawing cross-KG negative samples and estimating a confidence score for each triplet based on its degree of correctness. We evaluate the proposed framework on datasets across different domains. Experimental results show that the proposed framework achieves the best performance compared with the state-of-the-art methods on large-scale KGs."
