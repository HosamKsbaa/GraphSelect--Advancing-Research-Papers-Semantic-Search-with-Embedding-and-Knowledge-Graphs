Title,Link,Abstract
(1) Evaluating of efficacy semantic similarity methods for comparison of academic thesis and dissertation texts,https://typeset.io/papers/evaluating-of-efficacy-semantic-similarity-methods-for-igq9li36l9,"Abstract: Detecting semantic similarity between documents is vital in natural language processing applications. One widely used method for measuring the semantic similarity of text documents is embedding, which involves converting texts into numerical vectors using various NLP methods. This paper presents a comparative analysis of four embedding methods for detecting semantic similarity in theses and dissertations , namely Term Frequency–Inverse Document Frequency, Document to Vector, Sentence Bidirectional Encoder Representations from Transformers, and Bidirectional Encoder Representations from Transformers with cosine similarity. The study used two datasets consisting of 27 documents from Duhok Polytechnic University and 100 documents from ProQuest.com. The texts from these documents were pre-processed to make them suitable for semantic similarity analysis. The evaluation of the methods was based on several metrics, including accuracy, precision, Recall, F1 score, and processing time. The results showed that the traditional method, TF-IDF, outperformed modern methods in embedding and detecting actual semantic similarity between documents, with processing time not exceeding a few seconds."
(2) Semantic Similarity Detection and Analysis For Text Documents,https://typeset.io/papers/semantic-similarity-detection-and-analysis-for-text-1blh1322eu,"Abstract: This paper delves into the critical task of measuring semantic similarity in text documents, a fundamental need in today's data-rich landscape. Efficiently gauging semantic con-nections is vital for applications such as information retrieval, content recommendation, and document clustering. Traditional methods, centered on lexical and syntactic features, often fall short, especially when dealing with synonyms, abbreviations, or diverse language use. Consequently, there's a growing demand for advanced techniques that can better capture the underlying semantic relationships in text documents. The study introduces a comprehensive approach to semantic similarity analysis for text documents. This approach involves the development and evaluation of innovative methodologies that utilize advanced natural language processing and machine learning techniques. It explores the integration of domain-specific knowledge bases, advanced feature representations, and semantic embeddings to enhance the accuracy of similarity detection. Additionally, adaptable synonym repositories are investigated to align with evolving language trends. These efforts collectively aim to advance the field of semantic similarity detection, ultimately improving the precision and efficiency of document analysis across a broad spectrum of text-based applications."
(3) A Comparative Study of Sentence Embedding Models for Assessing Semantic Variation,https://typeset.io/papers/a-comparative-study-of-sentence-embedding-models-for-34tqvwlzaf,"Abstract: Analyzing the pattern of semantic variation in long real-world texts such as books or transcripts is interesting from the stylistic, cognitive, and linguistic perspectives. It is also useful for applications such as text segmentation, document summarization, and detection of semantic novelty. The recent emergence of several vector-space methods for sentence embedding has made such analysis feasible. However, this raises the issue of how consistent and meaningful the semantic representations produced by various methods are in themselves. In this paper, we compare several recent sentence embedding methods via time-series of semantic similarity between successive sentences and matrices of pairwise sentence similarity for multiple books of literature. In contrast to previous work using target tasks and curated datasets to compare sentence embedding methods, our approach provides an evaluation of the methods 'in the wild'. We find that most of the sentence embedding methods considered do infer highly correlated patterns of semantic similarity in a given document, but show interesting differences."
(4) Semantic Similarity for Text Comparison between Textual Documents or Sentences,https://typeset.io/papers/semantic-similarity-for-text-comparison-between-textual-58niim2vud,"Abstract: The inclusion of semantic information into any similarity metric boosts its effectiveness and gives findings that are interpretable by humans for further inquiry. A similarity calculation approach only centered on word properties inside the text sometimes provides less precise findings. This document gives three methods that aims to focus on textual terms and incorporate semantic information into their feature vectors, thereby computing semantic similarities. These strategies are founded in both corpus and knowledge-based approaches, namely: cosine similarity using tf-idf vectors, cosine similarity employing word embeddings, and soft cosine similarity utilizing word embeddings. Among these three, cosine similarity utilizing tf-idf vectors is deemed the most proficient in spotting similarities among succinct texts. The texts found by this technology are easily comprehensible and can be easily employed in other applications for important information retrieval."
(5) A Spectral Learning Based Model to Evaluate Semantic Textual Similarity,https://typeset.io/papers/a-spectral-learning-based-model-to-evaluate-semantic-textual-27e17p2m,"Abstract: Abstract Semantic Textual Similarity (STS) is a task in NLP that compares two sentences in a sentence-pair and scores the relationship between them using the degree of semantic equivalence. It has wide applicability in various fields. Consequently, the research around the task is constantly evolving. The demand for new as well as improved methods is endless. Numerous methods have been proposed that largely belong to either unsupervised or supervised learning approaches. The model proposed here is fairly simple and provides a fresh take on this classification problem using spectral learning. The model does not engage a large labeled corpus or lexical database like most STS supervised and unsupervised methods. Although, supervised STS methods achieve an accuracy that outperforms humans in some cases, but are often held back due to a lack of interpretation of the features instrumental in molding the decision-making process. The proposed model on the other hand generates features (latent knowledge) that are easy to ascertain and have a mathematical foundation. Given a sentence pair, the work focuses on finding latent states and variables from each sentence and performs classification by generating a similarity score. The latent variables are a result of projections learned by performing Canonical Correlation Analysis (CCA) amongst the sentence pair. To perform matching and determine the similarity score, Cosine similarity and Word Mover’s Distance (WMD) are employed. The performance of the proposed model does exhibit an improvement over various sophisticated supervised techniques such as LSTM and BiLSTM."
(6) Semantic Similarity Evaluation Method Based on Text Generation Data Augmentation,https://typeset.io/papers/semantic-similarity-evaluation-method-based-on-text-3oi1s8wa,"Abstract: The similarity evaluation method based on neural network has achieved good results, but it has higher requirements on the scale and quality of the corpus. Based on this problem, this paper proposes a semantic similarity evaluation method based on text generation data augmentation. This method combines Seq2Seq with a masked language model for data augmentation, and uses the expanded data to fine-tune the pre-trained language model. The pre-trained language model and the Siamese network are combined to build a semantic similarity evaluation model. Finally, experiments were carried out on the standard sentence similarity evaluation data set SentEva12012-2016. Compared with the benchmark model, the Spearman correlation coefficient improved by 3.11%. Experiments show that the semantic similarity evaluation method based on data augmentation can effectively solve the problem of low accuracy due to lack of data."
(7) Semantic Similarity Evaluation Method Based on Text Generation Data Augmentation,https://typeset.io/papers/semantic-similarity-evaluation-method-based-on-text-1r869oqe,"Abstract: The similarity evaluation method based on neural network has achieved good results, but it has higher requirements on the scale and quality of the corpus. Based on this problem, this paper proposes a semantic similarity evaluation method based on text generation data augmentation. This method combines Seq2Seq with a masked language model for data augmentation, and uses the expanded data to fine-tune the pre-trained language model. The pre-trained language model and the Siamese network are combined to build a semantic similarity evaluation model. Finally, experiments were carried out on the standard sentence similarity evaluation data set SentEva12012-2016. Compared with the benchmark model, the Spearman correlation coefficient improved by 3.11%. Experiments show that the semantic similarity evaluation method based on data augmentation can effectively solve the problem of low accuracy due to lack of data."
(8) Evaluation of Semantic Answer Similarity Metrics,https://typeset.io/papers/evaluation-of-semantic-answer-similarity-metrics-1uflat77,"Abstract: There are several issues with the existing general machine translation or natural language generation evaluation metrics, and question-answering (QA) systems are indifferent in that context. To build robust QA systems, we need the ability to have equivalently robust evaluation systems to verify whether model predictions to questions are similar to ground-truth annotations. The ability to compare similarity based on semantics as opposed to pure string overlap is important to compare models fairly and to indicate more realistic acceptance criteria in real-life applications. We build upon the first to our knowledge paper that uses transformer-based model metrics to assess semantic answer similarity and achieve higher correlations to human judgement in the case of no lexical overlap. We propose cross-encoder augmented bi-encoder and BERTScore models for semantic answer similarity, trained on a new dataset consisting of name pairs of US-American public figures. As far as we are concerned, we provide the first dataset of co-referent name string pairs along with their similarities, which can be used both for training and as a benchmark."
(9) Evaluation of Semantic Answer Similarity Metrics,https://typeset.io/papers/evaluation-of-semantic-answer-similarity-metrics-2pgqb2in,"Abstract: There are several issues with the existing general machine translation or natural language generation evaluation metrics, and question-answering (QA) systems are indifferent in that context. To build robust QA systems, we need the ability to have equivalently robust evaluation systems to verify whether model predictions to questions are similar to ground-truth annotations. The ability to compare similarity based on semantics as opposed to pure string overlap is important to compare models fairly and to indicate more realistic acceptance criteria in real-life applications. We build upon the first to our knowledge paper that uses transformer-based model metrics to assess semantic answer similarity and achieve higher correlations to human judgement in the case of no lexical overlap. We propose cross-encoder augmented bi-encoder and BERTScore models for semantic answer similarity, trained on a new dataset consisting of name pairs of US-American public figures. As far as we are concerned, we provide the first dataset of co-referent name string pairs along with their similarities, which can be used both for training and as a benchmark."
(10) Best Practices in Active Learning for Semantic Segmentation,https://typeset.io/papers/best-practices-in-active-learning-for-semantic-segmentation-3ln9jucqmy,"Abstract: Active learning is particularly of interest for semantic segmentation, where annotations are costly. Previous academic studies focused on datasets that are already very diverse and where the model is trained in a supervised manner with a large annotation budget. In contrast, data collected in many driving scenarios is highly redundant, and most medical applications are subject to very constrained annotation budgets. This work investigates the various types of existing active learning methods for semantic segmentation under diverse conditions across three dimensions - data distribution w.r.t. different redundancy levels, integration of semi-supervised learning, and different labeling budgets. We find that these three underlying factors are decisive for the selection of the best active learning approach. As an outcome of our study, we provide a comprehensive usage guide to obtain the best performance for each case. It is the first systematic study that investigates these dimensions covering a wide range of settings including more than 3K model training runs. In this work, we also propose an exemplary evaluation task for driving scenarios, where data has high redundancy, to showcase the practical implications of our research findings."
