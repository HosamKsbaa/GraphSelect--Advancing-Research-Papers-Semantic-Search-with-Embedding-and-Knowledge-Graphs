Title,Takeaway,Authors,Year,Citations,Abstract,Study Type,Journal,Journal SJR Quartile,DOI,Consensus Link
"DeeLM: Dependency-enhanced Large Language Model for Sentence Embeddings","DeeLM improves sentence embeddings by learning backward dependencies, outperforming baselines and achieving state-of-the-art performance across various semantic textual similarity tasks.","Xianming Li, Jing Li",2023,0,"Recent studies have proposed using large language models (LLMs) for sentence embeddings. However, most existing LLMs are built with an autoregressive architecture that primarily captures forward dependencies while neglecting backward dependencies. Previous work has highlighted the importance of backward dependencies in improving sentence embeddings. To address this issue, in this paper, we first present quantitative evidence demonstrating the limited learning of backward dependencies in LLMs. Then, we propose a novel approach called Dependency-Enhanced Large Language Model (DeeLM) to improve sentence embeddings. Specifically, we found a turning point in LLMs, where surpassing specific LLM layers leads to a significant performance drop in the semantic textual similarity (STS) task. STS is a crucial task for evaluating sentence embeddings. We then extract the layers after the turning point to make them bidirectional, allowing for the learning of backward dependencies. Extensive experiments demonstrate that DeeLM outperforms baselines and achieves state-of-the-art performance across various STS tasks.","","ArXiv","","10.48550/arXiv.2311.05296","https://consensus.app/papers/deelm-dependencyenhanced-large-language-model-sentence-li/3d98508848895c4d8c216a8b65072fe7/"
"RelBERT: Embedding Relations with Language Models","RelBERT, a small language model, can effectively capture relational similarity and outperform larger models in analogy benchmarks, even with limited training data.","Asahi Ushio, José Camacho-Collados, S. Schockaert",2023,0,"Many applications need access to background knowledge about how different concepts and entities are related. Although Knowledge Graphs (KG) and Large Language Models (LLM) can address this need to some extent, KGs are inevitably incomplete and their relational schema is often too coarse-grained, while LLMs are inefficient and difficult to control. As an alternative, we propose to extract relation embeddings from relatively small language models. In particular, we show that masked language models such as RoBERTa can be straightforwardly fine-tuned for this purpose, using only a small amount of training data. The resulting model, which we call RelBERT, captures relational similarity in a surprisingly fine-grained way, allowing us to set a new state-of-the-art in analogy benchmarks. Crucially, RelBERT is capable of modelling relations that go well beyond what the model has seen during training. For instance, we obtained strong results on relations between named entities with a model that was only trained on lexical relations between concepts, and we observed that RelBERT can recognise morphological analogies despite not being trained on such examples. Overall, we find that RelBERT significantly outperforms strategies based on prompting language models that are several orders of magnitude larger, including recent GPT-based models and open source models.","","ArXiv","","10.48550/arXiv.2310.00299","https://consensus.app/papers/relbert-embedding-relations-language-models-ushio/895f7b50ef905381b69cf4ee473a4b6e/"
"Generating Datasets with Pretrained Language Models","Pretrained language models can generate high-quality sentence embeddings without needing labeled data or finetuning, outperforming strong baselines on semantic textual similarity datasets.","Timo Schick, Hinrich Schütze",2021,140,"To obtain high-quality sentence embeddings from pretrained language models (PLMs), they must either be augmented with additional pretraining objectives or finetuned on a large set of labeled text pairs. While the latter approach typically outperforms the former, it requires great human effort to generate suitable datasets of sufficient size. In this paper, we show how PLMs can be leveraged to obtain high-quality sentence embeddings without the need for labeled data, finetuning or modifications to the pretraining objective: We utilize the generative abilities of large and high-performing PLMs to generate entire datasets of labeled text pairs from scratch, which we then use for finetuning much smaller and more efficient models. Our fully unsupervised approach outperforms strong baselines on several semantic textual similarity datasets.","","ArXiv","","10.18653/v1/2021.emnlp-main.555","https://consensus.app/papers/generating-datasets-pretrained-language-models-schick/7dd49068de735c99a2ad8b8ee0e8ab33/"
"Multi-sense embeddings through a word sense disambiguation process","Most Suitable Sense Annotation (MSSA) effectively disambiguates and annotates words by their specific senses, improving natural language understanding and outperforming more complex state-of-the-art systems.","Terry Ruas, W. Grosky, Akiko Aizawa",2019,33,"Abstract Natural Language Understanding has seen an increasing number of publications in the last few years, especially after robust word embeddings models became prominent, when they proved themselves able to capture and represent semantic relationships from massive amounts of data. Nevertheless, traditional models often fall short in intrinsic issues of linguistics, such as polysemy and homonymy. Any expert system that makes use of natural language in its core, can be affected by a weak semantic representation of text, resulting in inaccurate outcomes based on poor decisions. To mitigate such issues, we propose a novel approach called Most Suitable Sense Annotation (MSSA) , that disambiguates and annotates each word by its specific sense, considering the semantic effects of its context. Our approach brings three main contributions to the semantic representation scenario: (i) an unsupervised technique that disambiguates and annotates words by their senses, (ii) a multi-sense embeddings model that can be extended to any traditional word embeddings algorithm, and (iii) a recurrent methodology that allows our models to be re-used and their representations refined. We test our approach on six different benchmarks for the word similarity task, showing that our approach can produce state-of-the-art results and outperforms several more complex state-of-the-art systems.","","ArXiv","1","10.1016/j.eswa.2019.06.026","https://consensus.app/papers/multisense-embeddings-word-sense-disambiguation-process-ruas/368407071c955ef2b8ac41ca238f489d/"
"Scaling Sentence Embeddings with Large Language Models","In-context learning enables large language models to generate high-quality sentence embeddings, achieving comparable performance to current contrastive learning methods.","Ting Jiang, Shaohan Huang, Zhongzhi Luan, Deqing Wang, Fuzhen Zhuang",2023,2,"Large language models (LLMs) have recently garnered significant interest. With in-context learning, LLMs achieve impressive results in various natural language tasks. However, the application of LLMs to sentence embeddings remains an area of ongoing research. In this work, we propose an in-context learning-based method aimed at improving sentence embeddings performance. Our approach involves adapting the previous prompt-based representation method for autoregressive models, constructing a demonstration set that enables LLMs to perform in-context learning, and scaling up the LLMs to different model sizes. Through extensive experiments, in-context learning enables LLMs to generate high-quality sentence embeddings without any fine-tuning. It helps LLMs achieve performance comparable to current contrastive learning methods. By scaling model size, we find scaling to more than tens of billion parameters harms the performance on semantic textual similarity (STS) tasks. However, the largest model outperforms other counterparts and achieves the new state-of-the-art result on transfer tasks. We also fine-tune LLMs with current contrastive learning approach, and the 2.7B OPT model, incorporating our prompt-based method, surpasses the performance of 4.8B ST5, achieving the new state-of-the-art results on STS tasks. Our code is available at https://github.com/kongds/scaling_sentemb.","","ArXiv","","10.48550/arXiv.2307.16645","https://consensus.app/papers/scaling-sentence-embeddings-large-language-models-jiang/f5c6fb92876653a1a2cb0b6f3eac1fe2/"
"Distilling Semantic Concept Embeddings from Contrastively Fine-Tuned Language Models","Contrastively fine-tuned language models, such as ConceptNet, significantly improve the quality of semantic concept embeddings, outperforming existing methods in various tasks.","N. Li, Hanane Kteich, Zied Bouraoui, S. Schockaert",2023,3,"Learning vectors that capture the meaning of concepts remains a fundamental challenge. Somewhat surprisingly, perhaps, pre-trained language models have thus far only enabled modest improvements to the quality of such concept embeddings. Current strategies for using language models typically represent a concept by averaging the contextualised representations of its mentions in some corpus. This is potentially sub-optimal for at least two reasons. First, contextualised word vectors have an unusual geometry, which hampers downstream tasks. Second, concept embeddings should capture the semantic properties of concepts, whereas contextualised word vectors are also affected by other factors. To address these issues, we propose two contrastive learning strategies, based on the view that whenever two sentences reveal similar properties, the corresponding contextualised vectors should also be similar. One strategy is fully unsupervised, estimating the properties which are expressed in a sentence from the neighbourhood structure of the contextualised word embeddings. The second strategy instead relies on a distant supervision signal from ConceptNet. Our experimental results show that the resulting vectors substantially outperform existing concept embeddings in predicting the semantic properties of concepts, with the ConceptNet-based strategy achieving the best results. These findings are furthermore confirmed in a clustering task and in the downstream task of ontology completion.","","Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval","","10.1145/3539618.3591667","https://consensus.app/papers/distilling-semantic-concept-embeddings-contrastively-li/d530195b336a5e098a17508298784b43/"
"Learning Composition Models for Phrase Embeddings","Phrase embeddings can improve language modeling and semantic similarity tasks by learning to compose word embeddings using phrase structure and context features.","Mo Yu, Mark Dredze",2015,81,"Lexical embeddings can serve as useful representations for words for a variety of NLP tasks, but learning embeddings for phrases can be challenging. While separate embeddings are learned for each word, this is infeasible for every phrase. We construct phrase embeddings by learning how to compose word embeddings using features that capture phrase structure and context. We propose efficient unsupervised and task-specific learning objectives that scale our model to large datasets. We demonstrate improvements on both language modeling and several phrase semantic similarity tasks with various phrase lengths. We make the implementation of our model and the datasets available for general use.","","Transactions of the Association for Computational Linguistics","1","10.1162/tacl_a_00135","https://consensus.app/papers/learning-composition-models-phrase-embeddings-yu/25229cbec15a5009b88e916f0ef92ff4/"
"On the Sentence Embeddings from BERT for Semantic Textual Similarity","Our BERT-flow method significantly improves semantic textual similarity performance by transforming anisotropic sentence embeddings to smooth and isotropic Gaussian distributions through normalizing flows.","Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, Lei Li",2020,301,"Pre-trained contextual representations like BERT have achieved great success in natural language processing. However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences. In this paper, we argue that the semantic information in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks. The code is available at https://github.com/bohanli/BERT-flow.","","ArXiv","","10.18653/v1/2020.emnlp-main.733","https://consensus.app/papers/sentence-embeddings-bert-semantic-textual-similarity-li/292786363470552592656223a00da074/"
"SensEmbed: Learning Sense Embeddings for Word and Relational Similarity","SensEmbed transforms word embeddings to sense level and leverages knowledge from a large semantic network for effective semantic similarity measurement in NLP tasks.","Ignacio Iacobacci, Mohammad Taher Pilehvar, Roberto Navigli",2015,290,"Word embeddings have recently gained considerable popularity for modeling words in different Natural Language Processing (NLP) tasks including semantic similarity measurement. However, notwithstanding their success, word embeddings are by their very nature unable to capture polysemy, as different meanings of a word are conflated into a single representation. In addition, their learning process usually relies on massive corpora only, preventing them from taking advantage of structured knowledge. We address both issues by proposing a multifaceted approach that transforms word embeddings to the sense level and leverages knowledge from a large semantic network for effective semantic similarity measurement. We evaluate our approach on word similarity and relational similarity frameworks, reporting state-of-the-art performance on multiple datasets.","","","","10.3115/v1/P15-1010","https://consensus.app/papers/sensembed-learning-sense-embeddings-word-relational-iacobacci/e9e8ce59aa6e51a4ae2e9af0d4033226/"
"Enhancing Clinical Concept Extraction with Contextual Embedding","Contextual embeddings, trained on a large clinical corpus, achieve new state-of-the-art performance in clinical concept extraction, capturing valuable semantic information not accounted for by traditional word representations.","Yuqi Si, Jingqi Wang, Hua Xu, Kirk Roberts",2019,240,"OBJECTIVE
Neural network-based representations (""embeddings"") have dramatically advanced natural language processing (NLP) tasks, including clinical NLP tasks such as concept extraction. Recently, however, more advanced embedding methods and representations (eg, ELMo, BERT) have further pushed the state of the art in NLP, yet there are no common best practices for how to integrate these representations into clinical tasks. The purpose of this study, then, is to explore the space of possible options in utilizing these new models for clinical concept extraction, including comparing these to traditional word embedding methods (word2vec, GloVe, fastText).


MATERIALS AND METHODS
Both off-the-shelf, open-domain embeddings and pretrained clinical embeddings from MIMIC-III (Medical Information Mart for Intensive Care III) are evaluated. We explore a battery of embedding methods consisting of traditional word embeddings and contextual embeddings and compare these on 4 concept extraction corpora: i2b2 2010, i2b2 2012, SemEval 2014, and SemEval 2015. We also analyze the impact of the pretraining time of a large language model like ELMo or BERT on the extraction performance. Last, we present an intuitive way to understand the semantic information encoded by contextual embeddings.


RESULTS
Contextual embeddings pretrained on a large clinical corpus achieves new state-of-the-art performances across all concept extraction tasks. The best-performing model outperforms all state-of-the-art methods with respective F1-measures of 90.25, 93.18 (partial), 80.74, and 81.65.


CONCLUSIONS
We demonstrate the potential of contextual embeddings through the state-of-the-art performance these methods achieve on clinical concept extraction. Additionally, we demonstrate that contextual embeddings encode valuable semantic information not accounted for in traditional word representations.","","Journal of the American Medical Informatics Association : JAMIA","1","10.1093/jamia/ocz096","https://consensus.app/papers/enhancing-concept-extraction-contextual-embedding-si/3f78f3b7322851408a2132a4b50afe7d/"
