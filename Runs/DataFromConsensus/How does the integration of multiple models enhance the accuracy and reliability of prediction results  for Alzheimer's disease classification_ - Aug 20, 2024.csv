Title,Takeaway,Authors,Year,Citations,Abstract,Study Type,Journal,Journal SJR Quartile,DOI,Consensus Link
"Multimodal deep learning models for early detection of Alzheimer’s disease stage","Deep learning models combining imaging, genetic, and clinical data improve early detection of Alzheimer's disease stages.","Janani Venugopalan, L. Tong, H. Hassanzadeh, May D. Wang",2021,231,"Most current Alzheimer’s disease (AD) and mild cognitive disorders (MCI) studies use single data modality to make predictions such as AD stages. The fusion of multiple data modalities can provide a holistic view of AD staging analysis. Thus, we use deep learning (DL) to integrally analyze imaging (magnetic resonance imaging (MRI)), genetic (single nucleotide polymorphisms (SNPs)), and clinical test data to classify patients into AD, MCI, and controls (CN). We use stacked denoising auto-encoders to extract features from clinical and genetic data, and use 3D-convolutional neural networks (CNNs) for imaging data. We also develop a novel data interpretation method to identify top-performing features learned by the deep-models with clustering and perturbation analysis. Using Alzheimer’s disease neuroimaging initiative (ADNI) dataset, we demonstrate that deep models outperform shallow models, including support vector machines, decision trees, random forests, and k-nearest neighbors. In addition, we demonstrate that integrating multi-modality data outperforms single modality models in terms of accuracy, precision, recall, and meanF1 scores. Our models have identified hippocampus, amygdala brain areas, and the Rey Auditory Verbal Learning Test (RAVLT) as top distinguished features, which are consistent with the known AD literature.","","Scientific Reports","1","10.1038/s41598-020-74399-w","https://consensus.app/papers/multimodal-learning-models-detection-alzheimer-disease-venugopalan/0f1cc4babfda53d1a5f622c960d0071e/"
"A multilayer multimodal detection and prediction model based on explainable artificial intelligence for Alzheimer’s disease","This two-layer model accurately diagnoses Alzheimer's disease and detects progression, providing trustworthy, accountable, and medically applicable results for physicians.","Shaker El-Sappagh, J. M. Alonso, S. Islam, Ahmad M Sultan, K. Kwak",2021,103,"Alzheimer’s disease (AD) is the most common type of dementia. Its diagnosis and progression detection have been intensively studied. Nevertheless, research studies often have little effect on clinical practice mainly due to the following reasons: (1) Most studies depend mainly on a single modality, especially neuroimaging; (2) diagnosis and progression detection are usually studied separately as two independent problems; and (3) current studies concentrate mainly on optimizing the performance of complex machine learning models, while disregarding their explainability. As a result, physicians struggle to interpret these models, and feel it is hard to trust them. In this paper, we carefully develop an accurate and interpretable AD diagnosis and progression detection model. This model provides physicians with accurate decisions along with a set of explanations for every decision. Specifically, the model integrates 11 modalities of 1048 subjects from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) real-world dataset: 294 cognitively normal, 254 stable mild cognitive impairment (MCI), 232 progressive MCI, and 268 AD. It is actually a two-layer model with random forest (RF) as classifier algorithm. In the first layer, the model carries out a multi-class classification for the early diagnosis of AD patients. In the second layer, the model applies binary classification to detect possible MCI-to-AD progression within three years from a baseline diagnosis. The performance of the model is optimized with key markers selected from a large set of biological and clinical measures. Regarding explainability, we provide, for each layer, global and instance-based explanations of the RF classifier by using the SHapley Additive exPlanations (SHAP) feature attribution framework. In addition, we implement 22 explainers based on decision trees and fuzzy rule-based systems to provide complementary justifications for every RF decision in each layer. Furthermore, these explanations are represented in natural language form to help physicians understand the predictions. The designed model achieves a cross-validation accuracy of 93.95% and an F1-score of 93.94% in the first layer, while it achieves a cross-validation accuracy of 87.08% and an F1-Score of 87.09% in the second layer. The resulting system is not only accurate, but also trustworthy, accountable, and medically applicable, thanks to the provided explanations which are broadly consistent with each other and with the AD medical literature. The proposed system can help to enhance the clinical understanding of AD diagnosis and progression processes by providing detailed insights into the effect of different modalities on the disease risk.","","Scientific Reports","1","10.1038/s41598-021-82098-3","https://consensus.app/papers/multilayer-detection-prediction-model-based-elsappagh/3b4531fda35f55cdab0bd31dc0a35317/"
"Voting Ensemble Approach for Enhancing Alzheimer’s Disease Classification","The ensemble voting method, combining multiple individual classifier predictions, improves Alzheimer's disease classification accuracy and reliability.","Subhajit Chatterjee, Y. Byun",2022,5,"Alzheimer’s disease is dementia that impairs one’s thinking, behavior, and memory. It starts as a moderate condition affecting areas of the brain that make it challenging to retain recently learned information, causes mood swings, and causes confusion regarding occasions, times, and locations. The most prevalent type of dementia, called Alzheimer’s disease (AD), causes memory-related problems in patients. A precise medical diagnosis that correctly classifies AD patients results in better treatment. Currently, the most commonly used classification techniques extract features from longitudinal MRI data before creating a single classifier that performs classification. However, it is difficult to train a reliable classifier to achieve acceptable classification performance due to limited sample size and noise in longitudinal MRI data. Instead of creating a single classifier, we propose an ensemble voting method that generates multiple individual classifier predictions and then combines them to develop a more accurate and reliable classifier. The ensemble voting classifier model performs better in the Open Access Series of Imaging Studies (OASIS) dataset for older adults than existing methods in important assessment criteria such as accuracy, sensitivity, specificity, and AUC. For the binary classification of with dementia and no dementia, an accuracy of 96.4% and an AUC of 97.2% is attained.","","Sensors (Basel, Switzerland)","1","10.3390/s22197661","https://consensus.app/papers/voting-approach-enhancing-alzheimer-disease-chatterjee/c4778d8884cf5c56a0ef17783e557f5a/"
"A Stacking Framework for Multi-Classification of Alzheimer's Disease Using Neuroimaging and Clinical Features.","The Stacking framework, combining neuroimaging and clinical features, significantly improves Alzheimer's disease diagnosis accuracy compared to using only sMRI data alone.","Durong Chen, Fuliang Yi, Yao Qin, Jiajia Zhang, Xiaoyan Ge, Hongjuan Han, Jing Cui, Wenlin Bai, Yan Wu, Hongmei Yu",2022,2,"BACKGROUND
Alzheimer's disease (AD) is a severe health problem. Challenges still remain in early diagnosis.


OBJECTIVE
The objective of this study was to build a Stacking framework for multi-classification of AD by a combination of neuroimaging and clinical features to improve the performance.


METHODS
The data we used were from the Alzheimer's Disease Neuroimaging Initiative database with a total of 493 subjects, including 125 normal control (NC), 121 early mild cognitive impairment, 109 late mild cognitive impairment (LMCI), and 138 AD. We selected structural magnetic resonance imaging (sMRI) features by voting strategy. The imaging features, demographic information, Mini-Mental State Examination, and Alzheimer's Disease Assessment Scale-Cognitive Subscale were combined together as classification features. We proposed a two-layer Stacking ensemble framework to classify four types of people. The first layer represented support vector machine, random forests, adaptive boosting, and gradient boosting decision tree; the second layer was a logistic regression classifier. Additionally, we analyzed performance of only sMRI feature and combined features and compared the proposed model with four base classifiers.


RESULTS
The Stacking model combined with sMRI and non-imaging features outshined four base classifiers with an average accuracy of 86.96% . Compared with using sMRI data alone, sMRI combined with non-imaging features significantly improved diagnostic accuracy, especially in NC versus LMCI and LMCI versus AD by 14.08% .


CONCLUSION
The Stacking framework we used can improve performance in diagnosis of AD using combined features.","non-rct observational study","Journal of Alzheimer's disease : JAD","1","10.3233/jad-215654","https://consensus.app/papers/stacking-framework-multiclassification-alzheimers-chen/21bcd3e3b5de5448b3ee73ae03c83134/"
"Multi-modal neuroimaging feature selection with consistent metric constraint for diagnosis of Alzheimer's disease","The proposed multi-modal neuroimaging feature selection method with consistent metric constraint (MFCC) improves Alzheimer's disease diagnosis accuracy and discovers disease-related characteristics.","Xiaoke Hao, Yong Bao, Yingchun Guo, Ming Yu, Daoqiang Zhang, S. Risacher, A. Saykin, Xiaohui Yao, Li Shen",2019,93,"The accurate diagnosis of Alzheimer's disease (AD) and its early stage, e.g., mild cognitive impairment (MCI), is essential for timely treatment or possible intervention to slow down AD progression. Recent studies have demonstrated that multiple neuroimaging and biological measures contain complementary information for diagnosis and prognosis. Therefore, information fusion strategies with multi-modal neuroimaging data, such as voxel-based measures extracted from structural MRI (VBM-MRI) and fluorodeoxyglucose positron emission tomography (FDG-PET), have shown their effectiveness for AD diagnosis. However, most existing methods are proposed to simply integrate the multi-modal data, but do not make full use of structure information across the different modalities. In this paper, we propose a novel multi-modal neuroimaging feature selection method with consistent metric constraint (MFCC) for AD analysis. First, the similarity is calculated for each modality (i.e. VBM-MRI or FDG-PET) individually by random forest strategy, which can extract pairwise similarity measures for multiple modalities. Then the group sparsity regularization term and the sample similarity constraint regularization term are used to constrain the objective function to conduct feature selection from multiple modalities. Finally, the multi-kernel support vector machine (MK-SVM) is used to fuse the features selected from different models for final classification. The experimental results on the Alzheimer's Disease Neuroimaging Initiative (ADNI) show that the proposed method has better classification performance than the start-of-the-art multimodality-based methods. Specifically, we achieved higher accuracy and area under the curve (AUC) for AD versus normal controls (NC), MCI versus NC, and MCI converters (MCI-C) versus MCI non-converters (MCI-NC) on ADNI datasets. Therefore, the proposed model not only outperforms the traditional method in terms of AD/MCI classification, but also discovers the characteristics associated with the disease, demonstrating its promise for improving disease-related mechanistic understanding.","","Medical image analysis","1","10.1016/j.media.2019.101625","https://consensus.app/papers/multimodal-feature-selection-constraint-diagnosis-hao/1fa88dec14725756bc4059d2759168e9/"
"Multi-model and multi-slice ensemble learning architecture based on 2D convolutional neural networks for Alzheimer's disease diagnosis","The proposed ensemble learning architecture based on 2D CNNs provides an effective, accurate, and automatic diagnosis of Alzheimer's disease, even with limited neuroimaging data.","Wenjie Kang, Lan Lin, Baiwen Zhang, Xiaoqi Shen, Shuicai Wu",2021,52,"Alzheimer's Disease (AD) is a chronic neurodegenerative disease without effective medications or supplemental treatments. Thus, predicting AD progression is crucial for clinical practice and medical research. Due to limited neuroimaging data, two-dimensional convolutional neural networks (2D CNNs) have been commonly adopted to differentiate among cognitively normal subjects (CN), people with mild cognitive impairment (MCI), and AD patients. Therefore, this paper proposes an ensemble learning (EL) architecture based on 2D CNNs, using a multi-model and multi-slice ensemble. First, the top 11 coronal slices of grey matter density maps for AD versus CN classifications were selected. Second, the discriminator of a generative adversarial network, VGG16, and ResNet50 were trained with the selected slices, and the majority voting scheme was used to merge the multi-slice decisions of each model. Afterwards, those three classifiers were used to construct an ensemble model. Multi-slice ensemble learning was designed to obtain spatial features, while multi-model integration reduced the prediction error rate. Finally, transfer learning was used in domain adaptation to refine those CNNs, moving them from working solely with AD versus CN classifications to being applicable to other tasks. This ensemble approach achieved accuracy values of 90.36%, 77.19%, and 72.36% when classifying AD versus CN, AD versus MCI, and MCI versus CN, respectively. Compared with other state-of-the-art 2D studies, the proposed approach provides an effective, accurate, automatic diagnosis along the AD continuum. This technique may enhance AD diagnostics when the sample size is limited.","","Computers in biology and medicine","1","10.1016/j.compbiomed.2021.104678","https://consensus.app/papers/multimodel-multislice-learning-architecture-based-kang/690b29bb1e045a308316364de7ba0544/"
"Deep ensemble learning for Alzheimers disease classification","The proposed deep ensemble learning framework improves Alzheimer's disease classification accuracy by 4% compared to six well-known approaches, offering more accurate diagnostic services.","Ning An, Huitong Ding, Jiaoyun Yang, R. Au, T. F. Ang",2019,90,"Ensemble1learning uses multiple algorithms to obtain better predictive performance than any single one of its constituent algorithms could. With the growing popularity of deep learning technologies, researchers have started to ensemble these technologies for various purposes. Few, if any, however, have used the deep learning approach as a means to ensemble Alzheimer's disease classification algorithms. This paper presents a deep ensemble learning framework that aims to harness deep learning algorithms to integrate multisource data and tap the 'wisdom of experts.' At the voting layer, two sparse autoencoders are trained for feature learning to reduce the correlation of attributes and diversify the base classifiers ultimately. At the stacking layer, a nonlinear feature-weighted method based on a deep belief network is proposed to rank the base classifiers, which may violate the conditional independence. The neural network is used as a meta classifier. At the optimizing layer, over-sampling and threshold-moving are used to cope with the cost-sensitive problem. Optimized predictions are obtained based on an ensemble of probabilistic predictions by similarity calculation. The proposed deep ensemble learning framework is used for Alzheimer's disease classification. Experiments with the clinical dataset from National Alzheimer's Coordinating Center demonstrate that the classification accuracy of our proposed framework is 4% better than six well-known ensemble approaches, including the standard stacking algorithm as well. Adequate coverage of more accurate diagnostic services can be provided by utilizing the wisdom of averaged physicians. This paper points out a new way to boost the primary care of Alzheimer's disease from the view of machine learning.","","Journal of biomedical informatics","1","10.1016/j.jbi.2020.103411","https://consensus.app/papers/deep-learning-alzheimers-disease-classification-an/e20c022437b45f3b9ca49d83bd6078e8/"
"A multi-model deep convolutional neural network for automatic hippocampus segmentation and classification in Alzheimer’s disease","The proposed multi-model deep learning framework using convolutional neural networks effectively segmented the hippocampus and accurately classified Alzheimer's disease and mild cognitive impairment using structural MRI data.","Manhua Liu, Fan Li, Hao Yan, Kundong Wang, Yixin Ma, L. Shen, Mingqing Xu",2019,255,"Alzheimer's disease (AD) is a progressive and irreversible brain degenerative disorder. Mild cognitive impairment (MCI) is a clinical precursor of AD. Although some treatments can delay its progression, no effective cures are available for AD. Accurate early-stage diagnosis of AD is vital for the prevention and intervention of the disease progression. Hippocampus is one of the first affected brain regions in AD. To help AD diagnosis, the shape and volume of the hippocampus are often measured using structural magnetic resonance imaging (MRI). However, these features encode limited information and may suffer from segmentation errors. Additionally, the extraction of these features is independent of the classification model, which could result in sub-optimal performance. In this study, we propose a multi-model deep learning framework based on convolutional neural network (CNN) for joint automatic hippocampal segmentation and AD classification using structural MRI data. Firstly, a multi-task deep CNN model is constructed for jointly learning hippocampal segmentation and disease classification. Then, we construct a 3D Densely Connected Convolutional Networks (3D DenseNet) to learn features of the 3D patches extracted based on the hippocampal segmentation results for the classification task. Finally, the learned features from the multi-task CNN and DenseNet models are combined to classify disease status. Our method is evaluated on the baseline T1-weighted structural MRI data collected from 97 AD, 233 MCI, 119 Normal Control (NC) subjects in the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. The proposed method achieves a dice similarity coefficient of 87.0% for hippocampal segmentation. In addition, the proposed method achieves an accuracy of 88.9% and an AUC (area under the ROC curve) of 92.5% for classifying AD vs. NC subjects, and an accuracy of 76.2% and an AUC of 77.5% for classifying MCI vs. NC subjects. Our empirical study also demonstrates that the proposed multi-model method outperforms the single-model methods and several other competing methods.","","NeuroImage","1","10.1016/j.neuroimage.2019.116459","https://consensus.app/papers/multimodel-network-hippocampus-segmentation-liu/9d65e63bb0695d6facc7dadc61e52901/"
"Cascaded Multi-Modal Mixing Transformers for Alzheimer’s Disease Classification with Incomplete Data","The Multi-Modal Mixing Transformer (3MT) effectively classifies Alzheimer's disease using neuroimaging data, gender, age, and Mini-Mental State Examination scores, with state-of-the-art accuracy and full data utilization.","Linfeng Liu, Siyu Liu, Lu Zhang, X. To, F. Nasrallah, S. Chandra",2022,6,"Accurate medical classification requires a large number of multi-modal data, and in many cases, in different formats. Previous studies have shown promising results when using multi-modal data, outperforming single-modality models on when classifying disease such as Alzheimer’s disease (AD). However, those models are usually not flexible enough to handle missing modalities. Currently, the most common workaround is excluding samples with missing modalities which leads to considerable data under-utilisation. Adding to the fact that labelled medical images are already scarce, the performance of data-driven methods like deep learning is severely hampered. Therefore, a multi-modal method that can gracefully handle missing data in various clinical settings is highly desirable. In this paper, we present the Multi-Modal Mixing Transformer (3MT), a novel Transformer for disease classification based on multi-modal data. In this work, we test it for Alzheimer’s Disease (AD) or Cognitively normal (CN) classification using neuroimaging data, gender, age and Mini-Mental State Examination (MMSE) scores. The model uses a novel Cascaded Modality Transformers architecture with cross-attention to incorporate multi-modal information for more informed predictions. Auxiliary outputs and a novel modality dropout mechanism were incorporated to ensure an unprecedented level of modality independence and robustness. The result is a versatile network that enables the mixing of an unlimited number of modalities with different formats and full data utilization: handling all combinations of missing data gracefully while upholding the classification performance. 3MT was first tested on the Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset and achieved stateof-the-art test accuracy of 0.987 ± 0.0006. To test its generalisability, 3MT was directly applied to The Australian Imaging Biomarker & Lifestyle Flagship Study of Ageing (AIBL) after training on the ADNI dataset, and achieved a test accuracy of 0.925 ± 0.0004 without fine-tuning. Finally, we show that Gradient-weighted Class Activation Mapping (Grad-CAM) visualizations are also possible with our model for explainable results.","","NeuroImage","1","10.1016/j.neuroimage.2023.120267","https://consensus.app/papers/cascaded-multimodal-mixing-transformers-alzheimer-liu/a2775992e13454f18b23de8c6644c18d/"
"An Attention-Based 3D CNN With Multi-Scale Integration Block for Alzheimer's Disease Classification","The Attention-based 3D Multi-scale CNN model (AMSNet) effectively classifies Alzheimer's Disease with fewer parameters and lower computational load, outperforming seven comparative models.","Yuanchen Wu, Yuan Zhou, Weiming Zeng, Qian Qian, Miao Song",2022,4,"Convolutional Neural Networks (CNNs) have recently been introduced to Alzheimer's Disease (AD) diagnosis. Despite their encouraging prospects, most of the existing models only process AD-related brain atrophy on a single spatial scale, and have high computational complexity. Here, we propose a novel Attention-based 3D Multi-scale CNN model (AMSNet), which can better capture and integrate multiple spatial-scale features of AD, with a concise structure. For the binary classification between 384 AD patients and 389 Cognitively Normal (CN) controls using sMRI scannings, AMSNet achieves remarkable overall performance (91.3% accuracy, 88.3% sensitivity, and 94.2% specificity) with fewer parameters and lower computational load, generally surpassing seven comparative models. Furthermore, AMSNet generalizes well in other AD-related classification tasks, such as the three-way classification (AD-MCI-CN). Our results manifest the feasibility and efficiency of the proposed multi-scale spatial feature integration and attention mechanism used in AMSNet for AD classification, and provide potential biomarkers to explore the neuropathological causes of AD.","","IEEE Journal of Biomedical and Health Informatics","1","10.1109/JBHI.2022.3197331","https://consensus.app/papers/attentionbased-with-multiscale-integration-block-wu/d0f256740b9f50568d70972d43e5d81c/"
