Title,Takeaway,Authors,Year,Citations,Abstract,Study Type,Journal,Journal SJR Quartile,DOI,Consensus Link
"Unifying Large Language Models and Knowledge Graphs: A Roadmap","Unifying large language models and knowledge graphs can enhance their abilities in natural language processing and artificial intelligence.","Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, Xindong Wu",2023,95,"Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolving by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.","","ArXiv","","10.48550/arXiv.2306.08302","https://consensus.app/papers/unifying-language-models-knowledge-graphs-roadmap-pan/6b1e377e73515d669415887e670803d0/"
"LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities","GPT-4 outperforms ChatGPT in most Knowledge Graph construction tasks, and AutoKG, a multi-agent-based approach, shows potential for future advancements in Knowledge Graph construction and reasoning.","Yuqi Zhu, Xiaohan Wang, Jing Chen, Shuofei Qiao, Yixin Ou, Yunzhi Yao, Shumin Deng, Huajun Chen, Ningyu Zhang",2023,17,"This paper presents an exhaustive quantitative and qualitative evaluation of Large Language Models (LLMs) for Knowledge Graph (KG) construction and reasoning. We employ eight distinct datasets that encompass aspects including entity, relation and event extraction, link prediction, and question answering. Empirically, our findings suggest that GPT-4 outperforms ChatGPT in the majority of tasks and even surpasses fine-tuned models in certain reasoning and question-answering datasets. Moreover, our investigation extends to the potential generalization ability of LLMs for information extraction, which culminates in the presentation of the Virtual Knowledge Extraction task and the development of the VINE dataset. Drawing on these empirical findings, we further propose AutoKG, a multi-agent-based approach employing LLMs for KG construction and reasoning, which aims to chart the future of this field and offer exciting opportunities for advancement. We anticipate that our research can provide invaluable insights for future undertakings of KG\footnote{Code and datasets will be available in https://github.com/zjunlp/AutoKG.","","ArXiv","","10.48550/arXiv.2305.13168","https://consensus.app/papers/llms-knowledge-graph-construction-reasoning-recent-zhu/bc301ddc6b135419a9743367f3b5545c/"
"From Answers to Insights: Unveiling the Strengths and Limitations of ChatGPT and Biomedical Knowledge Graphs","ChatGPT excels in providing existing information, but Biomedical Knowledge Graphs provide superior information reliability and support novel discoveries and reasoning.","Yu Hou, Jeremy Yeung, Huan Xu, Chang Su, Fei Wang, Rui Zhang",2023,1,"Large Language Models (LLMs) have demonstrated exceptional performance in various natural language processing tasks, utilizing their language generation capabilities and knowledge acquisition potential from unstructured text. However, when applied to the biomedical domain, LLMs encounter limitations, resulting in erroneous and inconsistent answers. Knowledge Graphs (KGs) have emerged as valuable resources for structured information representation and organization. Specifically, Biomedical Knowledge Graphs (BKGs) have attracted significant interest in managing large-scale and heterogeneous biomedical knowledge. This study evaluates the capabilities of ChatGPT and existing BKGs in question answering, knowledge discovery, and reasoning. Results indicate that while ChatGPT with GPT-4.0 surpasses both GPT-3.5 and BKGs in providing existing information, BKGs demonstrate superior information reliability. Additionally, ChatGPT exhibits limitations in performing novel discoveries and reasoning, particularly in establishing structured links between entities compared to BKGs. To overcome these limitations, future research should focus on integrating LLMs and BKGs to leverage their respective strengths. Such an integrated approach would optimize task performance and mitigate potential risks, thereby advancing knowledge in the biomedical field and contributing to overall well-being.","","medRxiv","","10.1101/2023.06.09.23291208","https://consensus.app/papers/from-answers-insights-unveiling-strengths-limitations-hou/b1b9f0476c90519da875234e2091199f/"
"MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models","MindMap prompting improves large language models' ability to incorporate new knowledge and elicit reasoning pathways, leading to better performance than traditional methods.","Yilin Wen, Zifeng Wang, Jimeng Sun",2023,6,"LLMs usually exhibit limitations in their ability to incorporate new knowledge, the generation of hallucinations, and the transparency of their decision-making process. In this paper, we explore how to prompt LLMs with knowledge graphs (KG), working as a remedy to engage LLMs with up-to-date knowledge and elicit the reasoning pathways from LLMs. Specifically, we build a prompting pipeline that endows LLMs with the capability of comprehending KG inputs and inferring with a combined implicit knowledge and the retrieved external knowledge. In addition, we investigate eliciting the mind map on which LLMs perform the reasoning and generate the answers. It is identified that the produced mind map exhibits the reasoning pathways of LLMs grounded on the ontology of knowledge, hence bringing the prospects of probing and gauging LLM inference in production. The experiments on three question&answering datasets also show that MindMap prompting leads to a striking empirical gain. For instance, prompting a GPT-3.5 with MindMap yields an overwhelming performance over GPT-4 consistently. We also demonstrate that with structured facts retrieved from KG, MindMap can outperform a series of prompting-with-document-retrieval methods, benefiting from more accurate, concise, and comprehensive knowledge from KGs. To reproduce our results and extend the framework further, we make our codebase available at https://github.com/wyl.willing/MindMap.","","ArXiv","","10.48550/arXiv.2308.09729","https://consensus.app/papers/mindmap-knowledge-graph-prompting-sparks-graph-thoughts-wen/a3d2917032f650a98955967bbf40fbe9/"
"ChatGPT is not Enough: Enhancing Large Language Models with Knowledge Graphs for Fact-aware Language Modeling","Adding knowledge graphs to large language models (KGLLMs) improves their factual reasoning ability and ability to generate knowledge-grounded content.","Lin F. Yang, Hongyang Chen, Zhao Li, Xiao Ding, Xindong Wu",2023,22,"Recently, ChatGPT, a representative large language model (LLM), has gained considerable attention due to its powerful emergent abilities. Some researchers suggest that LLMs could potentially replace structured knowledge bases like knowledge graphs (KGs) and function as parameterized knowledge bases. However, while LLMs are proficient at learning probabilistic language patterns based on large corpus and engaging in conversations with humans, they, like previous smaller pre-trained language models (PLMs), still have difficulty in recalling facts while generating knowledge-grounded contents. To overcome these limitations, researchers have proposed enhancing data-driven PLMs with knowledge-based KGs to incorporate explicit factual knowledge into PLMs, thus improving their performance to generate texts requiring factual knowledge and providing more informed responses to user queries. This paper reviews the studies on enhancing PLMs with KGs, detailing existing knowledge graph enhanced pre-trained language models (KGPLMs) as well as their applications. Inspired by existing studies on KGPLM, this paper proposes to enhance LLMs with KGs by developing knowledge graph-enhanced large language models (KGLLMs). KGLLM provides a solution to enhance LLMs' factual reasoning ability, opening up new avenues for LLM research.","literature review","ArXiv","","10.48550/arXiv.2306.11489","https://consensus.app/papers/chatgpt-enough-enhancing-large-language-models-knowledge-yang/cc93b3c5070b5e7ca9c8d60e215eade7/"
"KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models","KG-GPT, a multi-purpose framework leveraging large language models for complex reasoning tasks on knowledge graphs, performs competitively and robustly, outperforming fully-supervised models.","Jiho Kim, Yeonsu Kwon, Yohan Jo, Edward Choi",2023,1,"While large language models (LLMs) have made considerable advancements in understanding and generating unstructured text, their application in structured data remains underexplored. Particularly, using LLMs for complex reasoning tasks on knowledge graphs (KGs) remains largely untouched. To address this, we propose KG-GPT, a multi-purpose framework leveraging LLMs for tasks employing KGs. KG-GPT comprises three steps: Sentence Segmentation, Graph Retrieval, and Inference, each aimed at partitioning sentences, retrieving relevant graph components, and deriving logical conclusions, respectively. We evaluate KG-GPT using KG-based fact verification and KGQA benchmarks, with the model showing competitive and robust performance, even outperforming several fully-supervised models. Our work, therefore, marks a significant step in unifying structured and unstructured data processing within the realm of LLMs.","","ArXiv","","10.48550/arXiv.2310.11220","https://consensus.app/papers/kggpt-general-framework-reasoning-knowledge-graphs-using-kim/4b0441eb574255409550f1a6a4a98432/"
"Text2KGBench: A Benchmark for Ontology-Driven Knowledge Graph Generation from Text","Text2KGBench evaluates language models' capabilities in generating Knowledge Graphs from natural language text guided by an ontology, revealing room for improvement in fact extraction, ontology conformance, and hallucinations.","Nandana Mihindukulasooriya, S. Tiwari, Carlos F. Enguix, K. Lata",2023,6,"The recent advances in large language models (LLM) and foundation models with emergent capabilities have been shown to improve the performance of many NLP tasks. LLMs and Knowledge Graphs (KG) can complement each other such that LLMs can be used for KG construction or completion while existing KGs can be used for different tasks such as making LLM outputs explainable or fact-checking in Neuro-Symbolic manner. In this paper, we present Text2KGBench, a benchmark to evaluate the capabilities of language models to generate KGs from natural language text guided by an ontology. Given an input ontology and a set of sentences, the task is to extract facts from the text while complying with the given ontology (concepts, relations, domain/range constraints) and being faithful to the input sentences. We provide two datasets (i) Wikidata-TekGen with 10 ontologies and 13,474 sentences and (ii) DBpedia-WebNLG with 19 ontologies and 4,860 sentences. We define seven evaluation metrics to measure fact extraction performance, ontology conformance, and hallucinations by LLMs. Furthermore, we provide results for two baseline models, Vicuna-13B and Alpaca-LoRA-13B using automatic prompt generation from test cases. The baseline results show that there is room for improvement using both Semantic Web and Natural Language Processing techniques.","","ArXiv","","10.48550/arXiv.2308.02357","https://consensus.app/papers/text2kgbench-benchmark-ontologydriven-knowledge-graph-mihindukulasooriya/b24be0d0ff9f52eebfa7a23833492952/"
"LLM-assisted Knowledge Graph Engineering: Experiments with ChatGPT","ChatGPT can effectively support Knowledge Graph Engineering, assisting in the development and management of Knowledge Graphs across various domains.","Lars Meyer, Claus Stadler, Johannes Frey, Norman Radtke, K. Junghanns, R. Meissner, Gordian Dziwis, Kirill Bulert, Michael Martin",2023,7,"Knowledge Graphs (KG) provide us with a structured, flexible, transparent, cross-system, and collaborative way of organizing our knowledge and data across various domains in society and industrial as well as scientific disciplines. KGs surpass any other form of representation in terms of effectiveness. However, Knowledge Graph Engineering (KGE) requires in-depth experiences of graph structures, web technologies, existing models and vocabularies, rule sets, logic, as well as best practices. It also demands a significant amount of work. Considering the advancements in large language models (LLMs) and their interfaces and applications in recent years, we have conducted comprehensive experiments with ChatGPT to explore its potential in supporting KGE. In this paper, we present a selection of these experiments and their results to demonstrate how ChatGPT can assist us in the development and management of KGs.","","ArXiv","","10.48550/arXiv.2307.06917","https://consensus.app/papers/llmassisted-knowledge-graph-engineering-experiments-meyer/a96cef3948ef583c8f18befd067f7440/"
"Making Large Language Models Perform Better in Knowledge Graph Completion","Integrating structural information into large language models improves knowledge graph completion by enhancing their knowledge reasoning ability.","Yichi Zhang, Zhuo Chen, Wen Zhang, Hua-zeng Chen",2023,0,"Large language model (LLM) based knowledge graph completion (KGC) aims to predict the missing triples in the KGs with LLMs and enrich the KGs to become better web infrastructure, which can benefit a lot of web-based automatic services. However, research about LLM-based KGC is limited and lacks effective utilization of LLM's inference capabilities, which ignores the important structural information in KGs and prevents LLMs from acquiring accurate factual knowledge. In this paper, we discuss how to incorporate the helpful KG structural information into the LLMs, aiming to achieve structrual-aware reasoning in the LLMs. We first transfer the existing LLM paradigms to structural-aware settings and further propose a knowledge prefix adapter (KoPA) to fulfill this stated goal. KoPA employs structural embedding pre-training to capture the structural information of entities and relations in the KG. Then KoPA informs the LLMs of the knowledge prefix adapter which projects the structural embeddings into the textual space and obtains virtual knowledge tokens as a prefix of the input prompt. We conduct comprehensive experiments on these structural-aware LLM-based KGC methods and provide an in-depth analysis comparing how the introduction of structural information would be better for LLM's knowledge reasoning ability. Our code is released at https://github.com/zjukg/KoPA.","","ArXiv","","10.48550/arXiv.2310.06671","https://consensus.app/papers/making-language-models-perform-better-knowledge-graph-zhang/e3d24a6241125880ba3ac65748ad1300/"
"Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning","Reasoning on graphs (RoG) synergizes large language models with knowledge graphs to enable faithful and interpretable reasoning, improving their performance and trustworthiness in complex tasks.","Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, Shirui Pan",2023,10,"Large language models (LLMs) have demonstrated impressive reasoning abilities in complex tasks. However, they lack up-to-date knowledge and experience hallucinations during reasoning, which can lead to incorrect reasoning processes and diminish their performance and trustworthiness. Knowledge graphs (KGs), which capture vast amounts of facts in a structured format, offer a reliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM reasoning methods only treat KGs as factual knowledge bases and overlook the importance of their structural information for reasoning. In this paper, we propose a novel method called reasoning on graphs (RoG) that synergizes LLMs with KGs to enable faithful and interpretable reasoning. Specifically, we present a planning-retrieval-reasoning framework, where RoG first generates relation paths grounded by KGs as faithful plans. These plans are then used to retrieve valid reasoning paths from the KGs for LLMs to conduct faithful reasoning. Furthermore, RoG not only distills knowledge from KGs to improve the reasoning ability of LLMs through training but also allows seamless integration with any arbitrary LLMs during inference. Extensive experiments on two benchmark KGQA datasets demonstrate that RoG achieves state-of-the-art performance on KG reasoning tasks and generates faithful and interpretable reasoning results.","","ArXiv","","10.48550/arXiv.2310.01061","https://consensus.app/papers/reasoning-graphs-faithful-interpretable-large-language-luo/67bc2ddc11b15fde93e4e90f135f73b4/"
