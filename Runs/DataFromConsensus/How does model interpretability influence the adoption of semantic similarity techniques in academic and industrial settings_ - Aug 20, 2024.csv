Title,Takeaway,Authors,Year,Citations,Abstract,Study Type,Journal,Journal SJR Quartile,DOI,Consensus Link
"CrabNet for Explainable Deep Learning in Materials Science: Bridging the Gap Between Academia and Industry","CrabNet, a structure-agnostic deep learning model, improves model interpretability and enables meaningful element representations for materials science, bridging the gap between academia and industry.","A. Wang, Mahamad Salah Mahmoud, Mathias Czasny, A. Gurlo",2021,3,"Despite recent breakthroughs in deep learning for materials informatics, there exists a disparity between their popularity in academic research and their limited adoption in the industry. A significant contributor to this “interpretability-adoption gap” is the prevalence of black-box models and the lack of built-in methods for model interpretation. While established methods for evaluating model performance exist, an intuitive understanding of the modeling and decision-making processes in models is nonetheless desired in many cases. In this work, we demonstrate several ways of incorporating model interpretability to the structure-agnostic Compositionally Restricted Attention-Based network, CrabNet. We show that CrabNet learns meaningful, material property-specific element representations based solely on the data with no additional supervision. These element representations can then be used to explore element identity, similarity, behavior, and interactions within different chemical environments. Chemical compounds can also be uniquely represented and examined to reveal clear structures and trends within the chemical space. Additionally, visualizations of the attention mechanism can be used in conjunction to further understand the modeling process, identify potential modeling or dataset errors, and hint at further chemical insights leading to a better understanding of the phenomena governing material properties. We feel confident that the interpretability methods introduced in this work for CrabNet will be of keen interest to materials informatics researchers as well as industrial practitioners alike.","","Integrating Materials and Manufacturing Innovation","1","10.1007/s40192-021-00247-y","https://consensus.app/papers/crabnet-explainable-deep-learning-materials-science-wang/cf07fdc7d13854708cf9bd97f0c8ad89/"
"Automatic design of semantic similarity controllers based on fuzzy logics","Our strategy for automatic design of semantic similarity controllers based on fuzzy logics, using genetic algorithms, achieves reasonable results and high interpretability in semantic similarity measurement.","Jorge Martínez Gil, J. M. Chaves-González",2019,21,"Abstract Recent advances in machine learning have been able to make improvements over the state-of-the-art regarding semantic similarity measurement techniques. In fact, we have all seen how classical techniques have given way to promising neural techniques. Nonetheless, these new techniques have a weak point: they are hardly interpretable. For this reason, we have oriented our research towards the design of strategies being able to be accurate enough but without sacrificing their interpretability. As a result, we have obtained a strategy for the automatic design of semantic similarity controllers based on fuzzy logics, which are automatically identified using genetic algorithms (GAs). After an exhaustive evaluation using a number of well-known benchmark datasets, we can conclude that our strategy fulfills both expectations: it is able of achieving reasonably good results, and at the same time, it can offer high degrees of interpretability.","","Expert Syst. Appl.","1","10.1016/J.ESWA.2019.04.046","https://consensus.app/papers/automatic-design-similarity-controllers-based-logics-gil/f9c92d9d33255c3f8d403f44a02568e7/"
"Building and Interpreting Deep Similarity Models","BiLRP effectively explains complex deep similarity models, providing insight and verifiability into machine learning models, and enhancing the understanding of historical documents.","Oliver Eberle, Jochen Büttner, Florian Kräutli, K. Müller, M. Valleriani, G. Montavon",2020,40,"Many learning algorithms such as kernel machines, nearest neighbors, clustering, or anomaly detection, are based on distances or similarities. Before similarities are used for training an actual machine learning model, we would like to verify that they are bound to meaningful patterns in the data. In this paper, we propose to make similarities interpretable by augmenting them with an explanation. We develop BiLRP, a scalable and theoretically founded method to systematically decompose the output of an already trained deep similarity model on pairs of input features. Our method can be expressed as a composition of LRP explanations, which were shown in previous works to scale to highly nonlinear models. Through an extensive set of experiments, we demonstrate that BiLRP robustly explains complex similarity models, e.g., built on VGG-16 deep neural network features. Additionally, we apply our method to an open problem in digital humanities: detailed assessment of similarity between historical documents, such as astronomical tables. Here again, BiLRP provides insight and brings verifiability into a highly engineered and problem-specific similarity model.","","IEEE Transactions on Pattern Analysis and Machine Intelligence","1","10.1109/TPAMI.2020.3020738","https://consensus.app/papers/building-interpreting-deep-similarity-models-eberle/405cca5ebd3855f9aa325eb4a0cdcd68/"
"A novel method based on symbolic regression for interpretable semantic similarity measurement","Our new approach based on symbolic regression achieves high interpretability without sacrificing accuracy in measuring semantic similarity between textual expressions.","Jorge Martínez Gil, J. M. Chaves-González",2020,16,"Abstract The problem of automatically measuring the degree of semantic similarity between textual expressions is a challenge that consists of calculating the degree of likeness between two text fragments that have none or few features in common according to human judgment. In recent times, several machine learning methods have been able to establish a new state-of-the-art regarding the accuracy, but none or little attention has been paid to their interpretability, i.e. the extent to which an end-user could be able to understand the cause of the output from these approaches. Although such solutions based on symbolic regression already exist in the field of clustering, we propose here a new approach which is being able to reach high levels of interpretability without sacrificing accuracy in the context of semantic textual similarity. After a complete empirical evaluation using several benchmark datasets, it is shown that our approach yields promising results in a wide range of scenarios.","","Expert Syst. Appl.","1","10.1016/j.eswa.2020.113663","https://consensus.app/papers/novel-method-based-regression-similarity-measurement-gil/b178bbe72c7b5c519197d9830e73ff70/"
"Definitions, methods, and applications in interpretable machine learning","The PDR framework clarifies interpretable machine learning and helps evaluate methods based on predictive accuracy, descriptive accuracy, and relevancy, highlighting the importance of human audiences.","W. James Murdoch, Chandan Singh, Karl Kumbier, R. Abbasi-Asl, Bin Yu",2019,991,"Significance The recent surge in interpretability research has led to confusion on numerous fronts. In particular, it is unclear what it means to be interpretable and how to select, evaluate, or even discuss methods for producing interpretations of machine-learning models. We aim to clarify these concerns by defining interpretable machine learning and constructing a unifying framework for existing methods which highlights the underappreciated role played by human audiences. Within this framework, methods are organized into 2 classes: model based and post hoc. To provide guidance in selecting and evaluating interpretation methods, we introduce 3 desiderata: predictive accuracy, descriptive accuracy, and relevancy. Using our framework, we review existing work, grounded in real-world studies which exemplify our desiderata, and suggest directions for future work. Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the predictive, descriptive, relevant (PDR) framework for discussing interpretations. The PDR framework provides 3 overarching desiderata for evaluation: predictive accuracy, descriptive accuracy, and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post hoc categories, with subgroups including sparsity, modularity, and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often underappreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.","literature review","Proceedings of the National Academy of Sciences","1","10.1073/pnas.1900654116","https://consensus.app/papers/definitions-methods-applications-machine-learning-murdoch/e80cf05220ce5ab2ba6da5d3dba1d93a/"
"Explainable AI: A Review of Machine Learning Interpretability Methods","This study presents a literature review and taxonomy of machine learning interpretability methods, aiming to improve the transparency of AI systems in sensitive domains like healthcare.","Pantelis Linardatos, Vasilis Papastefanopoulos, S. Kotsiantis",2020,977,"Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into “black box” approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.","literature review","Entropy","2","10.3390/e23010018","https://consensus.app/papers/review-machine-learning-interpretability-methods-linardatos/50ac5396597f56a18f6683daacf32da8/"
"Similarity Calculations of Academic Articles Using Topic Events and Domain Knowledge","Our method improves document-level semantic similarity in academic literatures by incorporating external knowledge and topic event models, achieving significant improvements over state-of-the-art methods.","Ming Liu, Bo Lang, Zepeng Gu",2018,2,"While studies investigating the semantic similarity among concepts, sentences and short text fragments have been fruitful, the problem of document-level semantic matching remains largely unexplored due to its complexity. In this paper, we explore the document-level semantic similarity issue in the academic literatures using an interpretable method. To integrally describe the semantics of an article, we construct a topic event model that utilizes multiple information facets, such as the study purposes, methodologies and domains. Furthermore, to better understand the documents and achieve a more accurate similarity comparison, we incorporate external knowledge into the topic event construction and similarity calculation. Our approach achieves significant improvements over state-of-the-art methods.","","","","10.1007/978-3-319-96890-2_4","https://consensus.app/papers/similarity-calculations-academic-articles-using-topic-liu/a05a0edd076250e88bdfffd00c34b67b/"
"Human Factors in Model Interpretability: Industry Practices, Challenges, and Needs","Model interpretability in machine learning models often involves cooperation and mental model comparison among different roles, aiming to build trust within organizations.","Sungsoo Ray Hong, J. Hullman, E. Bertini",2020,127,"As the use of machine learning (ML) models in product development and data-driven decision-making processes became pervasive in many domains, people's focus on building a well-performing model has increasingly shifted to understanding how their model works. While scholarly interest in model interpretability has grown rapidly in research communities like HCI, ML, and beyond, little is known about how practitioners perceive and aim to provide interpretability in the context of their existing workflows. This lack of understanding of interpretability as practiced may prevent interpretability research from addressing important needs, or lead to unrealistic solutions. To bridge this gap, we conducted 22 semi-structured interviews with industry practitioners to understand how they conceive of and design for interpretability while they plan, build, and use their models. Based on a qualitative analysis of our results, we differentiate interpretability roles, processes, goals and strategies as they exist within organizations making heavy use of ML models. The characterization of interpretability work that emerges from our analysis suggests that model interpretability frequently involves cooperation and mental model comparison between people in different roles, often aimed at building trust not only between people and models but also between people within the organization. We present implications for design that discuss gaps between the interpretability challenges that practitioners face in their practice and approaches proposed in the literature, highlighting possible research directions that can better address real-world needs.","","Proceedings of the ACM on Human-Computer Interaction","1","10.1145/3392878","https://consensus.app/papers/factors-model-interpretability-industry-practices-hong/ab3e237ce2d55909a7ab48258015f862/"
"A Comparison of Semantic Similarity Methods for Maximum Human Interpretability","Cosine similarity using tf-idf vectors is the most effective method for finding semantic similarities between short news texts, providing easy-to-interpret results for information retrieval applications.","P. Sitikhu, Kritish Pahi, Pujan Thapa, S. Shakya",2019,58,"The inclusion of semantic information in any similarity measures improves the efficiency of the similarity measure and provides human interpretable results for further analysis. The similarity calculation method that focuses on features related to the text's words only, will give less accurate results. This paper presents three different methods that not only focus on the text's words but also incorporates semantic information of texts in their feature vector and computes semantic similarities. These methods are based on corpus-based and knowledge-based methods, which are: cosine similarity using tf-idf vectors, cosine similarity using word embedding and soft cosine similarity using word embedding. Among these three, cosine similarity using tf-idf vectors performed best in finding similarities between short news texts. The similar texts given by the method are easy to interpret and can be used directly in other information retrieval applications.","","2019 Artificial Intelligence for Transforming Business and Society (AITB)","","10.1109/AITB48515.2019.8947433","https://consensus.app/papers/comparison-semantic-similarity-methods-maximum-human-sitikhu/323412cdbff65ead810125fa60f2317a/"
"Review Study of Interpretation Methods for Future Interpretable Machine Learning","This paper reviews various interpretable machine learning methods, categorizes them into two categories (self-explanatory and external co-explanation), and explores future challenges and trends in interpretable machine learning.","Jian-Xun Mi, An-Di Li, Li-Fang Zhou",2020,42,"In recent years, black-box models have developed rapidly because of their high accuracy. Balancing the interpretability and accuracy is increasingly important. The lack of interpretability severely limits the application of the model in academia and industry. Despite the various interpretable machine learning methods, the perspective and meaning of the interpretation are also different. We provide a review of the current interpretable methods and divide them based on the model being applied. We divide them into two categories: interpretable methods with the self-explanatory model and interpretable methods with external co-explanation. And the interpretable methods with external co-explanation are further divided into subbranch methods based on instances, SHAP, knowledge graph, deep learning, and clustering model. The classification aims to help us understand the model characteristics applied in the interpretable method better. This survey makes the researcher find a suitable model to solve interpretability problems more easily. And the comparison experiments contribute to discovering complementary features from different methods. At the same time, we explore the future challenges and trends of interpretable machine learning to promote the development of interpretable machine learning.","literature review","IEEE Access","1","10.1109/ACCESS.2020.3032756","https://consensus.app/papers/review-study-interpretation-methods-future-mi/0ea3ef47d9c35035bb03b14bf4df0423/"
