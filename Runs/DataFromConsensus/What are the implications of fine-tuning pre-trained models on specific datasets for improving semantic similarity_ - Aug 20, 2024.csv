Title,Takeaway,Authors,Year,Citations,Abstract,Study Type,Journal,Journal SJR Quartile,DOI,Consensus Link
"Evaluating the Utility of Model Configurations and Data Augmentation on Clinical Semantic Textual Similarity","Data augmentation techniques, such as data pooling and domain-specific fine-tuning, significantly improve clinical semantic textual similarity models compared to general domain models.","Yuxia Wang, Fei Liu, Karin M. Verspoor, Timothy Baldwin",2020,20,"In this paper, we apply pre-trained language models to the Semantic Textual Similarity (STS) task, with a specific focus on the clinical domain. In low-resource setting of clinical STS, these large models tend to be impractical and prone to overfitting. Building on BERT, we study the impact of a number of model design choices, namely different fine-tuning and pooling strategies. We observe that the impact of domain-specific fine-tuning on clinical STS is much less than that in the general domain, likely due to the concept richness of the domain. Based on this, we propose two data augmentation techniques. Experimental results on N2C2-STS 1 demonstrate substantial improvements, validating the utility of the proposed methods.","","","","10.18653/v1/2020.bionlp-1.11","https://consensus.app/papers/evaluating-utility-model-configurations-data-wang/8fd28d07af7054639c579e8057567d2e/"
"Improving Fine-tuning of Self-supervised Models with Contrastive Initialization","Our Contrastive Initialization (COIN) method significantly improves fine-tuning of self-supervised models by enhancing semantic relations among images, setting new state-of-the-arts on various downstream tasks.","Haolin Pan, Yong Guo, Qinyi Deng, Hao-Fan Yang, Yiqun Chen, Jian Chen",2022,9,"Self-supervised learning (SSL) has achieved remarkable performance in pre-training the models that can be further used in downstream tasks via fine-tuning. However, these self-supervised models may not capture meaningful semantic information since the images belonging to the same class are often regarded as negative pairs in the contrastive loss. Consequently, the images of the same class are often located far away from each other in the learned feature space, which would inevitably hamper the fine-tuning process. To address this issue, we seek to explicitly enhance the semantic relation among instances on the targeted downstream task and provide a better initialization for the subsequent fine-tuning. To this end, we propose a Contrastive Initialization (COIN) method that breaks the standard fine-tuning pipeline by introducing an extra class-aware initialization stage before fine-tuning. Specifically, we exploit a supervised contrastive loss to increase inter-class discrepancy and intra-class compactness of features on the target dataset. In this way, self-supervised models can be easily trained to discriminate instances of different classes during the final fine-tuning stage. Extensive experiments show that, with the enriched semantics, our COIN significantly outperforms existing methods without introducing extra training cost and sets new state-of-the-arts on multiple downstream tasks. For example, compared with the baseline method, our COIN improves the accuracy by 5% on ImageNet-20 and 2.57% on CIFAR100, respectively.","","Neural networks : the official journal of the International Neural Network Society","1","10.48550/arXiv.2208.00238","https://consensus.app/papers/improving-finetuning-models-contrastive-initialization-pan/63e707aa01305240b13c0b370da32f57/"
"CSS-LM: A Contrastive Framework for Semi-Supervised Fine-Tuning of Pre-Trained Language Models","CSS-LM improves fine-tuning of pre-trained language models by up to 7.8% on downstream tasks with few-shot settings, outperforming the latest supervised contrastive fine-tuning strategy by up to 7.1%.","Yusheng Su, Xu Han, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Peng Li, Maosong Sun",2021,6,"Fine-tuning pre-trained language models (PLMs) has demonstrated its effectiveness on various downstream NLP tasks recently. However, in many scenarios with limited supervised data, the conventional fine-tuning strategies cannot sufficiently capture the important semantic features for downstream tasks. To address this issue, we introduce a novel framework (named “CSS-LM”) to improve the fine-tuning phase of PLMs via contrastive semi-supervised learning. Specifically, given a specific task, we retrieve positive and negative instances from large-scale unlabeled corpora according to their domain-level and class-level semantic relatedness to the task. We then perform contrastive semi-supervised learning on both the retrieved unlabeled instances and original labeled instances to help PLMs capture crucial task-related semantic features. The experimental results show that CSS-LM achieves better results than the conventional fine-tuning strategy on a series of downstream tasks with few-shot settings by up to 7.8%, and outperforms the latest supervised contrastive fine-tuning strategy by up to 7.1%. Our datasets and source code will be available to provide more details.","","IEEE/ACM Transactions on Audio, Speech, and Language Processing","1","10.1109/TASLP.2021.3105013","https://consensus.app/papers/csslm-contrastive-framework-semisupervised-finetuning-su/eaa37d710732593cac02676767db5345/"
"Deep entity matching with pre-trained language models","Ditto, a deep entity matching system based on pre-trained language models, significantly improves matching quality and outperforms previous state-of-the-art, with optimization techniques boosting performance by up to 9.8%.","Yuliang Li, Jinfeng Li, Yoshihiko Suhara, A. Doan, W. Tan",2020,229,"We present Ditto, a novel entity matching system based on pre-trained Transformer-based language models. We fine-tune and cast EM as a sequence-pair classification problem to leverage such models with a simple architecture. Our experiments show that a straight-forward application of language models such as BERT, DistilBERT, or RoBERTa pre-trained on large text corpora already significantly improves the matching quality and outperforms previous state-of-the-art (SOTA), by up to 29% of F1 score on benchmark datasets. We also developed three optimization techniques to further improve Ditto's matching capability. Ditto allows domain knowledge to be injected by highlighting important pieces of input information that may be of interest when making matching decisions. Ditto also summarizes strings that are too long so that only the essential information is retained and used for EM. Finally, Ditto adapts a SOTA technique on data augmentation for text to EM to augment the training data with (difficult) examples. This way, Ditto is forced to learn ""harder"" to improve the model's matching capability. The optimizations we developed further boost the performance of Ditto by up to 9.8%. Perhaps more surprisingly, we establish that Ditto can achieve the previous SOTA results with at most half the number of labeled data. Finally, we demonstrate Ditto's effectiveness on a real-world large-scale EM task. On matching two company datasets consisting of 789K and 412K records, Ditto achieves a high F1 score of 96.5%.","","Proceedings of the VLDB Endowment","1","10.14778/3421424.3421431","https://consensus.app/papers/entity-matching-language-models-li/8ad469ebdf6d52a992f5a9482780dcb6/"
"Prompt Tuning for Parameter-efficient Medical Image Segmentation","Prompt tuning in neural networks can effectively adapt to medical imaging datasets, reducing the gap between fully fine-tuned and parameter-efficient models.","Marc Fischer, Alexander Bartler, Bin Yang",2022,1,"Neural networks pre-trained on a self-supervision scheme have become the standard when operating in data rich environments with scarce annotations. As such, fine-tuning a model to a downstream task in a parameter-efficient but effective way, e.g. for a new set of classes in the case of semantic segmentation, is of increasing importance. In this work, we propose and investigate several contributions to achieve a parameter-efficient but effective adaptation for semantic segmentation on two medical imaging datasets. Relying on the recently popularized prompt tuning approach, we provide a prompt-able UNETR (PUNETR) architecture, that is frozen after pre-training, but adaptable throughout the network by class-dependent learnable prompt tokens. We pre-train this architecture with a dedicated dense self-supervision scheme based on assignments to online generated prototypes (contrastive prototype assignment, CPA) of a student teacher combination. Concurrently, an additional segmentation loss is applied for a subset of classes during pre-training, further increasing the effectiveness of leveraged prompts in the fine-tuning phase. We demonstrate that the resulting method is able to attenuate the gap between fully fine-tuned and parameter-efficiently adapted models on CT imaging datasets. To this end, the difference between fully fine-tuned and prompt-tuned variants amounts to 7.81 pp for the TCIA/BTCV dataset as well as 5.37 and 6.57 pp for subsets of the TotalSegmentator dataset in the mean Dice Similarity Coefficient (DSC, in %) while only adjusting prompt tokens, corresponding to 0.51% of the pre-trained backbone model with 24.4M frozen parameters. The code for this work is available on https://github.com/marcdcfischer/PUNETR.","","Medical image analysis","1","10.48550/arXiv.2211.09233","https://consensus.app/papers/prompt-tuning-parameterefficient-medical-image-fischer/eac932424b9751c0b437647328e81c32/"
"DR-Tune: Improving Fine-tuning of Pretrained Visual Models by Distribution Regularization with Semantic Calibration","DR-Tune improves fine-tuning of pretrained visual models by combining distribution regularization with semantic calibration, preventing overfitting and enabling sufficient training of downstream encoders.","Nana Zhou, Jiaxin Chen, Di Huang",2023,0,"The visual models pretrained on large-scale benchmarks encode general knowledge and prove effective in building more powerful representations for downstream tasks. Most existing approaches follow the fine-tuning paradigm, either by initializing or regularizing the downstream model based on the pretrained one. The former fails to retain the knowledge in the successive fine-tuning phase, thereby prone to be over-fitting, and the latter imposes strong constraints to the weights or feature maps of the downstream model without considering semantic drift, often incurring insufficient optimization. To deal with these issues, we propose a novel fine-tuning framework, namely distribution regularization with semantic calibration (DR-Tune). It employs distribution regularization by enforcing the downstream task head to decrease its classification error on the pretrained feature distribution, which prevents it from over-fitting while enabling sufficient training of downstream encoders. Furthermore, to alleviate the interference by semantic drift, we develop the semantic calibration (SC) module to align the global shape and class centers of the pretrained and downstream feature distributions. Extensive experiments on widely used image classification datasets show that DR-Tune consistently improves the performance when combing with various backbones under different pretraining strategies. Code is available at: https://github.com/weeknan/DR-Tune.","","ArXiv","","10.48550/arXiv.2308.12058","https://consensus.app/papers/drtune-improving-finetuning-pretrained-visual-models-zhou/d622d7ebd2115a0988e9ae8072ba6472/"
"On the Transferability of Pre-trained Language Models: A Study from Artificial Datasets","Pre-trained language models with explicit dependencies and longer implicit dependencies perform better on downstream tasks than models trained from scratch, but may learn spurious correlations.","Cheng-Han Chiang, Hung-yi Lee",2021,15,"Pre-training language models (LMs) on large-scale unlabeled text data makes the model much easier to achieve exceptional downstream performance than their counterparts directly trained on the downstream tasks. 
 In this work, we study what specific traits in the pre-training data, other than the semantics, make a pre-trained LM superior to their counterparts trained from scratch on downstream tasks.
 We propose to use artificially constructed datasets as the pre-training data to exclude the effect of semantics, and further control what characteristics the pre-training corpora have.
 By fine-tuning the pre-trained models on GLUE benchmark, we can learn how beneficial it is to transfer the knowledge from the model trained on the dataset possessing that specific trait.
 We define and discuss three different characteristics in the artificial dataset: 1) matching the token's uni-gram or bi-gram distribution between pre-training and downstream fine-tuning, 2) the presence of the explicit dependencies among the tokens in a sequence, 3) the length of the implicit dependencies among the tokens in a sequence. 
 Our experiments show that the explicit dependencies in the sequences of the pre-training data are critical to the downstream performance.
 Our results also reveal that models achieve better downstream performance when pre-trained on a dataset with a longer range of implicit dependencies.
 Based on our analysis, we find that models pre-trained with artificial datasets are prone to learn spurious correlation in downstream tasks.
 Our work reveals that even if the LMs are not pre-trained on natural language, they still gain transferability on certain human language downstream tasks once the LMs learn to model the token dependencies in the sequences. 
 This result helps us understand the exceptional transferability of pre-trained LMs.","","","","10.1609/aaai.v36i10.21295","https://consensus.app/papers/transferability-pretrained-language-models-study-chiang/2c4c15699ad05349a13f489f2be17a5b/"
"Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning","Supervised contrastive learning (SCL) objective improves fine-tuning of pre-trained language models, leading to more robust models and better generalization to related tasks with limited labeled data.","Beliz Gunel, Jingfei Du, Alexis Conneau, Ves Stoyanov",2020,327,"State-of-the-art natural language understanding classification models follow two-stages: pre-training a large language model on an auxiliary task, and then fine-tuning the model on a task-specific labeled dataset using cross-entropy loss. Cross-entropy loss has several shortcomings that can lead to sub-optimal generalization and instability. Driven by the intuition that good generalization requires capturing the similarity between examples in one class and contrasting them with examples in other classes, we propose a supervised contrastive learning (SCL) objective for the fine-tuning stage. Combined with cross-entropy, the SCL loss we propose obtains improvements over a strong RoBERTa-Large baseline on multiple datasets of the GLUE benchmark in both the high-data and low-data regimes, and it does not require any specialized architecture, data augmentation of any kind, memory banks, or additional unsupervised data. We also demonstrate that the new objective leads to models that are more robust to different levels of noise in the training data, and can generalize better to related tasks with limited labeled task data.","","ArXiv","","","https://consensus.app/papers/supervised-learning-pretrained-language-model-gunel/6fe4fb47b9925cd08a201d318dfb1121/"
"Task-Specific Skill Localization in Fine-tuned Language Models","Fine-tuning a small subset of parameters in pre-trained language models can improve performance and reduce errors, with no further re-training needed.","A. Panigrahi, Nikunj Saunshi, Haoyu Zhao, Sanjeev Arora",2023,13,"Pre-trained language models can be fine-tuned to solve diverse NLP tasks, including in few-shot settings. Thus fine-tuning allows the model to quickly pick up task-specific ``skills,'' but there has been limited study of where these newly-learnt skills reside inside the massive model. This paper introduces the term skill localization for this problem and proposes a solution. Given the downstream task and a model fine-tuned on that task, a simple optimization is used to identify a very small subset of parameters ($\sim0.01$% of model parameters) responsible for ($>95$%) of the model's performance, in the sense that grafting the fine-tuned values for just this tiny subset onto the pre-trained model gives performance almost as well as the fine-tuned model. While reminiscent of recent works on parameter-efficient fine-tuning, the novel aspects here are that: (i) No further re-training is needed on the subset (unlike, say, with lottery tickets). (ii) Notable improvements are seen over vanilla fine-tuning with respect to calibration of predictions in-distribution ($40$-$90$% error reduction) as well as the quality of predictions out-of-distribution (OOD). In models trained on multiple tasks, a stronger notion of skill localization is observed, where the sparse regions corresponding to different tasks are almost disjoint, and their overlap (when it happens) is a proxy for task similarity. Experiments suggest that localization via grafting can assist certain forms of continual learning.","","","","10.48550/arXiv.2302.06600","https://consensus.app/papers/taskspecific-skill-localization-finetuned-language-panigrahi/1fffe60879d55f019900f7a2b3702439/"
"Improved Fine-tuning by Leveraging Pre-training Data: Theory and Practice","Leveraging pre-training data in fine-tuning can improve the final performance on a target task, especially when using a novel selection strategy.","Ziquan Liu, Yi Xu, Yuanhong Xu, Qi Qian, Hao Li, Antoni B. Chan, Rong Jin",2021,6,"As a dominant paradigm, fine-tuning a pre-trained model on the target data is widely used in many deep learning applications, especially for small data sets. However, recent studies have empirically shown that training from scratch has the final performance that is no worse than this pre-training strategy once the number of training iterations is increased in some vision tasks. In this work, we revisit this phenomenon from the perspective of generalization analysis which is popular in learning theory. Our result reveals that the final prediction precision may have a weak dependency on the pre-trained model especially in the case of large training iterations. The observation inspires us to leverage pre-training data for fine-tuning, since this data is also available for fine-tuning. The generalization result of using pre-training data shows that the final performance on a target task can be improved when the appropriate pre-training data is included in fine-tuning. With the insight of the theoretical finding, we propose a novel selection strategy to select a subset from pre-training data to help improve the generalization on the target task. Extensive experimental results for image classification tasks on 8 benchmark data sets verify the effectiveness of the proposed data selection based fine-tuning pipeline.","","ArXiv","","","https://consensus.app/papers/improved-finetuning-leveraging-pretraining-data-theory-liu/13e8f76f5b3c5cc59ae84c3a68563373/"
