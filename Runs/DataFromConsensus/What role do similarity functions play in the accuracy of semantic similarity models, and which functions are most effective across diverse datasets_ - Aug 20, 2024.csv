Title,Takeaway,Authors,Year,Citations,Abstract,Study Type,Journal,Journal SJR Quartile,DOI,Consensus Link
"Determining Semantic Similarity among Entity Classes from Different Ontologies","This study presents a semantic similarity model that accounts for differences in ontology specifications and matches synonym sets, semantic neighborhoods, and distinguishing features to determine similar entity classes across different ontologies.","Michael A. Rodriguez, M. Egenhofer",2003,1012,"Semantic similarity measures play an important role in information retrieval and information integration. Traditional approaches to modeling semantic similarity compute the semantic distance between definitions within a single ontology. This single ontology is either a domain-independent ontology or the result of the integration of existing ontologies. We present an approach to computing semantic similarity that relaxes the requirement of a single ontology and accounts for differences in the levels of explicitness and formalization of the different ontology specifications. A similarity function determines similar entity classes by using a matching process over synonym sets, semantic neighborhoods, and distinguishing features that are classified into parts, functions, and attributes. Experimental results with different ontologies indicate that the model gives good results when ontologies have complete and detailed representations of entity classes. While the combination of word matching and semantic neighborhood matching is adequate for detecting equivalent entity classes, feature matching allows us to discriminate among similar, but not necessarily equivalent entity classes.","","IEEE Trans. Knowl. Data Eng.","1","10.1109/TKDE.2003.1185844","https://consensus.app/papers/determining-similarity-among-entity-classes-different-rodriguez/483c445879505982977ef61c92b5d4d1/"
"Similarity Judgment Within and Across Categories: A Comprehensive Model Comparison","Cosine similarity and Pearson correlation are the best unweighted similarity functions, while word vectors derived from free association norms often outperform text-based vectors, and category-specific weights on dimensions improve predictions across all similarity functions and representations.","Russell Richie, Sudeep Bhatia",2020,15,"Similarity is one of the most important relations humans perceive, arguably subserving category learning and categorization, generalization and discrimination, judgment and decision making, and other cognitive functions. Researchers have proposed a wide range of representations and metrics that could be at play in similarity judgment, yet have not comprehensively compared the power of these representations and metrics for predicting similarity within and across different semantic categories. We performed such a comparison by pairing nine prominent vector semantic representations with seven established similarity metrics that could operate on these representations, as well as supervised methods for dimensional weighting in the similarity function. This approach yields a factorial model structure with 126 distinct representation-metric pairs, which we tested on a novel dataset of similarity judgments between pairs of cohyponymic words in eight categories. We found that cosine similarity and Pearson correlation were the overall best performing unweighted similarity functions, and that word vectors derived from free association norms often outperformed word vectors derived from text (including those specialized for similarity). Importantly, models that used human similarity judgments to learn category-specific weights on dimensions yielded substantially better predictions than all unweighted approaches across all types of similarity functions and representations, although dimension weights did not generalize well across semantic categories, suggesting strong category context effects in similarity judgment. We discuss implications of these results for cognitive modeling and natural language processing, as well as for theories of the representations and metrics involved in similarity.","","Cognitive science","1","10.31234/osf.io/5pa9r","https://consensus.app/papers/similarity-judgment-within-across-categories-richie/c9c034226967503eb1fb458e8f2c8698/"
"Evaluation of taxonomic and neural embedding methods for calculating semantic similarity","Taxonomic similarity measures rely on shortest path length, edge-counting is free from sense distribution bias, and neural embeddings with concept relations can improve semantic similarity prediction.","Dongqiang Yang, Yanqin Yin",2021,2,"Abstract Modelling semantic similarity plays a fundamental role in lexical semantic applications. A natural way of calculating semantic similarity is to access handcrafted semantic networks, but similarity prediction can also be anticipated in a distributional vector space. Similarity calculation continues to be a challenging task, even with the latest breakthroughs in deep neural language models. We first examined popular methodologies in measuring taxonomic similarity, including edge-counting that solely employs semantic relations in a taxonomy, as well as the complex methods that estimate concept specificity. We further extrapolated three weighting factors in modelling taxonomic similarity. To study the distinct mechanisms between taxonomic and distributional similarity measures, we ran head-to-head comparisons of each measure with human similarity judgements from the perspectives of word frequency, polysemy degree and similarity intensity. Our findings suggest that without fine-tuning the uniform distance, taxonomic similarity measures can depend on the shortest path length as a prime factor to predict semantic similarity; in contrast to distributional semantics, edge-counting is free from sense distribution bias in use and can measure word similarity both literally and metaphorically; the synergy of retrofitting neural embeddings with concept relations in similarity prediction may indicate a new trend to leverage knowledge bases on transfer learning. It appears that a large gap still exists on computing semantic similarity among different ranges of word frequency, polysemous degree and similarity intensity.","","Natural Language Engineering","1","10.1017/S1351324921000279","https://consensus.app/papers/evaluation-embedding-methods-calculating-similarity-yang/b173931791c156b38bc0ceac63340417/"
"Global semantic similarity effects in recognition memory: Insights from BEAGLE representations and the diffusion decision model","Global semantic similarity in recognition memory increases false alarm rates, with item vector similarity having a greater impact than order vector similarity, and a linear relationship with drift rate.","Adam F. Osth, Kevin D. Shabahang, Douglas J. K. Mewhort, Andrew  Heathcote",2020,3,"Abstract Recognition memory models posit that false alarm rates increase as the global similarity between the probe cue and the contents of memory is increased. Global similarity predictions have been commonly tested using category length designs where it has been found that false alarm rates increase as the number of studied items from a common category is increased. In this work, we explored global similarity predictions within unstructured lists of words using representations from the BEAGLE model (Jones & Mewhort, 2007). BEAGLE differs from traditional semantic space models in that it contains two types of representations: item vectors, which encode unordered co-occurrence, and order vectors, in which words are similar to the extent to which they are share neighboring words in the same relative positions. Global similarity among item and order vectors was regressed onto drift rates in the diffusion decision model (DDM: Ratcliff, 1978), which unifies both response times and accuracy. We implemented this model in a hierarchical Bayesian framework across seven datasets with lists composed of unrelated words. Results indicated clear deficits due to global similarity among item vectors, but only a minimal impact of global similarity among the order vectors. We also found evidence for a linear relationship between global similarity and drift rate and did not find any evidence that global similarity differentially affected performance in speed vs. accuracy emphasis conditions. In addition, we found that global semantic similarity could only partially account for the word frequency effect, suggesting that other factors besides semantic similarity may be responsible.","","Journal of Memory and Language","1","10.1016/j.jml.2019.104071","https://consensus.app/papers/global-similarity-effects-recognition-memory-insights-osth/fff6230aeab652ae849fe7f4ed4d307c/"
"How aspects of similar datasets can impact distributional models","BERTimbau shows considerable differences in accuracy between related datasets for Semantic Similarity identification, indicating that data set characteristics may impact distributional models' performance.","Isabella Maria Alonso Gomes, N. T. Roman",2022,0,"Distributional models have become popular due to the abstractions that allowed their immediate use, with good results and little implementation effort when compared to precursor models. Given their presumed high level of generalization it would be expected that good and similar results would be found in data sets sharing the same nature and purpose. However, this is not always the case. In this work, we present the results of the application of BERTimbau in two related data sets, built for the task of Semantic Similarity identification, with the goal of detecting redundancy in text. Results showed that there are considerable differences in accuracy between the data sets. We explore aspects of the data sets that could explain why accuracy results are different across them.","","Anais do XIX Encontro Nacional de Inteligência Artificial e Computacional (ENIAC 2022)","","10.5753/eniac.2022.227085","https://consensus.app/papers/aspects-datasets-models-gomes/a5d1d71c4c8a59c3bcdc67bce23d5cf3/"
"Global semantic similarity effects in recognition memory: Insights from BEAGLE representations and the diffusion decision model","Global similarity among item vectors impairs recognition memory performance, while global similarity among order vectors has minimal influence, supporting associative similarity as a stronger performance impairment than taxonomic similarity.","Adam F. Osth, Kevin D. Shabahang, D. Mewhort, A. Heathcote",2019,11,"Recognition memory models posit that performance is impaired as the similarity between the probe cue and the contents of memory is increased (global similarity). Global similarity predictions have been commonly tested using category length designs, in which the number of items from a common taxonomic or associative category is manipulated. Prior work has demonstrated that increases in the length of associative categories show clear detriments on performance, but that result is found only inconsistently for taxonomic categories. In this work, we explored global similarity predictions using representations from the BEAGLE model (Jones & Mewhort, 2007). BEAGLE’s two types of word representations, item and order vectors, exhibit similarity relations that resemble relations among associative and taxonomic category members, respectively. Global similarity among item and order vectors was regressed onto drift rates in the diffusion decision model (DDM: Ratcliff, 1978), which simultaneously accounts for both response times and accuracy. We implemented this model in a hiearchical Bayesian framework across seven datasets with lists composed of unrelated words. Results indicated clear deficits due to global similarity among item vectors, suggesting that lists of unrelated words exhibit semantic structure that impairs performance. However, there were relatively small influences of global similarity among the order vectors. These results are consistent with prior work suggesting associative similarity causes stronger performance impairments than taxonomic similarity.","","Journal of Memory and Language","1","10.31234/osf.io/yda2r","https://consensus.app/papers/global-similarity-effects-recognition-memory-insights-osth/19fdb7b3eb465b9097511945884b5ace/"
"Computing Semantic Similarity of Concepts in Knowledge Graphs","The wpath semantic similarity method significantly improves accuracy and F score compared to other methods in category classification evaluations.","Ganggao Zhu, C. Iglesias",2017,181,"This paper presents a method for measuring the semantic similarity between concepts in Knowledge Graphs (KGs) such as WordNet and DBpedia. Previous work on semantic similarity methods have focused on either the structure of the semantic network between concepts (e.g., path length and depth), or only on the Information Content (IC) of concepts. We propose a semantic similarity method, namely wpath, to combine these two approaches, using IC to weight the shortest path length between concepts. Conventional corpus-based IC is computed from the distributions of concepts over textual corpus, which is required to prepare a domain corpus containing annotated concepts and has high computational cost. As instances are already extracted from textual corpus and annotated by concepts in KGs, graph-based IC is proposed to compute IC based on the distributions of concepts over instances. Through experiments performed on well known word similarity datasets, we show that the wpath semantic similarity method has produced a statistically significant improvement over other semantic similarity methods. Moreover, in a real category classification evaluation, the wpath method has shown the best performance in terms of accuracy and F score.","","IEEE Transactions on Knowledge and Data Engineering","1","10.1109/TKDE.2016.2610428","https://consensus.app/papers/computing-similarity-concepts-knowledge-graphs-zhu/93128d8cf18852eb884b33986c9ae834/"
"A Large Probabilistic Semantic Network Based Approach to Compute Term Similarity","Our large scale semantic network-based approach accurately computes semantic similarity between terms in multi-word expressions, outperforming 12 competing methods and being more efficient than existing methods.","Peipei Li, Haixun Wang, Kenny Q. Zhu, Zhongyuan Wang, Xuegang Hu, Xindong Wu",2015,32,"Measuring semantic similarity between two terms is essential for a variety of text analytics and understanding applications. Currently, there are two main approaches for this task, namely the knowledge based and the corpus based approaches. However, existing approaches are more suitable for semantic similarity between words rather than the more general multi-word expressions (MWEs), and they do not scale very well. Contrary to these existing techniques, we propose an efficient and effective approach for semantic similarity using a large scale semantic network. This semantic network is automatically acquired from billions of web documents. It consists of millions of concepts, which explicitly model the context of semantic relationships. In this paper, we first show how to map two terms into the concept space, and compare their similarity there. Then, we introduce a clustering approach to orthogonalize the concept space in order to improve the accuracy of the similarity measure. Finally, we conduct extensive studies to demonstrate that our approach can accurately compute the semantic similarity between terms of MWEs and with ambiguity, and significantly outperforms 12 competing methods under Pearson Correlation Coefficient. Meanwhile, our approach is much more efficient than all competing algorithms, and can be used to compute semantic similarity in a large scale.","","IEEE Transactions on Knowledge and Data Engineering","1","10.1109/TKDE.2015.2419673","https://consensus.app/papers/probabilistic-semantic-network-based-approach-compute-li/90052fce1c115ffc88e5f5a558b5b6ce/"
"Semantic similarity on constraints datasets: A latent approach","Latent space methods improve semantic similarity models for constrained environments, achieving better correlations and performance improvements in IoT and Miller-Charles datasets.","M. Antunes, D. Gomes, Rui L. Aguiar",2021,1,"The technological world has grown by incorporating billions of small sensing devices, collecting and sharing large amounts of diversified data over the new generation of wireless and mobile networks. Semantic similarity models have been used as a means to organize and optimize devices in constrained environments such as IoT, edge computing, and 5G and next-generation networks. In this paper, we reviewed the commonly used semantic similarity models, discussed the limitations of our previous model, and explored latent space methods (through matrix factorization) as a way to reduce noise and correct the model profiles with no additional data. Our solution was evaluated on two datasets: Miller-Charles and IoT semantic datasets. The improved model achieved a correlation of 0.62 and 0.53 respectively (which represents an improvement of 0.21 and 0.13 for each dataset).","","2021 8th International Conference on Future Internet of Things and Cloud (FiCloud)","","10.1109/FiCloud49777.2021.00044","https://consensus.app/papers/similarity-constraints-datasets-approach-antunes/e90628733950525ebfd59eaed6ae7989/"
