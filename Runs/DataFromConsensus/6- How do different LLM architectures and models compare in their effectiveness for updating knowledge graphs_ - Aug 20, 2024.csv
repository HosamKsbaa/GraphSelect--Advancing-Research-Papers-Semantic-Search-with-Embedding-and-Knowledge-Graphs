Title,Takeaway,Authors,Year,Citations,Abstract,Study Type,Journal,Journal SJR Quartile,DOI,Consensus Link
"Enhancing Knowledge Graph Construction Using Large Language Models","Advanced Large Language Models (LLM) like ChatGPT and REBEL can improve the accuracy of creating Knowledge Graphs from unstructured text, with potential for automatic ontology creation.","M. Trajanoska, Riste Stojanov, D. Trajanov",2023,8,"The growing trend of Large Language Models (LLM) development has attracted significant attention, with models for various applications emerging consistently. However, the combined application of Large Language Models with semantic technologies for reasoning and inference is still a challenging task. This paper analyzes how the current advances in foundational LLM, like ChatGPT, can be compared with the specialized pretrained models, like REBEL, for joint entity and relation extraction. To evaluate this approach, we conducted several experiments using sustainability-related text as our use case. We created pipelines for the automatic creation of Knowledge Graphs from raw texts, and our findings indicate that using advanced LLM models can improve the accuracy of the process of creating these graphs from unstructured text. Furthermore, we explored the potential of automatic ontology creation using foundation LLM models, which resulted in even more relevant and accurate knowledge graphs.","","ArXiv","","10.48550/arXiv.2305.04676","https://consensus.app/papers/enhancing-knowledge-graph-construction-using-large-trajanoska/80ffe83041735fdf94bf4b60dd32ba1a/"
"Exploring Large Language Models for Knowledge Graph Completion","Large Language Models (LLM) can effectively complete knowledge graphs, improving performance in tasks like triple classification and relation prediction.","Liang Yao, Jiazhen Peng, Chengsheng Mao, Yuan Luo",2023,3,"Knowledge graphs play a vital role in numerous artificial intelligence tasks, yet they frequently face the issue of incompleteness. In this study, we explore utilizing Large Language Models (LLM) for knowledge graph completion. We consider triples in knowledge graphs as text sequences and introduce an innovative framework called Knowledge Graph LLM (KG-LLM) to model these triples. Our technique employs entity and relation descriptions of a triple as prompts and utilizes the response for predictions. Experiments on various benchmark knowledge graphs demonstrate that our method attains state-of-the-art performance in tasks such as triple classification and relation prediction. We also find that fine-tuning relatively smaller models (e.g., LLaMA-7B, ChatGLM-6B) outperforms recent ChatGPT and GPT-4.","","ArXiv","","10.48550/arXiv.2308.13916","https://consensus.app/papers/exploring-language-models-knowledge-graph-completion-yao/b8b9041413ee5284b6831190235f0c7b/"
"Unifying Large Language Models and Knowledge Graphs: A Roadmap","Unifying large language models and knowledge graphs can enhance their abilities in natural language processing and artificial intelligence.","Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, Xindong Wu",2023,95,"Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolving by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.","","ArXiv","","10.48550/arXiv.2306.08302","https://consensus.app/papers/unifying-language-models-knowledge-graphs-roadmap-pan/6b1e377e73515d669415887e670803d0/"
"LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities","GPT-4 outperforms ChatGPT in most Knowledge Graph construction tasks, and AutoKG, a multi-agent-based approach, shows potential for future advancements in Knowledge Graph construction and reasoning.","Yuqi Zhu, Xiaohan Wang, Jing Chen, Shuofei Qiao, Yixin Ou, Yunzhi Yao, Shumin Deng, Huajun Chen, Ningyu Zhang",2023,17,"This paper presents an exhaustive quantitative and qualitative evaluation of Large Language Models (LLMs) for Knowledge Graph (KG) construction and reasoning. We employ eight distinct datasets that encompass aspects including entity, relation and event extraction, link prediction, and question answering. Empirically, our findings suggest that GPT-4 outperforms ChatGPT in the majority of tasks and even surpasses fine-tuned models in certain reasoning and question-answering datasets. Moreover, our investigation extends to the potential generalization ability of LLMs for information extraction, which culminates in the presentation of the Virtual Knowledge Extraction task and the development of the VINE dataset. Drawing on these empirical findings, we further propose AutoKG, a multi-agent-based approach employing LLMs for KG construction and reasoning, which aims to chart the future of this field and offer exciting opportunities for advancement. We anticipate that our research can provide invaluable insights for future undertakings of KG\footnote{Code and datasets will be available in https://github.com/zjunlp/AutoKG.","","ArXiv","","10.48550/arXiv.2305.13168","https://consensus.app/papers/llms-knowledge-graph-construction-reasoning-recent-zhu/bc301ddc6b135419a9743367f3b5545c/"
"Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs","Large Language Models (LLMs) show potential in learning on graphs, particularly node classification, by enhancing node text attributes and acting as standalone predictors.","Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Haifang Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, Jiliang Tang",2023,39,"Learning on Graphs has attracted immense attention due to its wide real-world applications. The most popular pipeline for learning on graphs with textual node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes shallow text embedding as initial node representations, which has limitations in general knowledge and profound semantic understanding. In recent years, Large Language Models (LLMs) have been proven to possess extensive common knowledge and powerful semantic comprehension abilities that have revolutionized existing workflows to handle text data. In this paper, we aim to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigate two possible pipelines: LLMs-as-Enhancers and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text attributes with their massive knowledge and then generate predictions through GNNs. The latter attempts to directly employ LLMs as standalone predictors. We conduct comprehensive and systematical studies on these two pipelines under various settings. From comprehensive empirical results, we make original observations and find new insights that open new possibilities and suggest promising directions to leverage LLMs for learning on graphs. Our codes and datasets are available at https://github.com/CurryTang/Graph-LLM.","","ArXiv","","10.48550/arXiv.2307.03393","https://consensus.app/papers/exploring-potential-large-language-models-llms-learning-chen/37c1ba56008f5284b2b53c017f69ae3c/"
"Iterative Zero-Shot LLM Prompting for Knowledge Graph Construction","Our innovative knowledge graph generation approach, using large language models like GPT-3.5, provides scalable and versatile construction without the need for external resources or human expertise.","S. Carta, Alessandro Giuliani, L. piano, Alessandro Sebastian Podda, Livio Pompianu, Sandro Gabriele Tiddia",2023,6,"In the current digitalization era, capturing and effectively representing knowledge is crucial in most real-world scenarios. In this context, knowledge graphs represent a potent tool for retrieving and organizing a vast amount of information in a properly interconnected and interpretable structure. However, their generation is still challenging and often requires considerable human effort and domain expertise, hampering the scalability and flexibility across different application fields. This paper proposes an innovative knowledge graph generation approach that leverages the potential of the latest generative large language models, such as GPT-3.5, that can address all the main critical issues in knowledge graph building. The approach is conveyed in a pipeline that comprises novel iterative zero-shot and external knowledge-agnostic strategies in the main stages of the generation process. Our unique manifold approach may encompass significant benefits to the scientific community. In particular, the main contribution can be summarized by: (i) an innovative strategy for iteratively prompting large language models to extract relevant components of the final graph; (ii) a zero-shot strategy for each prompt, meaning that there is no need for providing examples for""guiding""the prompt result; (iii) a scalable solution, as the adoption of LLMs avoids the need for any external resources or human expertise. To assess the effectiveness of our proposed model, we performed experiments on a dataset that covered a specific domain. We claim that our proposal is a suitable solution for scalable and versatile knowledge graph construction and may be applied to different and novel contexts.","","ArXiv","","10.48550/arXiv.2307.01128","https://consensus.app/papers/zeroshot-prompting-knowledge-graph-construction-carta/9ef7b532b94652c5b64e01bca5988334/"
"Developing a Scalable Benchmark for Assessing Large Language Models in Knowledge Graph Engineering","Our LLM-KG-Bench framework effectively evaluates Large Language Models in knowledge graph engineering, improving their performance in syntax, error correction, facts extraction, and dataset generation.","Lars Meyer, Johannes Frey, K. Junghanns, Felix Brei, Kirill Bulert, Sabine Grunder-Fahrer, Michael Martin",2023,1,"As the field of Large Language Models (LLMs) evolves at an accelerated pace, the critical need to assess and monitor their performance emerges. We introduce a benchmarking framework focused on knowledge graph engineering (KGE) accompanied by three challenges addressing syntax and error correction, facts extraction and dataset generation. We show that while being a useful tool, LLMs are yet unfit to assist in knowledge graph generation with zero-shot prompting. Consequently, our LLM-KG-Bench framework provides automatic evaluation and storage of LLM responses as well as statistical data and visualization tools to support tracking of prompt engineering and model performance.","","ArXiv","","10.48550/arXiv.2308.16622","https://consensus.app/papers/developing-benchmark-assessing-large-language-models-meyer/6e8f7907779352c5989ffd3ea9cb5054/"
"Graph Neural Prompting with Large Language Models","Graph Neural Prompting (GNP) effectively enhances pre-trained large language models by learning beneficial knowledge from knowledge graphs, improving performance in commonsense and biomedical reasoning tasks.","Yijun Tian, Huan Song, Zichen Wang, Haozhu Wang, Ziqing Hu, Fang Wang, N. Chawla, Panpan Xu",2023,4,"Large language models (LLMs) have shown remarkable generalization capability with exceptional performance in various language modeling tasks. However, they still exhibit inherent limitations in precisely capturing and returning grounded knowledge. While existing work has explored utilizing knowledge graphs (KGs) to enhance language modeling via joint training and customized model architectures, applying this to LLMs is problematic owing to their large number of parameters and high computational cost. Therefore, how to enhance pre-trained LLMs using grounded knowledge, e.g., retrieval-augmented generation, remains an open question. In this work, we propose Graph Neural Prompting (GNP), a novel plug-and-play method to assist pre-trained LLMs in learning beneficial knowledge from KGs. GNP encompasses various designs, including a standard graph neural network encoder, a cross-modality pooling module, a domain projector, and a self-supervised link prediction objective. Extensive experiments on multiple datasets demonstrate the superiority of GNP on both commonsense and biomedical reasoning tasks across different LLM sizes and settings. Code is available at https://github.com/meettyj/GNP.","","ArXiv","","10.48550/arXiv.2309.15427","https://consensus.app/papers/graph-neural-prompting-large-language-models-tian/21c64cc66cec5acfaa1c446b0d93e63a/"
"KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models","KG-GPT, a multi-purpose framework leveraging large language models for complex reasoning tasks on knowledge graphs, performs competitively and robustly, outperforming fully-supervised models.","Jiho Kim, Yeonsu Kwon, Yohan Jo, Edward Choi",2023,1,"While large language models (LLMs) have made considerable advancements in understanding and generating unstructured text, their application in structured data remains underexplored. Particularly, using LLMs for complex reasoning tasks on knowledge graphs (KGs) remains largely untouched. To address this, we propose KG-GPT, a multi-purpose framework leveraging LLMs for tasks employing KGs. KG-GPT comprises three steps: Sentence Segmentation, Graph Retrieval, and Inference, each aimed at partitioning sentences, retrieving relevant graph components, and deriving logical conclusions, respectively. We evaluate KG-GPT using KG-based fact verification and KGQA benchmarks, with the model showing competitive and robust performance, even outperforming several fully-supervised models. Our work, therefore, marks a significant step in unifying structured and unstructured data processing within the realm of LLMs.","","ArXiv","","10.48550/arXiv.2310.11220","https://consensus.app/papers/kggpt-general-framework-reasoning-knowledge-graphs-using-kim/4b0441eb574255409550f1a6a4a98432/"
"Knowledge Graph Completion Models are Few-shot Learners: An Empirical Study of Relation Labeling in E-commerce with LLMs","Large Language Models (LLMs) significantly outperform existing Knowledge Graph completion models in relation labeling for e-commerce, achieving competitive performance compared to humans with limited labeled data.","Jiaoayan Chen, Luyi Ma, Xiaohan Li, Nikhil Thakurdesai, Jianpeng Xu, Jason H. D. Cho, Kaushiki Nag, Evren Korpeoglu, Sushant Kumar, Kannan Achan",2023,6,"Knowledge Graphs (KGs) play a crucial role in enhancing e-commerce system performance by providing structured information about entities and their relationships, such as complementary or substitutable relations between products or product types, which can be utilized in recommender systems. However, relation labeling in KGs remains a challenging task due to the dynamic nature of e-commerce domains and the associated cost of human labor. Recently, breakthroughs in Large Language Models (LLMs) have shown surprising results in numerous natural language processing tasks. In this paper, we conduct an empirical study of LLMs for relation labeling in e-commerce KGs, investigating their powerful learning capabilities in natural language and effectiveness in predicting relations between product types with limited labeled data. We evaluate various LLMs, including PaLM and GPT-3.5, on benchmark datasets, demonstrating their ability to achieve competitive performance compared to humans on relation labeling tasks using just 1 to 5 labeled examples per relation. Additionally, we experiment with different prompt engineering techniques to examine their impact on model performance. Our results show that LLMs significantly outperform existing KG completion models in relation labeling for e-commerce KGs and exhibit performance strong enough to replace human labeling.","","ArXiv","","10.48550/arXiv.2305.09858","https://consensus.app/papers/knowledge-graph-completion-models-fewshot-learners-chen/a1835ade5e9e5770b30caa948cad571b/"
