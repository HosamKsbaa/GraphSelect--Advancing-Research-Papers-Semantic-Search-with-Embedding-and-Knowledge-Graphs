Title,Takeaway,Authors,Year,Citations,Abstract,Study Type,Journal,Journal SJR Quartile,DOI,Consensus Link
"ScanQA: 3D Question Answering for Spatial Scene Understanding","ScanQA model effectively performs object-grounded question answering in 3D environments, enabling accurate object localization and alignment in indoor scenes from textual questions.","Daich Azuma, Taiki Miyanishi, Shuhei Kurita, M. Kawanabe",2021,53,"We propose a new 3D spatial understanding task for 3D question answering (3D-QA). In the 3D-QA task, models receive visual information from the entire 3D scene of a rich RGB-D indoor scan and answer given textual questions about the 3D scene. Unlike the 2D-question answering of visual question answering, the conventional 2D-QA models suffer from problems with spatial understanding of object alignment and directions and fail in object localization from the textual questions in 3D-QA. We propose a baseline model for 3D-QA, called the ScanQA11https://github.com/ATR-DBI/ScanQA, which learns a fused descriptor from 3D object proposals and encoded sentence embeddings. This learned descriptor correlates language expressions with the underlying geometric features of the 3D scan and facilitates the regression of 3D bounding boxes to determine the described objects in textual questions. We collected human-edited question-answer pairs with free-form answers grounded in 3D objects in each 3D scene. Our new ScanQA dataset contains over 41k question-answer pairs from 800 indoor scenes obtained from the ScanNet dataset. To the best of our knowledge, ScanQA is the first large-scale effort to perform object-grounded question answering in 3D environments.","","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","10.1109/CVPR52688.2022.01854","https://consensus.app/papers/scanqa-question-answering-spatial-scene-understanding-azuma/555f825f7ee55af8be7efac6bd4c5b50/"
"3D Question Answering","Our 3DQA-TR framework effectively predicts target answers for 3D real-world scenarios, outperforming existing VQA frameworks on the ScanQA dataset.","Shuquan Ye, Dongdong Chen, Songfang Han, Jing Liao",2021,20,"Visual question answering (VQA) has experienced tremendous progress in recent years. However, most efforts have only focused on 2D image question-answering tasks. In this paper, we extend VQA to its 3D counterpart, 3D question answering (3DQA), which can facilitate a machine's perception of 3D real-world scenarios. Unlike 2D image VQA, 3DQA takes the color point cloud as input and requires both appearance and 3D geometrical comprehension to answer the 3D-related questions. To this end, we propose a novel transformer-based 3DQA framework ""3DQA-TR"", which consists of two encoders to exploit the appearance and geometry information, respectively. Finally, the multi-modal information about the appearance, geometry, and linguistic question can attend to each other via a 3D-linguistic Bert to predict the target answers. To verify the effectiveness of our proposed 3DQA framework, we further develop the first 3DQA dataset ""ScanQA"", which builds on the ScanNet dataset and contains over 10 K question-answer pairs for 806 scenes. To the best of our knowledge, ScanQA is the first large-scale dataset with natural-language questions and free-form answers in 3D environments that is fully human-annotated. We also use several visualizations and experiments to investigate the astonishing diversity of the collected questions and the significant differences between this task from 2D VQA and 3D captioning. Extensive experiments on this dataset demonstrate the obvious superiority of our proposed 3DQA framework over state-of-the-art VQA frameworks and the effectiveness of our major designs. Our code and dataset will be made publicly available to facilitate research in this direction. The code and data are available at http://shuquanye.com/3DQA\_website/.","","IEEE transactions on visualization and computer graphics","1","10.1109/TVCG.2022.3225327","https://consensus.app/papers/question-answering-ye/5d5d6ee9233a59d8b3efb20fa7acb633/"
"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering","Our balanced VQA dataset shows that state-of-the-art models perform significantly worse on it, suggesting they exploit language priors, and our novel interpretable model helps build trust among users.","Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh",2016,2006,"The problem of visual question answering (VQA) is of significant importance both as a challenging research question and for the rich set of applications it enables. In this context, however, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in VQA models that ignore visual information, leading to an inflated sense of their capability. We propose to counter these language priors for the task of VQA and make vision (the V in VQA) matter! Specifically, we balance the popular VQA dataset (Antol et al., in: ICCV, 2015) by collecting complementary images such that every question in our balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. Our dataset is by construction more balanced than the original VQA dataset and has approximately twice the number of image-question pairs. Our complete balanced dataset is publicly available at http://visualqa.org/ as part of the 2nd iteration of the VQA Dataset and Challenge (VQA v2.0). We further benchmark a number of state-of-art VQA models on our balanced dataset. All models perform significantly worse on our balanced dataset, suggesting that these models have indeed learned to exploit language priors. This finding provides the first concrete empirical evidence for what seems to be a qualitative sense among practitioners. We also present interesting insights from analysis of the participant entries in VQA Challenge 2017, organized by us on the proposed VQA v2.0 dataset. The results of the challenge were announced in the 2nd VQA Challenge Workshop at the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017. Finally, our data collection protocol for identifying complementary images enables us to develop a novel interpretable model, which in addition to providing an answer to the given (image, question) pair, also provides a counter-example based explanation. Specifically, it identifies an image that is similar to the original image, but it believes has a different answer to the same question. This can help in building trust for machines among their users.","","International Journal of Computer Vision","1","10.1007/s11263-018-1116-0","https://consensus.app/papers/making-matter-elevating-role-image-understanding-visual-goyal/bc57b53798e1513fbae379a2ebcd5a8c/"
"Visual Question Answering on Image Sets","ISVQA can effectively answer questions about objects and relationships in images, with potential applications in indoor and outdoor scenes.","Ankan Bansal, Yuting Zhang, R. Chellappa",2020,20,"We introduce the task of Image-Set Visual Question Answering (ISVQA), which generalizes the commonly studied single-image VQA problem to multi-image settings. Taking a natural language question and a set of images as input, it aims to answer the question based on the content of the images. The questions can be about objects and relationships in one or more images or about the entire scene depicted by the image set. To enable research in this new topic, we introduce two ISVQA datasets - indoor and outdoor scenes. They simulate the real-world scenarios of indoor image collections and multiple car-mounted cameras, respectively. The indoor-scene dataset contains 91,479 human annotated questions for 48,138 image sets, and the outdoor-scene dataset has 49,617 questions for 12,746 image sets. We analyze the properties of the two datasets, including question-and-answer distributions, types of questions, biases in dataset, and question-image dependencies. We also build new baseline models to investigate new research challenges in ISVQA.","","ArXiv","","10.1007/978-3-030-58589-1_4","https://consensus.app/papers/question-answering-image-sets-bansal/c70882d8042356148910ee9ef6e31fa7/"
"SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine","SearchQA is a large-scale dataset for machine comprehension, augmented with context from a search engine, showing a meaningful gap between human and machine performance.","Matthew Dunn, Levent Sagun, Mike Higgins, V. U. Güney, Volkan Cirik, Kyunghyun Cho",2017,392,"We publicly release a new large-scale dataset, called SearchQA, for machine comprehension, or question-answering. Unlike recently released datasets, such as DeepMind CNN/DailyMail and SQuAD, the proposed SearchQA was constructed to reflect a full pipeline of general question-answering. That is, we start not from an existing article and generate a question-answer pair, but start from an existing question-answer pair, crawled from J! Archive, and augment it with text snippets retrieved by Google. Following this approach, we built SearchQA, which consists of more than 140k question-answer pairs with each pair having 49.6 snippets on average. Each question-answer-context tuple of the SearchQA comes with additional meta-data such as the snippet's URL, which we believe will be valuable resources for future research. We conduct human evaluation as well as test two baseline methods, one simple word selection and the other deep learning based, on the SearchQA. We show that there is a meaningful gap between the human and machine performances. This suggests that the proposed dataset could well serve as a benchmark for question-answering.","","ArXiv","","","https://consensus.app/papers/searchqa-dataset-augmented-context-search-engine-dunn/a69a1f272a645a20afdbfff44f9b69fd/"
"Fine-Grained Feature Extraction from Indoor Data to Enhance Visual Question Answering","The proposed VQA model, using Stacked-Attention Network (SAN) and customized attention layers, effectively extracts fine-grained features from indoor images and questions, outperforming existing models.","Rajat Subraya Gaonkar, V. A. Pruthvi, L. Prem Kumar, Rohan Madan Ghodake, M. Raghavendra, B. N. Krupa",2023,0,"Current Visual Question Answering (VQA) models have limited contribution in the area of indoor images. In this paper, the proposed VQA model extracts image and question features from pre-trained VGG16 or ResNetl52v2 and Glove models respectively. Then these features are fed to the novel approach proposed in this paper where Stacked-Attention Network (SAN) which includes customized attention layers with Self-Focus Network (SFN) is used to extract fine-grained features of both the image and question. With these features, the answer prediction model generates an answer. This model is trained and tested on a subset of VQA v2 dataset. It gives a comparable performance when compared with other existing models.","","2023 7th International Conference on Intelligent Computing and Control Systems (ICICCS)","","10.1109/ICICCS56967.2023.10142434","https://consensus.app/papers/finegrained-feature-extraction-indoor-data-enhance-gaonkar/4faeb63e6e49528c85714a62867f825c/"
"TAG: Boosting Text-VQA via Text-aware Visual Question-answer Generation","TAG, a text-aware visual question-answer generation architecture, effectively improves Text-VQA performance by utilizing underexplored scene text information and outperforming state-of-the-art approaches without extra labeling effort.","Jun Wang, M. Gao, Yuqian Hu, Ramprasaath R. Selvaraju, Chetan Ramaiah, Ran Xu, J. JáJá, Larry S. Davis",2022,12,"Text-VQA aims at answering questions that require understanding the textual cues in an image. Despite the great progress of existing Text-VQA methods, their performance suffers from insufficient human-labeled question-answer (QA) pairs. However, we observe that, in general, the scene text is not fully exploited in the existing datasets -- only a small portion of the text in each image participates in the annotated QA activities. This results in a huge waste of useful information. To address this deficiency, we develop a new method to generate high-quality and diverse QA pairs by explicitly utilizing the existing rich text available in the scene context of each image. Specifically, we propose, TAG, a text-aware visual question-answer generation architecture that learns to produce meaningful, and accurate QA samples using a multimodal transformer. The architecture exploits underexplored scene text information and enhances scene understanding of Text-VQA models by combining the generated QA pairs with the initial training data. Extensive experimental results on two well-known Text-VQA benchmarks (TextVQA and ST-VQA) demonstrate that our proposed TAG effectively enlarges the training data that helps improve the Text-VQA performance without extra labeling effort. Moreover, our model outperforms state-of-the-art approaches that are pre-trained with extra large-scale data. Code is available at https://github.com/HenryJunW/TAG.","","ArXiv","","10.48550/arXiv.2208.01813","https://consensus.app/papers/boosting-textvqa-textaware-visual-questionanswer-wang/8e6d9ec86ea45d91b1e7e6e734acbe8b/"
"VQA: Visual Question Answering","Visual Question Answering (VQA) requires detailed understanding of images and complex reasoning, making it suitable for real-world scenarios like helping the visually impaired.","Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. L. Zitnick, Devi Parikh, Dhruv Batra",2015,4244,"We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing $$\sim $$∼0.25 M images, $$\sim $$∼0.76 M questions, and $$\sim $$∼10 M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines and methods for VQA are provided and compared with human performance. Our VQA demo is available on CloudCV (http://cloudcv.org/vqa).","","International Journal of Computer Vision","1","10.1007/s11263-016-0966-6","https://consensus.app/papers/question-answering-agrawal/162adb4f84185299818ee2e2c6b9eba8/"
"InStereo2K: a large real dataset for stereo matching in indoor scenes","The InStereo2K dataset significantly improves the performance of recent deep neural networks on the Middlebury 2014 dataset for stereo matching in indoor scenes.","Wei Bao, Wen Wang, Yuhua Xu, Yulan Guo, Siyu Hong, Xiaohu Zhang",2020,38,"Deep neural networks have shown great success in stereo matching in recent years. On the KITTI datasets, most top performing methods are based on neural networks. However, on the Middlebury datasets, these methods usually do not perform well. The KITTI datasets are collected in outdoor scenes while the Middlebury datasets are collected in indoor scenes. It is commonly believed that the community still lacks a large labelled dataset for stereo matching in indoor scenes. In this paper, we introduce a new stereo dataset called InStereo2K. It contains 2050 pairs of stereo images with highly accurate groundtruth disparity maps, including 2000 pairs for training and 50 pairs for test. Experimental results show that our dataset can significantly improve the performance of several latest networks (including StereoNet and PSMNet) on the Middlebury 2014 dataset. The large scale, high accuracy and rich diversity of the proposed InStereo2K dataset provide new opportunities to researchers in the area of stereo matching and beyond. It also takes end-to-end stereo matching methods a step towards practical applications.","","Science China Information Sciences","1","10.1007/s11432-019-2803-x","https://consensus.app/papers/instereo2k-dataset-stereo-matching-scenes-bao/9bcdd953f91e5fcd9da6a023166c89e2/"
"NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario","NuScenes-QA is a balanced large-scale benchmark for visual question answering in autonomous driving scenarios, addressing challenges in multi-modal, multi-frame, and moving outdoor scenes.","Tianwen Qian, Jingjing Chen, Linhai Zhuo, Yang Jiao, Yueping Jiang",2023,14,"We introduce a novel visual question answering (VQA) task in the context of autonomous driving, aiming to answer natural language questions based on street-view clues. Compared to traditional VQA tasks, VQA in autonomous driving scenario presents more challenges. Firstly, the raw visual data are multi-modal, including images and point clouds captured by camera and LiDAR, respectively. Secondly, the data are multi-frame due to the continuous, real-time acquisition. Thirdly, the outdoor scenes exhibit both moving foreground and static background. Existing VQA benchmarks fail to adequately address these complexities. To bridge this gap, we propose NuScenes-QA, the first benchmark for VQA in the autonomous driving scenario, encompassing 34K visual scenes and 460K question-answer pairs. Specifically, we leverage existing 3D detection annotations to generate scene graphs and design question templates manually. Subsequently, the question-answer pairs are generated programmatically based on these templates. Comprehensive statistics prove that our NuScenes-QA is a balanced large-scale benchmark with diverse question formats. Built upon it, we develop a series of baselines that employ advanced 3D detection and VQA techniques. Our extensive experiments highlight the challenges posed by this new task. Codes and dataset are available at https://github.com/qiantianwen/NuScenes-QA.","","ArXiv","","10.48550/arXiv.2305.14836","https://consensus.app/papers/nuscenesqa-multimodal-visual-question-answering-qian/08c602453e9c52caa1705426e9449676/"
