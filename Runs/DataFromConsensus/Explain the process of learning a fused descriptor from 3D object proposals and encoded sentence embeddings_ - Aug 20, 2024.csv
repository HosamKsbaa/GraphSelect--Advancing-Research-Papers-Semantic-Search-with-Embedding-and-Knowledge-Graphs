Title,Takeaway,Authors,Year,Citations,Abstract,Study Type,Journal,Journal SJR Quartile,DOI,Consensus Link
"ScanRefer: 3D Object Localization in RGB-D Scans using Natural Language","ScanRefer effectively localizes 3D objects in RGB-D scans using natural language descriptions, enabling the first large-scale effort to perform object localization via natural language expressions in 3D.","Dave Zhenyu Chen, Angel X. Chang, M. Nießner",2019,159,"We introduce the task of 3D object localization in RGB-D scans using natural language descriptions. As input, we assume a point cloud of a scanned 3D scene along with a free-form description of a specified target object. To address this task, we propose ScanRefer, learning a fused descriptor from 3D object proposals and encoded sentence embeddings. This fused descriptor correlates language expressions with geometric features, enabling regression of the 3D bounding box of a target object. We also introduce the ScanRefer dataset, containing 51,583 descriptions of 11,046 objects from 800 ScanNet scenes. ScanRefer is the first large-scale effort to perform object localization via natural language expression directly in 3D.","","ArXiv","","10.1007/978-3-030-58565-5_13","https://consensus.app/papers/scanrefer-object-localization-rgbd-scans-using-natural-chen/16c4b57ba7d85cd58388fbfef57c12c0/"
"ScanQA: 3D Question Answering for Spatial Scene Understanding","ScanQA model effectively performs object-grounded question answering in 3D environments, enabling accurate object localization and alignment in indoor scenes from textual questions.","Daich Azuma, Taiki Miyanishi, Shuhei Kurita, M. Kawanabe",2021,53,"We propose a new 3D spatial understanding task for 3D question answering (3D-QA). In the 3D-QA task, models receive visual information from the entire 3D scene of a rich RGB-D indoor scan and answer given textual questions about the 3D scene. Unlike the 2D-question answering of visual question answering, the conventional 2D-QA models suffer from problems with spatial understanding of object alignment and directions and fail in object localization from the textual questions in 3D-QA. We propose a baseline model for 3D-QA, called the ScanQA11https://github.com/ATR-DBI/ScanQA, which learns a fused descriptor from 3D object proposals and encoded sentence embeddings. This learned descriptor correlates language expressions with the underlying geometric features of the 3D scan and facilitates the regression of 3D bounding boxes to determine the described objects in textual questions. We collected human-edited question-answer pairs with free-form answers grounded in 3D objects in each 3D scene. Our new ScanQA dataset contains over 41k question-answer pairs from 800 indoor scenes obtained from the ScanNet dataset. To the best of our knowledge, ScanQA is the first large-scale effort to perform object-grounded question answering in 3D environments.","","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","10.1109/CVPR52688.2022.01854","https://consensus.app/papers/scanqa-question-answering-spatial-scene-understanding-azuma/555f825f7ee55af8be7efac6bd4c5b50/"
"Learning Discriminative and Generative Shape Embeddings for Three-Dimensional Shape Retrieval","The ERFA-Net model effectively combines discriminative and generative shape embeddings for 3D shape retrieval, achieving state-of-the-art performance on large-scale datasets like ModelNet and ShapeNetCore55.","Cheng Xu, B. Leng, B. Chen, Cheng Zhang, Xiaocheng Zhou",2020,6,"As an important solution for 3D shape retrieval, a multi-view shape descriptor has achieved impressive performance. One crucial part of view-based shape descriptors is to interpret 3D structures through various 2D observations. Most existing methods like MVCNN believe that a strong classification model trained with deep learning, can often provide an efficient shape embedding for 3D shape retrieval. However, these methods pay much attention to discriminative models and none of them necessarily incorporate the underlying 3D properties of the objects from 2D images. In this paper, we present a novel encoder-decoder recurrent feature aggregation network (ERFA-Net) to address this problem. Aiming at emphasizing the 3D properties of 3D shapes in the fusion of multiple view features, 3D properties prediction tasks are introduced into the 3D shape retrieval. Specifically, an image sequence of the shape is recurrently aggregated into a discriminative shape embedding based on LSTM network, and then this latent shape embedding is trained to predict the original voxel grids and estimate images of unseen viewpoints. This generation task gives an effective supervision which makes the network exploit 3D properties of shapes through various 2D images. Our method achieves the state-of-the-art performance for 3D shape retrieval, on two large-scale 3D shape datasets, ModelNet and ShapeNetCore55. Extensive experiments show that the proposed 3D representation performs robust discrimination against view occlusion, and strong generation ability for various 3D shape tasks.","","IEEE Transactions on Multimedia","1","10.1109/TMM.2019.2957933","https://consensus.app/papers/learning-discriminative-generative-shape-embeddings-xu/4652ed28ef6650758bb9c7e50d71a274/"
"LCD: Learned Cross-Domain Descriptors for 2D-3D Matching","Our dual auto-encoder neural network effectively learns local cross-domain descriptors for 2D-3D matching, proving more discriminative than individual training in 2D and 3D domains.","Quang-Hieu Pham, Mikaela Angelina Uy, Binh-Son Hua, D. Nguyen, G. Roig, Sai-Kit Yeung",2019,46,"In this work, we present a novel method to learn a local cross-domain descriptor for 2D image and 3D point cloud matching. Our proposed method is a dual auto-encoder neural network that maps 2D and 3D input into a shared latent space representation. We show that such local cross-domain descriptors in the shared embedding are more discriminative than those obtained from individual training in 2D and 3D domains. To facilitate the training process, we built a new dataset by collecting ≈ 1.4 millions of 2D-3D correspondences with various lighting conditions and settings from publicly available RGB-D scenes. Our descriptor is evaluated in three main experiments: 2D-3D matching, cross-domain retrieval, and sparse-to-dense depth estimation. Experimental results confirm the robustness of our approach as well as its competitive performance not only in solving cross-domain tasks but also in being able to generalize to solve sole 2D and 3D tasks. Our dataset and code are released publicly at https://hkust-vgd.github.io/lcd.","","","","10.1609/AAAI.V34I07.6859","https://consensus.app/papers/learned-crossdomain-descriptors-2d3d-matching-pham/272797d1f80051de8dcde7b57feb9d08/"
"Efficient Recognition of Highly Similar 3D Objects in Range Images","This paper proposes a novel method for efficient recognition of highly similar 3D objects in range images, combining feature embedding, novel similarity measures, and a support vector machine-based learning technique.","Hui Chen, B. Bhanu",2009,81,"Most existing work in 3D object recognition in computer vision has been on recognizing dissimilar objects using a small database. For rapid indexing and recognition of highly similar objects, this paper proposes a novel method which combines the feature embedding for the fast retrieval of surface descriptors, novel similarity measures for correspondence and a support vector machine (SVM)-based learning technique for ranking the hypotheses. The local surface patch (LSP) representation is used to find the correspondences between a model-test pair. Due to its high dimensionality, an embedding algorithm is used that maps the feature vectors to a low-dimensional space where distance relationships are preserved. By searching the nearest neighbors in low dimensions, the similarity between a model-test pair is computed using the novel features. The similarities for all model-test pairs are ranked using the learning algorithm to generate a short list of candidate models for verification. The verification is performed by aligning a model with the test object. The experimental results, on the UND dataset (302 subjects with 604 images) and the UCR dataset (155 subjects with 902 images) that contain 3D human ears, are presented and compared with the geometric hashing technique to demonstrate the efficiency and effectiveness of the proposed approach.","","IEEE Transactions on Pattern Analysis and Machine Intelligence","1","10.1109/TPAMI.2008.176","https://consensus.app/papers/recognition-highly-similar-objects-range-images-chen/04b778f7bb865d1496ad345c6756d267/"
"Improving 3D Object Detection with Channel-wise Transformer","Our two-stage 3D object detection framework (CT3D) outperforms state-of-the-art detectors with superior performance and scalability, achieving 81.77% accuracy in the moderate car category on the KITTI test 3D detection benchmark.","Hualian Sheng, Sijia Cai, Yuan Liu, Bing Deng, Jianqiang Huang, Xiansheng Hua, Mingbi Zhao",2021,137,"Though 3D object detection from point clouds has achieved rapid progress in recent years, the lack of flexible and high-performance proposal refinement remains a great hurdle for existing state-of-the-art two-stage detectors. Previous works on refining 3D proposals have relied on human-designed components such as keypoints sampling, set abstraction and multi-scale feature fusion to produce powerful 3D object representations. Such methods, however, have limited ability to capture rich contextual dependencies among points. In this paper, we leverage the high-quality region proposal network and a Channel-wise Transformer architecture to constitute our two-stage 3D object detection framework (CT3D) with minimal hand-crafted design. The proposed CT3D simultaneously performs proposal-aware embedding and channel-wise context aggregation for the point features within each proposal. Specifically, CT3D uses proposal’s keypoints for spatial contextual modelling and learns attention propagation in the encoding module, mapping the proposal to point embeddings. Next, a new channel-wise decoding module enriches the query-key interaction via channel-wise re-weighting to effectively merge multi-level contexts, which contributes to more accurate object predictions. Extensive experiments demonstrate that our CT3D method has superior performance and excellent scalability. Remarkably, CT3D achieves the AP of 81.77% in the moderate car category on the KITTI test 3D detection benchmark, outperforms state-of-the-art 3D detectors.","","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","","10.1109/ICCV48922.2021.00274","https://consensus.app/papers/improving-object-detection-channelwise-transformer-sheng/2b1ae050bd025ff9aa0791155c69a840/"
"Hypergraph-Based Multi-Modal Representation for Open-Set 3D Object Retrieval.","The HGM 2R framework effectively learns generalized 3D object embeddings from multi-modal representations in open-set 3D object retrieval tasks, outperforming existing methods by 12.12%/12.88% on OS-MN40-core and OS-ABO-core datasets.","Yifan Feng, Shuyi Ji, Yu-Shen Liu, Shaoyi Du, Qionghai Dai, Yue Gao",2023,0,"The traditional 3D object retrieval (3DOR) task is under the close-set setting, which assumes the categories of objects in the retrieval stage are all seen in the training stage. Existing methods under this setting may tend to only lazily discriminate their categories, while not learning a generalized 3D object embedding. Under such circumstances, it is still a challenging and open problem in real-world applications due to the existence of various unseen categories. In this paper, we first introduce the open-set 3DOR task to expand the applications of the traditional 3DOR task. Then, we propose the Hypergraph-Based Multi-Modal Representation (HGM 2 R) framework to learn 3D object embeddings from multi-modal representations under the open-set setting. The proposed framework is composed of two modules, i.e., the Multi-Modal 3D Object Embedding (MM3DOE) module and the Structure-Aware and Invariant Knowledge Learning (SAIKL) module. By utilizing the collaborative information of modalities derived from the same 3D object, the MM3DOE module is able to overcome the distinction across different modality representations and generate unified 3D object embeddings. Then, the SAIKL module utilizes the constructed hypergraph structure to model the high-order correlation among 3D objects from both seen and unseen categories. The SAIKL module also includes a memory bank that stores typical representations of 3D objects. By aligning with those memory anchors in the memory bank, the aligned embeddings can integrate the invariant knowledge to exhibit a powerful generalized capacity toward unseen categories. We formally prove that hypergraph modeling has better representative capability on data correlation than graph modeling. We generate four multi-modal datasets for the open-set 3DOR task, i.e., OS-ESB-core, OS-NTU-core, OS-MN40-core, and OS-ABO-core, in which each 3D object contains three modality representations: multi-view, point clouds, and voxel. Experiments on these four datasets show that the proposed method can significantly outperform existing methods. In particular, the proposed method outperforms the state-of-the-art by 12.12%/12.88% in terms of mAP on the OS-MN40-core/OS-ABO-core dataset, respectively. Results and visualizations demonstrate that the proposed method can effectively extract the generalized 3D object embeddings on the open-set 3DOR task and achieve satisfactory performance.","","IEEE transactions on pattern analysis and machine intelligence","1","10.1109/TPAMI.2023.3332768","https://consensus.app/papers/hypergraphbased-multimodal-representation-openset-feng/23b18e8d01e55c66b876d70b72ff08c6/"
"Learning Disentangled Representation for Multi-View 3D Object Recognition","Our new method effectively learns a comprehensive descriptor for 3D objects from its views while maintaining robustness to view permutation variations, outperforming baseline models on ModelNet40, ModelNet10, and ShapeNetCore55 datasets.","Jingjia Huang, Wei Yan, Ge Li, Thomas H. Li, Shan Liu",2022,9,"3D object recognition is a hot research topic. Particularly, view-based methods, which represent a 3D object with a collection of its rendered views on the 2D domain, play an important role in this field. Currently, view-based researches tend to aggregate information from multiple views via pooling based strategies to endow the models with the characteristic of view permutation invariance, at the cost of inevitable loss of useful features. In this paper, we introduce a new method that learns a more comprehensive descriptor for a 3D object from its views while successfully keeping its robustness to the variation of view permutation. Our method disentangles the information in the set of multi-view images into a global category-related feature and a set of view-permutation related features. To unbind these two parts, an encode-decoder based disentangling architecture is proposed, which barely bring extra computations compared to the baseline model. Systematic experiments are conducted for this new method to demonstrates the effectiveness and the competitive performance based on ModelNet40, ModelNet10, and ShapeNetCore55 datasets. Codes for our paper will be released soon on “https://github.com/hjjpku/multi_view_sort”.","","IEEE Transactions on Circuits and Systems for Video Technology","1","10.1109/TCSVT.2021.3062190","https://consensus.app/papers/learning-disentangled-representation-multiview-object-huang/54b8000db3755ebf92c3b9c71026f628/"
"New image descriptors based on color, texture, shape, and wavelets for object and scene image classification","The proposed new image descriptors, based on color, texture, shape, and wavelets, achieve better image classification performance than popular ones like SIFT and H-descriptors.","S. Banerji, A. Sinha, Chengjun Liu",2013,75,"Abstract This paper presents new image descriptors based on color, texture, shape, and wavelets for object and scene image classification. First, a new three Dimensional Local Binary Patterns (3D-LBP) descriptor, which produces three new color images, is proposed for encoding both color and texture information of an image. The 3D-LBP images together with the original color image then undergo the Haar wavelet transform with further computation of the Histograms of Oriented Gradients (HOG) for encoding shape and local features. Second, a novel H-descriptor, which integrates the 3D-LBP and the HOG of its wavelet transform, is presented to encode color, texture, shape, as well as local information. Feature extraction for the H-descriptor is implemented by means of Principal Component Analysis (PCA) and Enhanced Fisher Model (EFM) and classification by the nearest neighbor rule for object and scene image classification. And finally, an innovative H-fusion descriptor is proposed by fusing the PCA features of the H-descriptors in seven color spaces in order to further incorporate color information. Experimental results using three datasets, the Caltech 256 object categories dataset, the UIUC Sports Event dataset, and the MIT Scene dataset, show that the proposed new image descriptors achieve better image classification performance than other popular image descriptors, such as the Scale Invariant Feature Transform (SIFT), the Pyramid Histograms of visual Words (PHOW), the Pyramid Histograms of Oriented Gradients (PHOG), Spatial Envelope, Color SIFT four Concentric Circles (C4CC), Object Bank, the Hierarchical Matching Pursuit, as well as LBP.","","Neurocomputing","1","10.1016/j.neucom.2013.02.014","https://consensus.app/papers/image-descriptors-based-color-texture-shape-wavelets-banerji/04f232cc50d85efc83794504e67fde37/"
"Revisiting Self-Similarity: Structural Embedding for Image Retrieval","The Structural Embedding Network (SENet) effectively encodes both visual and structural cues of an image, improving image retrieval performance.","Seongwon Lee, Suhyeon Lee, Hongje Seong, Euntai Kim",2023,2,"Despite advances in global image representation, existing image retrieval approaches rarely consider geometric structure during the global retrieval stage. In this work, we revisit the conventional self-similarity descriptor from a convolutional perspective, to encode both the visual and structural cues of the image to global image representation. Our proposed network, named Structural Embedding Network (SENet), captures the internal structure of the images and gradually compresses them into dense self-similarity descriptors while learning diverse structures from various images. These self-similarity descriptors and original image features are fused and then pooled into global embedding, so that global embedding can represent both geometric and visual cues of the image. Along with this novel structural embedding, our proposed network sets new state-of-the-art performances on several image retrieval benchmarks, convincing its robustness to look-alike distractors. The code and models are available: https://github.com/sungonce/SENet.","","2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","10.1109/CVPR52729.2023.02242","https://consensus.app/papers/revisiting-selfsimilarity-structural-embedding-image-lee/51c4f09f93925d6abfff5a0138e8df67/"
