Title,Takeaway,Authors,Year,Citations,Abstract,Study Type,Journal,Journal SJR Quartile,DOI,Consensus Link
"On the Sentence Embeddings from BERT for Semantic Textual Similarity","Our BERT-flow method significantly improves semantic textual similarity performance by transforming anisotropic sentence embeddings to smooth and isotropic Gaussian distributions through normalizing flows.","Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, Lei Li",2020,301,"Pre-trained contextual representations like BERT have achieved great success in natural language processing. However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences. In this paper, we argue that the semantic information in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks. The code is available at https://github.com/bohanli/BERT-flow.","","ArXiv","","10.18653/v1/2020.emnlp-main.733","https://consensus.app/papers/sentence-embeddings-bert-semantic-textual-similarity-li/292786363470552592656223a00da074/"
"SupMPN: Supervised Multiple Positives and Negatives Contrastive Learning Model for Semantic Textual Similarity","SupMPN outperforms state-of-the-art SimCSE and other previous models in semantic textual similarity tasks by focusing on semantic meaning instead of word frequency.","Somaiyeh Dehghan, M. Amasyali",2022,3,"Semantic Textual Similarity (STS) is an important task in the area of Natural Language Processing (NLP) that measures the similarity of the underlying semantics of two texts. Although pre-trained contextual embedding models such as Bidirectional Encoder Representations from Transformers (BERT) have achieved state-of-the-art performance on several NLP tasks, BERT-derived sentence embeddings have been proven to collapse in some way, i.e., sentence embeddings generated by BERT depend on the frequency of words. Therefore, almost all BERT-derived sentence embeddings are mapped into a small area and have a high cosine similarity. Hence, sentence embeddings generated by BERT are not so robust in the STS task as they cannot capture the full semantic meaning of the sentences. In this paper, we propose SupMPN: A Supervised Multiple Positives and Negatives Contrastive Learning Model, which accepts multiple hard-positive sentences and multiple hard-negative sentences simultaneously and then tries to bring hard-positive sentences closer, while pushing hard-negative sentences away from them. In other words, SupMPN brings similar sentences closer together in the representation space by discrimination among multiple similar and dissimilar sentences. In this way, SupMPN can learn the semantic meanings of sentences by contrasting among multiple similar and dissimilar sentences and can generate sentence embeddings based on the semantic meaning instead of the frequency of the words. We evaluate our model on standard STS and transfer-learning tasks. The results reveal that SupMPN outperforms state-of-the-art SimCSE and all other previous supervised and unsupervised models.","","Applied Sciences","2","10.3390/app12199659","https://consensus.app/papers/supmpn-supervised-multiple-positives-negatives-dehghan/bf26317505265385bc601732de812bb7/"
"Semantic Data Set Construction from Human Clustering and Spatial Arrangement","This study proposes a large-scale data set construction methodology using spatial multi-arrangement, which captures multi-way similarity judgments of verbs, potentially improving representation learning models for semantic clustering and similarity tasks.","Olga Majewska, Diana McCarthy, Jasper J. F. van den Bosch, N. Kriegeskorte, Ivan Vulic, A. Korhonen",2021,5,"Abstract Research into representation learning models of lexical semantics usually utilizes some form of intrinsic evaluation to ensure that the learned representations reflect human semantic judgments. Lexical semantic similarity estimation is a widely used evaluation method, but efforts have typically focused on pairwise judgments of words in isolation, or are limited to specific contexts and lexical stimuli. There are limitations with these approaches that either do not provide any context for judgments, and thereby ignore ambiguity, or provide very specific sentential contexts that cannot then be used to generate a larger lexical resource. Furthermore, similarity between more than two items is not considered. We provide a full description and analysis of our recently proposed methodology for large-scale data set construction that produces a semantic classification of a large sample of verbs in the first phase, as well as multi-way similarity judgments made within the resultant semantic classes in the second phase. The methodology uses a spatial multi-arrangement approach proposed in the field of cognitive neuroscience for capturing multi-way similarity judgments of visual stimuli. We have adapted this method to handle polysemous linguistic stimuli and much larger samples than previous work. We specifically target verbs, but the method can equally be applied to other parts of speech. We perform cluster analysis on the data from the first phase and demonstrate how this might be useful in the construction of a comprehensive verb resource. We also analyze the semantic information captured by the second phase and discuss the potential of the spatially induced similarity judgments to better reflect human notions of word similarity. We demonstrate how the resultant data set can be used for fine-grained analyses and evaluation of representation learning models on the intrinsic tasks of semantic clustering and semantic similarity. In particular, we find that stronger static word embedding methods still outperform lexical representations emerging from more recent pre-training methods, both on word-level similarity and clustering. Moreover, thanks to the data set’s vast coverage, we are able to compare the benefits of specializing vector representations for a particular type of external knowledge by evaluating FrameNet- and VerbNet-retrofitted models on specific semantic domains such as “Heat” or “Motion.”","","Computational Linguistics","1","10.1162/coli_a_00396","https://consensus.app/papers/data-construction-human-clustering-spatial-arrangement-majewska/7b3aad7ee1a45a52b56f7829b6098ebd/"
"Learning Contextual Embeddings for Structural Semantic Similarity using Categorical Information","Our semantic tree kernel approach, combining tree kernels and neural networks, significantly improves question and sentiment classification results compared to previous methods.","M. Nicosia, Alessandro Moschitti",2017,2,"Tree kernels (TKs) and neural networks are two effective approaches for automatic feature engineering. In this paper, we combine them by modeling context word similarity in semantic TKs. This way, the latter can operate subtree matching by applying neural-based similarity on tree lexical nodes. We study how to learn representations for the words in context such that TKs can exploit more focused information. We found that neural embeddings produced by current methods do not provide a suitable contextual similarity. Thus, we define a new approach based on a Siamese Network, which produces word representations while learning a binary text similarity. We set the latter considering examples in the same category as similar. The experiments on question and sentiment classification show that our semantic TK highly improves previous results.","","","","10.18653/v1/K17-1027","https://consensus.app/papers/learning-embeddings-structural-semantic-similarity-nicosia/3d1673566f8f5f699c15baef74f9aae0/"
"Long short-term memory network for learning sentences similarity using deep contextual embeddings","The proposed deep contextual long semantic textual similarity network effectively performs better than human annotation in various applications like information retrieval, plagiarism detection, and question answering platforms.","Suraj Meshram, M. Anand Kumar",2021,16,"Semantic text similarity (STS) is a challenging issue for natural language processing due to linguistic expression variability and ambiguities. The degree of the likelihood between the two sentences is calculated by sentence similarity. It plays a prominent role in many applications like information retrieval (IR), plagiarism detection (PD), question answering platform and text paraphrasing, etc. Now, deep contextualised word representations became a better way for feature extraction in sentences. It has shown exciting experimental results from recent studies. In this paper, we propose a deep contextual long semantic textual similarity network. Deep contextual mechanisms for collecting high-level semantic knowledge is used in the LSTM network. Through implementing architecture in multiple datasets, we have demonstrated our model’s effectiveness. By applying architecture to various semantic similarity datasets, we showed the usefulness of our model’s on regression and classification dataset. Detailed experimentation and results show that the proposed deep contextual model performs better than the human annotation.","","International Journal of Information Technology","2","10.1007/s41870-021-00686-y","https://consensus.app/papers/memory-network-learning-sentences-similarity-using-meshram/42b49c17eb1e55e49bd842d515285e13/"
"The Devil is in the Details: Evaluating Limitations of Transformer-based Methods for Granular Tasks","Transformer-based neural language models can perform well in abstract document matching, but struggle on granular tasks like textual similarity.","Brihi Joshi, Neil Shah, Francesco Barbieri, Leonardo Neves",2020,2,"Contextual embeddings derived from transformer-based neural language models have shown state-of-the-art performance for various tasks such as question answering, sentiment analysis, and textual similarity in recent years. Extensive work shows how accurately such models can represent abstract, semantic information present in text. In this expository work, we explore a tangent direction and analyze such models’ performance on tasks that require a more granular level of representation. We focus on the problem of textual similarity from two perspectives: matching documents on a granular level (requiring embeddings to capture fine-grained attributes in the text), and an abstract level (requiring embeddings to capture overall textual semantics). We empirically demonstrate, across two datasets from different domains, that despite high performance in abstract document matching as expected, contextual embeddings are consistently (and at times, vastly) outperformed by simple baselines like TF-IDF for more granular tasks. We then propose a simple but effective method to incorporate TF-IDF into models that use contextual embeddings, achieving relative improvements of up to 36% on granular tasks.","","","","10.18653/V1/2020.COLING-MAIN.326","https://consensus.app/papers/devil-details-evaluating-limitations-transformerbased-joshi/11cf797c72a2543ebe7c5ca5cfd38cf5/"
"SBERT-WK: A Sentence Embedding Method by Dissecting BERT-Based Word Models","SBERT-WK is a new sentence embedding method that achieves state-of-the-art performance in natural language processing tasks by dissecting BERT-based word models through geometric analysis of the space spanned by the word representation.","Bin Wang, C.-C. Jay Kuo",2020,114,"Sentence embedding is an important research topic in natural language processing (NLP) since it can transfer knowledge to downstream tasks. Meanwhile, a contextualized word representation, called BERT, achieves the state-of-the-art performance in quite a few NLP tasks. Yet, it is an open problem to generate a high quality sentence representation from BERT-based word models. It was shown in previous study that different layers of BERT capture different linguistic properties. This allows us to fuse information across layers to find better sentence representations. In this work, we study the layer-wise pattern of the word representation of deep contextualized models. Then, we propose a new sentence embedding method by dissecting BERT-based word models through geometric analysis of the space spanned by the word representation. It is called the SBERT-WK method. No further training is required in SBERT-WK. We evaluate SBERT-WK on semantic textual similarity and downstream supervised tasks. Furthermore, ten sentence-level probing tasks are presented for detailed linguistic analysis. Experiments show that SBERT-WK achieves the state-of-the-art performance. Our codes are publicly available.","","IEEE/ACM Transactions on Audio, Speech, and Language Processing","1","10.1109/taslp.2020.3008390","https://consensus.app/papers/sbertwk-sentence-embedding-method-dissecting-bertbased-wang/5f498325201c598b988a06f3b5fc875d/"
"Enhancing Clinical Concept Extraction with Contextual Embedding","Contextual embeddings, trained on a large clinical corpus, achieve new state-of-the-art performance in clinical concept extraction, capturing valuable semantic information not accounted for by traditional word representations.","Yuqi Si, Jingqi Wang, Hua Xu, Kirk Roberts",2019,240,"OBJECTIVE
Neural network-based representations (""embeddings"") have dramatically advanced natural language processing (NLP) tasks, including clinical NLP tasks such as concept extraction. Recently, however, more advanced embedding methods and representations (eg, ELMo, BERT) have further pushed the state of the art in NLP, yet there are no common best practices for how to integrate these representations into clinical tasks. The purpose of this study, then, is to explore the space of possible options in utilizing these new models for clinical concept extraction, including comparing these to traditional word embedding methods (word2vec, GloVe, fastText).


MATERIALS AND METHODS
Both off-the-shelf, open-domain embeddings and pretrained clinical embeddings from MIMIC-III (Medical Information Mart for Intensive Care III) are evaluated. We explore a battery of embedding methods consisting of traditional word embeddings and contextual embeddings and compare these on 4 concept extraction corpora: i2b2 2010, i2b2 2012, SemEval 2014, and SemEval 2015. We also analyze the impact of the pretraining time of a large language model like ELMo or BERT on the extraction performance. Last, we present an intuitive way to understand the semantic information encoded by contextual embeddings.


RESULTS
Contextual embeddings pretrained on a large clinical corpus achieves new state-of-the-art performances across all concept extraction tasks. The best-performing model outperforms all state-of-the-art methods with respective F1-measures of 90.25, 93.18 (partial), 80.74, and 81.65.


CONCLUSIONS
We demonstrate the potential of contextual embeddings through the state-of-the-art performance these methods achieve on clinical concept extraction. Additionally, we demonstrate that contextual embeddings encode valuable semantic information not accounted for in traditional word representations.","","Journal of the American Medical Informatics Association : JAMIA","1","10.1093/jamia/ocz096","https://consensus.app/papers/enhancing-concept-extraction-contextual-embedding-si/3f78f3b7322851408a2132a4b50afe7d/"
"When is a bishop not like a rook? When it’s like a rabbi! Multi-prototype BERT embeddings for estimating semantic relationships","Multi-prototype BERT embeddings retain contextual knowledge for type-level tasks, improving semantic similarity and relatedness estimation while being less cumbersome and subject to outlier effects.","Gabriella Chronis, K. Erk",2020,36,"This paper investigates contextual language models, which produce token representations, as a resource for lexical semantics at the word or type level. We construct multi-prototype word embeddings from bert-base-uncased (Devlin et al., 2018). These embeddings retain contextual knowledge that is critical for some type-level tasks, while being less cumbersome and less subject to outlier effects than exemplar models. Similarity and relatedness estimation, both type-level tasks, benefit from this contextual knowledge, indicating the context-sensitivity of these processes. BERT’s token level knowledge also allows the testing of a type-level hypothesis about lexical abstractness, demonstrating the relationship between token-level phenomena and type-level concreteness ratings. Our findings provide important insight into the interpretability of BERT: layer 7 approximates semantic similarity, while the final layer (11) approximates relatedness.","","","","10.18653/v1/2020.conll-1.17","https://consensus.app/papers/when-bishop-like-rook-when-like-rabbi-multiprototype-bert-chronis/b4d10a6a18445b48b58f0bb92534e7bf/"
"Disentangling Semantics and Syntax in Sentence Embeddings with Pre-trained Language Models","ParaBART effectively disentangles semantics and syntax in pre-trained language models, improving robustness against syntactic variation in downstream semantic tasks.","James Y. Huang, Kuan-Hao Huang, Kai-Wei Chang",2021,14,"Pre-trained language models have achieved huge success on a wide range of NLP tasks. However, contextual representations from pre-trained models contain entangled semantic and syntactic information, and therefore cannot be directly used to derive useful semantic sentence embeddings for some tasks. Paraphrase pairs offer an effective way of learning the distinction between semantics and syntax, as they naturally share semantics and often vary in syntax. In this work, we present ParaBART, a semantic sentence embedding model that learns to disentangle semantics and syntax in sentence embeddings obtained by pre-trained language models. ParaBART is trained to perform syntax-guided paraphrasing, based on a source sentence that shares semantics with the target paraphrase, and a parse tree that specifies the target syntax. In this way, ParaBART learns disentangled semantic and syntactic representations from their respective inputs with separate encoders. Experiments in English show that ParaBART outperforms state-of-the-art sentence embedding models on unsupervised semantic similarity tasks. Additionally, we show that our approach can effectively remove syntactic information from semantic sentence embeddings, leading to better robustness against syntactic variation on downstream semantic tasks.","","","","10.18653/V1/2021.NAACL-MAIN.108","https://consensus.app/papers/disentangling-semantics-syntax-sentence-embeddings-huang/594f3c0f029852e982336d60b4dafad9/"
