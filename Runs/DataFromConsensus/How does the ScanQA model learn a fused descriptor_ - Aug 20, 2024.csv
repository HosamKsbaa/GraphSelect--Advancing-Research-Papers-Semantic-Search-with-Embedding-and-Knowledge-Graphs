Title,Takeaway,Authors,Year,Citations,Abstract,Study Type,Journal,Journal SJR Quartile,DOI,Consensus Link
"ScanQA: 3D Question Answering for Spatial Scene Understanding","ScanQA model effectively performs object-grounded question answering in 3D environments, enabling accurate object localization and alignment in indoor scenes from textual questions.","Daich Azuma, Taiki Miyanishi, Shuhei Kurita, M. Kawanabe",2021,53,"We propose a new 3D spatial understanding task for 3D question answering (3D-QA). In the 3D-QA task, models receive visual information from the entire 3D scene of a rich RGB-D indoor scan and answer given textual questions about the 3D scene. Unlike the 2D-question answering of visual question answering, the conventional 2D-QA models suffer from problems with spatial understanding of object alignment and directions and fail in object localization from the textual questions in 3D-QA. We propose a baseline model for 3D-QA, called the ScanQA11https://github.com/ATR-DBI/ScanQA, which learns a fused descriptor from 3D object proposals and encoded sentence embeddings. This learned descriptor correlates language expressions with the underlying geometric features of the 3D scan and facilitates the regression of 3D bounding boxes to determine the described objects in textual questions. We collected human-edited question-answer pairs with free-form answers grounded in 3D objects in each 3D scene. Our new ScanQA dataset contains over 41k question-answer pairs from 800 indoor scenes obtained from the ScanNet dataset. To the best of our knowledge, ScanQA is the first large-scale effort to perform object-grounded question answering in 3D environments.","","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","10.1109/CVPR52688.2022.01854","https://consensus.app/papers/scanqa-question-answering-spatial-scene-understanding-azuma/555f825f7ee55af8be7efac6bd4c5b50/"
"ACPred-Fuse: fusing multi-view information improves the prediction of anticancer peptides","The ACPred-Fuse machine learning predictor accurately predicts anticancer peptides from large-scale proteins, offering a more precise and promising identification method than existing predictors.","B. Rao, Chen Zhou, Guoying Zhang, R. Su, Leyi Wei",2019,82,"Fast and accurate identification of the peptides with anticancer activity potential from large-scale proteins is currently a challenging task. In this study, we propose a new machine learning predictor, namely, ACPred-Fuse, that can automatically and accurately predict protein sequences with or without anticancer activity in peptide form. Specifically, we establish a feature representation learning model that can explore class and probabilistic information embedded in anticancer peptides (ACPs) by integrating a total of 29 different sequence-based feature descriptors. In order to make full use of various multiview information, we further fused the class and probabilistic features with handcrafted sequential features and then optimized the representation ability of the multiview features, which are ultimately used as input for training our prediction model. By comparing the multiview features and existing feature descriptors, we demonstrate that the fused multiview features have more discriminative ability to capture the characteristics of ACPs. In addition, the information from different views is complementary for the performance improvement. Finally, our benchmarking comparison results showed that the proposed ACPred-Fuse is more precise and promising in the identification of ACPs than existing predictors. To facilitate the use of the proposed predictor, we built a web server, which is now freely available via http://server.malab.cn/ACPred-Fuse.","","Briefings in bioinformatics","1","10.1093/bib/bbz088","https://consensus.app/papers/acpredfuse-fusing-multiview-information-improves-rao/53940ff7777c5910ac555d49251c9289/"
"Learning to Fuse Local Geometric Features for 3D Rigid Data Matching","Our neural network model optimizes local geometric features for 3D rigid data matching, resulting in compact, distinctive representations that are more lightweight and rotational invariant than deep learned descriptors.","Jiaqi Yang, Chen Zhao, Ke Xian, Angfan Zhu, ZHIGUO CAO",2019,12,"This paper presents a simple yet very effective data-driven approach to fuse both low-level and high-level local geometric features for 3D rigid data matching. It is a common practice to generate distinctive geometric descriptors by fusing low-level features from various viewpoints or subspaces, or enhance geometric feature matching by leveraging multiple high-level features. In prior works, they are typically performed via linear operations such as concatenation and min pooling. We show that more compact and distinctive representations can be achieved by optimizing a neural network (NN) model under the triplet framework that non-linearly fuses local geometric features in Euclidean spaces. The NN model is trained by an improved triplet loss function that fully leverages all pairwise relationships within the triplet. Moreover, the fused descriptor by our approach is also competitive to deep learned descriptors from raw data while being more lightweight and rotational invariant. Experimental results on four standard datasets with various data modalities and application contexts confirm the advantages of our approach in terms of both feature matching and geometric registration.","","Inf. Fusion","1","10.1016/j.inffus.2020.03.008","https://consensus.app/papers/learning-fuse-local-geometric-features-rigid-data-yang/b49ca554d3f158afbc1b032c0cb9b453/"
"Person re-identification by order-induced metric fusion","This paper proposes a novel two-pronged framework for person re-identification, combining different descriptors and a confidence measure mechanism to significantly improve results compared to individual descriptors.","Behzad Mirmahboub, M. L. Mekhalfi, Vittorio Murino",2018,10,"Abstract This paper presents a novel two-pronged framework for person re-identification. Its idea articulates over the fact that distinct descriptors manifest different ranking scores for the same probe pattern. Thus, if conveniently fused, the descriptors in hand are ought to compensate each other, leading to significant improvements. In this respect, this paper proposes a learning-free weighting method that penalizes and averages the re-identification estimates (e.g., distances) pointed out by different descriptors according to their confidence in evidencing the correct match, to a given probe person, among a given gallery. We particularly show that tangible improvements can be attained with respect to utilizing each descriptor individually. Moreover, we consider a confidence measure mechanism that treats the mutual pairwise distances within the gallery, in order to raise the scores obtained at the fusion stage, and we show that interesting improvements can be achieved. We evaluate the proposed framework on four benchmark datasets and advance late works by large margins.","","Neurocomputing","1","10.1016/j.neucom.2017.09.019","https://consensus.app/papers/person-reidentification-orderinduced-fusion-mirmahboub/a42409f7c6d1595cb00b0c9dfe1daaeb/"
"Fusing Local and Global Features for High-Resolution Scene Classification","The proposed fused global saliency-based multiscale multiresolution multistructure local binary pattern (salM3LBP) and local codebookless model (CLM) feature improves high-resolution image scene classification performance compared to state-of-the-art methods.","Xiaoyong Bian, Chen Chen, Long Tian, Q. Du",2017,149,"In this paper, a fused global saliency-based multiscale multiresolution multistructure local binary pattern (salM 3LBP) feature and local codebookless model (CLM) feature is proposed for high-resolution image scene classification. First, two different but complementary types of descriptors (pixel intensities and differences) are developed to extract global features, characterizing the dominant spatial features in multiple scale, multiple resolution, and multiple structure manner. The micro/macrostructure information and rotation invariance are guaranteed in the global feature extraction process. For dense local feature extraction, CLM is utilized to model local enrichment scale invariant feature transform descriptor and dimension reduction is conducted via joint low-rank learning with support vector machine. Finally, a fused feature representation between salM3LBP and CLM as the scene descriptor to train a kernel-based extreme learning machine for scene classification is presented. The proposed approach is extensively evaluated on three challenging benchmark scene datasets (the 21-class land-use scene, 19-class satellite scene, and a newly available 30-class aerial scene), and the experimental results show that the proposed approach leads to superior classification performance compared with the state-of-the-art classification methods.","","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","1","10.1109/JSTARS.2017.2683799","https://consensus.app/papers/fusing-local-global-features-highresolution-scene-bian/ee497492e1d55b308dbba9aae1d9b95f/"
"Unsupervised Learning Based on Multiple Descriptors for WSIs Diagnosis","Our unsupervised deep learning model using multiple descriptors improves whole-slide image diagnosis accuracy by overcoming limitations in cellular heterogeneity and cellular structure.","T. S. Sheikh, Jee-Yeon Kim, Jae-Ju Shim, Migyung Cho",2022,1,"An automatic pathological diagnosis is a challenging task because histopathological images with different cellular heterogeneity representations are sometimes limited. To overcome this, we investigated how the holistic and local appearance features with limited information can be fused to enhance the analysis performance. We propose an unsupervised deep learning model for whole-slide image diagnosis, which uses stacked autoencoders simultaneously feeding multiple-image descriptors such as the histogram of oriented gradients and local binary patterns along with the original image to fuse the heterogeneous features. The pre-trained latent vectors are extracted from each autoencoder, and these fused feature representations are utilized for classification. We observed that training with additional descriptors helps the model to overcome the limitations of multiple variants and the intricate cellular structure of histopathology data by various experiments. Our model outperforms existing state-of-the-art approaches by achieving the highest accuracies of 87.2 for ICIAR2018, 94.6 for Dartmouth, and other significant metrics for public benchmark datasets. Our model does not rely on a specific set of pre-trained features based on classifiers to achieve high performance. Unsupervised spaces are learned from the number of independent multiple descriptors and can be used with different variants of classifiers to classify cancer diseases from whole-slide images. Furthermore, we found that the proposed model classifies the types of breast and lung cancer similar to the viewpoint of pathologists by visualization. We also designed our whole-slide image processing toolbox to extract and process the patches from whole-slide images.","","Diagnostics","2","10.3390/diagnostics12061480","https://consensus.app/papers/unsupervised-learning-multiple-descriptors-wsis-sheikh/5cf6113397bb59c785cc0ce40e4381af/"
"ScanRefer: 3D Object Localization in RGB-D Scans using Natural Language","ScanRefer effectively localizes 3D objects in RGB-D scans using natural language descriptions, enabling the first large-scale effort to perform object localization via natural language expressions in 3D.","Dave Zhenyu Chen, Angel X. Chang, M. Nießner",2019,159,"We introduce the task of 3D object localization in RGB-D scans using natural language descriptions. As input, we assume a point cloud of a scanned 3D scene along with a free-form description of a specified target object. To address this task, we propose ScanRefer, learning a fused descriptor from 3D object proposals and encoded sentence embeddings. This fused descriptor correlates language expressions with geometric features, enabling regression of the 3D bounding box of a target object. We also introduce the ScanRefer dataset, containing 51,583 descriptions of 11,046 objects from 800 ScanNet scenes. ScanRefer is the first large-scale effort to perform object localization via natural language expression directly in 3D.","","ArXiv","","10.1007/978-3-030-58565-5_13","https://consensus.app/papers/scanrefer-object-localization-rgbd-scans-using-natural-chen/16c4b57ba7d85cd58388fbfef57c12c0/"
"PREDICTION OF DIABETES EMPOWERED WITH FUSED MACHINE LEARNING","The proposed fused machine learning model accurately predicts diabetes diagnosis with a 94.87 prediction accuracy, improving early disease detection.","U. Ahmed, Ghassan F. Issa, Muhammad Adnan Khan, Shabib Aftab, Muhammad Farhan Khan, Raed A. Said, Taher M. Ghazal, Munir Ahmad",2022,76,"In the medical field, it is essential to predict diseases early to prevent them. Diabetes is one of the most dangerous diseases all over the world. In modern lifestyles, sugar and fat are typically present in our dietary habits, which have increased the risk of diabetes. To predict the disease, it is extremely important to understand its symptoms. Currently, machine-learning (ML) algorithms are valuable for disease detection. This article presents a model using a fused machine learning approach for diabetes prediction. The conceptual framework consists of two types of models: Support Vector Machine (SVM) and Artificial Neural Network (ANN) models. These models analyze the dataset to determine whether a diabetes diagnosis is positive or negative. The dataset used in this research is divided into training data and testing data with a ratio of 70:30 respectively. The output of these models becomes the input membership function for the fuzzy model, whereas the fuzzy logic finally determines whether a diabetes diagnosis is positive or negative. A cloud storage system stores the fused models for future use. Based on the patient’s real-time medical record, the fused model predicts whether the patient is diabetic or not. The proposed fused ML model has a prediction accuracy of 94.87, which is higher than the previously published methods.","","IEEE Access","1","10.1109/access.2022.3142097","https://consensus.app/papers/prediction-diabetes-empowered-with-fused-machine-ahmed/39cc752cea115c4a842d412ef1cfc227/"
"Selective multi-descriptor fusion for face identification","Selective multi-descriptor fusion (SMDF) improves face identification performance while reducing memory and computational costs, making it more accessible for everyday PCs.","Xin Wei, Hui Wang, B. Scotney, Huan Wan",2019,4,"Over the last 2 decades, face identification has been an active field of research in computer vision. As an important class of image representation methods for face identification, fused descriptor-based methods are known to lack sufficient discriminant information, especially when compared with deep learning-based methods. This paper presents a new face representation method, multi-descriptor fusion (MDF), which represents face images through a combination of multiple descriptors, resulting in hyper-high dimensional fused descriptor features. MDF enables excellent performance in face identification, exceeding the state-of-the-art, but it comes with high memory and computational costs. As a solution to the high cost problem, this paper also presents an optimisation method, discriminant ability-based multi-descriptor selection (DAMS), to select a subset of descriptors from the set of 65 initial descriptors whilst maximising the discriminant ability. The MDF face representation, after being refined by DAMS, is named selective multi-descriptor fusion (SMDF). Compared with MDF, SMDF has much smaller feature dimension and is thus usable on an ordinary PC, but still has similar performance. Various experiments are conducted on the CAS-PEAL-R1 and LFW datasets to demonstrate the performance of the proposed methods.","","International Journal of Machine Learning and Cybernetics","2","10.1007/S13042-019-00929-2","https://consensus.app/papers/multidescriptor-fusion-face-identification-wei/8aabfcb393ea574db8fc8bd03ac92cdc/"
"A novel fusion approach in the extraction of kernel descriptor with improved effectiveness and efficiency","Our novel fusion approach fuses different image features before descriptor extraction, resulting in a compact, efficient, and effective kernel descriptor for image representation.","Priyabrata Karmakar, S. Teng, Guojun Lu, Dengsheng Zhang",2021,1,"Image representation using feature descriptors is crucial. A number of histogram-based descriptors are widely used for this purpose. However, histogram-based descriptors have certain limitations and kernel descriptors (KDES) are proven to overcome them. Moreover, the combination of more than one KDES performs better than an individual KDES. Conventionally, KDES fusion is performed by concatenating them after the gradient, colour and shape descriptors have been extracted. This approach has limitations in regard to the efficiency as well as the effectiveness. In this paper, we propose a novel approach to fuse different image features before the descriptor extraction, resulting in a compact descriptor which is efficient and effective. In addition, we have investigated the effect on the proposed descriptor when texture-based features are fused along with the conventionally used features. Our proposed descriptor is examined on two publicly available image databases and shown to provide outstanding performances.","","Multimedia Tools and Applications","1","10.1007/s11042-020-10300-1","https://consensus.app/papers/fusion-approach-extraction-kernel-descriptor-improved-karmakar/c2a4d60e9337544a947aa4ba969d85a3/"
