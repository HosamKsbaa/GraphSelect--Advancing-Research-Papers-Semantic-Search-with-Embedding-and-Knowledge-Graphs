Title,Takeaway,Authors,Year,Citations,Abstract,Study Type,Journal,Journal SJR Quartile,DOI,Consensus Link
"Multimodal deep learning models for early detection of Alzheimer’s disease stage","Deep learning models combining imaging, genetic, and clinical data improve early detection of Alzheimer's disease stages.","Janani Venugopalan, L. Tong, H. Hassanzadeh, May D. Wang",2021,231,"Most current Alzheimer’s disease (AD) and mild cognitive disorders (MCI) studies use single data modality to make predictions such as AD stages. The fusion of multiple data modalities can provide a holistic view of AD staging analysis. Thus, we use deep learning (DL) to integrally analyze imaging (magnetic resonance imaging (MRI)), genetic (single nucleotide polymorphisms (SNPs)), and clinical test data to classify patients into AD, MCI, and controls (CN). We use stacked denoising auto-encoders to extract features from clinical and genetic data, and use 3D-convolutional neural networks (CNNs) for imaging data. We also develop a novel data interpretation method to identify top-performing features learned by the deep-models with clustering and perturbation analysis. Using Alzheimer’s disease neuroimaging initiative (ADNI) dataset, we demonstrate that deep models outperform shallow models, including support vector machines, decision trees, random forests, and k-nearest neighbors. In addition, we demonstrate that integrating multi-modality data outperforms single modality models in terms of accuracy, precision, recall, and meanF1 scores. Our models have identified hippocampus, amygdala brain areas, and the Rey Auditory Verbal Learning Test (RAVLT) as top distinguished features, which are consistent with the known AD literature.","","Scientific Reports","1","10.1038/s41598-020-74399-w","https://consensus.app/papers/multimodal-learning-models-detection-alzheimer-disease-venugopalan/0f1cc4babfda53d1a5f622c960d0071e/"
"Deep Learning in Alzheimer's Disease: Diagnostic Classification and Prognostic Prediction Using Neuroimaging Data","Deep learning approaches show promise for diagnostic classification of Alzheimer's disease using multimodal neuroimaging data, with the best performance achieved when combining multimodal neuroimaging and fluid biomarkers.","T. Jo, K. Nho, A. Saykin",2019,352,"Deep learning, a state-of-the-art machine learning approach, has shown outstanding performance over traditional machine learning in identifying intricate structures in complex high-dimensional data, especially in the domain of computer vision. The application of deep learning to early detection and automated classification of Alzheimer's disease (AD) has recently gained considerable attention, as rapid progress in neuroimaging techniques has generated large-scale multimodal neuroimaging data. A systematic review of publications using deep learning approaches and neuroimaging data for diagnostic classification of AD was performed. A PubMed and Google Scholar search was used to identify deep learning papers on AD published between January 2013 and July 2018. These papers were reviewed, evaluated, and classified by algorithm and neuroimaging type, and the findings were summarized. Of 16 studies meeting full inclusion criteria, 4 used a combination of deep learning and traditional machine learning approaches, and 12 used only deep learning approaches. The combination of traditional machine learning for classification and stacked auto-encoder (SAE) for feature selection produced accuracies of up to 98.8% for AD classification and 83.7% for prediction of conversion from mild cognitive impairment (MCI), a prodromal stage of AD, to AD. Deep learning approaches, such as convolutional neural network (CNN) or recurrent neural network (RNN), that use neuroimaging data without pre-processing for feature selection have yielded accuracies of up to 96.0% for AD classification and 84.2% for MCI conversion prediction. The best classification performance was obtained when multimodal neuroimaging and fluid biomarkers were combined. Deep learning approaches continue to improve in performance and appear to hold promise for diagnostic classification of AD using multimodal neuroimaging data. AD research that uses deep learning is still evolving, improving performance by incorporating additional hybrid data types, such as—omics data, increasing transparency with explainable approaches that add knowledge of specific disease-related features and mechanisms.","systematic review","Frontiers in Aging Neuroscience","1","10.3389/fnagi.2019.00220","https://consensus.app/papers/deep-learning-alzheimers-disease-diagnostic-jo/c09b595ff18b50feadfdb073214e1779/"
"Explainable AI-based Alzheimer’s prediction and management using multimodal data","This paper proposes a novel explainable Alzheimer's disease prediction model using multimodal data, with a 10-fold cross-validation accuracy of 98.81%, and a novel patient management architecture.","Sobhana Jahan, Kazi Abu Taher, M. Kaiser, M. Mahmud, Md. Sazzadur Rahman, A. Hosen, In-Ho Ra",2023,1,"Background According to the World Health Organization (WHO), dementia is the seventh leading reason of death among all illnesses and one of the leading causes of disability among the world’s elderly people. Day by day the number of Alzheimer’s patients is rising. Considering the increasing rate and the dangers, Alzheimer’s disease should be diagnosed carefully. Machine learning is a potential technique for Alzheimer’s diagnosis but general users do not trust machine learning models due to the black-box nature. Even, some of those models do not provide the best performance because of using only neuroimaging data. Objective To solve these issues, this paper proposes a novel explainable Alzheimer’s disease prediction model using a multimodal dataset. This approach performs a data-level fusion using clinical data, MRI segmentation data, and psychological data. However, currently, there is very little understanding of multimodal five-class classification of Alzheimer’s disease. Method For predicting five class classifications, 9 most popular Machine Learning models are used. These models are Random Forest (RF), Logistic Regression (LR), Decision Tree (DT), Multi-Layer Perceptron (MLP), K-Nearest Neighbor (KNN), Gradient Boosting (GB), Adaptive Boosting (AdaB), Support Vector Machine (SVM), and Naive Bayes (NB). Among these models RF has scored the highest value. Besides for explainability, SHapley Additive exPlanation (SHAP) is used in this research work. Results and conclusions The performance evaluation demonstrates that the RF classifier has a 10-fold cross-validation accuracy of 98.81% for predicting Alzheimer’s disease, cognitively normal, non-Alzheimer’s dementia, uncertain dementia, and others. In addition, the study utilized Explainable Artificial Intelligence based on the SHAP model and analyzed the causes of prediction. To the best of our knowledge, we are the first to present this multimodal (Clinical, Psychological, and MRI segmentation data) five-class classification of Alzheimer’s disease using Open Access Series of Imaging Studies (OASIS-3) dataset. Besides, a novel Alzheimer’s patient management architecture is also proposed in this work.","","PLOS ONE","1","10.1371/journal.pone.0294253","https://consensus.app/papers/alzheimer-prediction-management-using-multimodal-data-jahan/c792a74ead1b53c79de38a37bfd11ffc/"
"Effective feature learning and fusion of multimodality data using stage‐wise deep neural network for dementia diagnosis","The proposed three-stage deep feature learning and fusion framework effectively utilizes multimodality neuroimaging and genetic data for Alzheimer's disease diagnosis, outperforming other state-of-the-art methods.","Tao Zhou, Kim-Han Thung, Xiaofeng Zhu, D. Shen",2018,169,"In this article, the authors aim to maximally utilize multimodality neuroimaging and genetic data for identifying Alzheimer's disease (AD) and its prodromal status, Mild Cognitive Impairment (MCI), from normal aging subjects. Multimodality neuroimaging data such as MRI and PET provide valuable insights into brain abnormalities, while genetic data such as single nucleotide polymorphism (SNP) provide information about a patient's AD risk factors. When these data are used together, the accuracy of AD diagnosis may be improved. However, these data are heterogeneous (e.g., with different data distributions), and have different number of samples (e.g., with far less number of PET samples than the number of MRI or SNPs). Thus, learning an effective model using these data is challenging. To this end, we present a novel three‐stage deep feature learning and fusion framework, where deep neural network is trained stage‐wise. Each stage of the network learns feature representations for different combinations of modalities, via effective training using the maximum number of available samples. Specifically, in the first stage, we learn latent representations (i.e., high‐level features) for each modality independently, so that the heterogeneity among modalities can be partially addressed, and high‐level features from different modalities can be combined in the next stage. In the second stage, we learn joint latent features for each pair of modality combination by using the high‐level features learned from the first stage. In the third stage, we learn the diagnostic labels by fusing the learned joint latent features from the second stage. To further increase the number of samples during training, we also use data at multiple scanning time points for each training subject in the dataset. We evaluate the proposed framework using Alzheimer's disease neuroimaging initiative (ADNI) dataset for AD diagnosis, and the experimental results show that the proposed framework outperforms other state‐of‐the‐art methods.","","Human Brain Mapping","1","10.1002/hbm.24428","https://consensus.app/papers/feature-learning-fusion-multimodality-data-using-zhou/b765b6842532584ab220d1d7d06af97e/"
"Alzheimer's disease diagnosis framework from incomplete multimodal data using convolutional neural networks","Our novel multimodal neuroimaging and genetic data fusion method using convolutional neural networks improves Alzheimer's disease diagnosis accuracy and clinical score regression.","Mohammed Abdelaziz, A. Elazab, Tianfu Wang",2021,23,"Alzheimer's disease (AD) is a severe irreversible neurodegenerative disease that has great sufferings on patients and eventually leads to death. Early detection of AD and its prodromal stage, mild cognitive impairment (MCI) which can be either stable (sMCI) or progressive (pMCI), is highly desirable for effective treatment planning and tailoring therapy. Recent studies recommended using multimodal data fusion of genetic (single nucleotide polymorphisms, SNPs) and neuroimaging data (magnetic resonance imaging (MRI) and positron emission tomography (PET)) to discriminate AD/MCI from normal control (NC) subjects. However, missing multimodal data in the cohort under study is inevitable. In addition, data heterogeneity between phenotypes and genotypes biomarkers makes learning capability of the models more challenging. Also, the current studies mainly focus on identifying brain disease classification and ignoring the regression task. Furthermore, they utilize multistage for predicting the brain disease progression. To address these issues, we propose a novel multimodal neuroimaging and genetic data fusion for joint classification and clinical score regression tasks using the maximum number of available samples in one unified framework using convolutional neural network (CNN). Specifically, we initially perform a technique based on linear interpolation to fill the missing features for each incomplete sample. Then, we learn the neuroimaging features from MRI, PET, and SNPs using CNN to alleviate the heterogeneity among genotype and phenotype data. Meanwhile, the high learned features from each modality are combined for jointly identifying brain diseases and predicting clinical scores. To validate the performance of the proposed method, we test our method on 805 subjects from Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. Also, we verify the similarity between the synthetic and real data using statistical analysis. Moreover, the experimental results demonstrate that the proposed method can yield better performance in both classification and regression tasks. Specifically, our proposed method achieves accuracy of 98.22%, 93.11%, and 97.35% for NC vs. AD, NC vs. sMCI, and NC vs. pMCI, respectively. On the other hand, our method attains the lowest root mean square error and the highest correlation coefficient for different clinical scores regression tasks compared with the state-of-the-art methods.","","Journal of biomedical informatics","1","10.1016/j.jbi.2021.103863","https://consensus.app/papers/alzheimers-disease-diagnosis-framework-multimodal-data-abdelaziz/4ec432aa10155518afc27132887de436/"
"Multimodal multitask deep learning model for Alzheimer's disease progression detection based on time series data","Our multimodal multitask deep learning model effectively detects Alzheimer's disease progression using time series data and background knowledge, achieving state-of-the-art performance.","Shaker El-Sappagh, Tamer Abuhmed, S. Islam, K. Kwak",2020,105,"Abstract Early prediction of Alzheimer’s disease (AD) is crucial for delaying its progression. As a chronic disease, ignoring the temporal dimension of AD data affects the performance of a progression detection and medically unacceptable. Besides, AD patients are represented by heterogeneous, yet complementary, multimodalities. Multitask modeling improves progression-detection performance, robustness, and stability. However, multimodal multitask modeling has not been evaluated using time series and deep learning paradigm, especially for AD progression detection. In this paper, we propose a robust ensemble deep learning model based on a stacked convolutional neural network (CNN) and a bidirectional long short-term memory (BiLSTM) network. This multimodal multitask model jointly predicts multiple variables based on the fusion of five types of multimodal time series data plus a set of background (BG) knowledge. Predicted variables include AD multiclass progression task, and four critical cognitive scores regression tasks. The proposed model extracts local and longitudinal features of each modality using a stacked CNN and BiLSTM network. Concurrently, local features are extracted from the BG data using a feed-forward neural network. Resultant features are fused to a deep network to detect common patterns which jointly used to predict the classification and regression tasks. To validate our model, we performed six experiments on five modalities from Alzheimer’s Disease Neuroimaging Initiative (ADNI) of 1536 subjects. The results of the proposed approach achieve state-of-the-art performance for both multiclass progression and regression tasks. Moreover, our approach can be generalized in other medial domains to analyze heterogeneous temporal data for predicting patient’s future status.","","Neurocomputing","1","10.1016/j.neucom.2020.05.087","https://consensus.app/papers/multimodal-multitask-learning-model-alzheimers-disease-elsappagh/21de4911f2de57f4af3928a125452230/"
"Multimodal Neuroimaging Feature Learning for Multiclass Diagnosis of Alzheimer's Disease","Our novel diagnostic framework improves Alzheimer's disease diagnosis by fusing multimodal neuroimaging features and requiring less labeled data, offering performance gains in both binary and multiclass classification.","Siqi Liu, Sidong Liu, Weidong (Tom) Cai, Hangyu Che, Sonia Pujol, R. Kikinis, D. Feng, M. Fulham",2015,409,"The accurate diagnosis of Alzheimer's disease (AD) is essential for patient care and will be increasingly important as disease modifying agents become available, early in the course of the disease. Although studies have applied machine learning methods for the computer-aided diagnosis of AD, a bottleneck in the diagnostic performance was shown in previous methods, due to the lacking of efficient strategies for representing neuroimaging biomarkers. In this study, we designed a novel diagnostic framework with deep learning architecture to aid the diagnosis of AD. This framework uses a zero-masking strategy for data fusion to extract complementary information from multiple data modalities. Compared to the previous state-of-the-art workflows, our method is capable of fusing multimodal neuroimaging features in one setting and has the potential to require less labeled data. A performance gain was achieved in both binary classification and multiclass classification of AD. The advantages and limitations of the proposed framework are discussed.","","IEEE Transactions on Biomedical Engineering","1","10.1109/TBME.2014.2372011","https://consensus.app/papers/multimodal-neuroimaging-feature-learning-multiclass-liu/1d0845a3ffd650a8b1a7f08801fe0acd/"
"Multimodal Self-Paced Locality-Preserving Learning for Diagnosis of Alzheimer’s Disease","The MSLPL framework effectively preserves structural relationships and improves Alzheimer's disease diagnosis by combining neuroimaging and genetic data.","Xiaoke Hao, Ruxue Wang, Yingchun Guo, Yunjia Xiao, Ming Yu, Meiling Wang, Weibin Chen, Daoqiang Zhang",2023,1,"Alzheimer’s disease (AD) is an irreversible neurodegenerative disease that severely impairs human thinking and memory. The accurate diagnosis of AD and its prodromal stages, such as mild cognitive impairment (MCI), is very important for timely treatment or possible interventions of AD. Recent studies have shown that multiple neuroimaging and biological measures contain supplementary information for diagnosis and prognosis. Most existing methods are proposed to simply integrate the multimodal data and train the model using all samples once, which do not fully explore the structural information across the different modalities and ignore the significance of sample learning in the training process. In this article, we propose a multimodal self-paced locality-preserving learning (MSLPL) framework to preserve the inherent structural relationships of the original data and realize the sample selection process from “simple” to “complex.” Specifically, the model can project the neuroimaging and genetic data into the label space and learn dimensionality reduction manners with preserving locality structure. Meanwhile, the contributions of each sample are adaptively evaluated by weighting optimization so that the impact of noises can be reduced during model training by self-paced learning (SPL). Finally, a multikernel support vector machine (MK-SVM) is used to fuse the features selected from different modalities for the final prediction. We evaluate MSLPL on 913 subjects from the AD neuroimaging initiative (ADNI) database with imaging and genetic data. The experimental results demonstrate that the proposed method can achieve better classification performances compared with the start-of-the-art multimodality-based methods.","","IEEE Transactions on Cognitive and Developmental Systems","1","10.1109/TCDS.2022.3189701","https://consensus.app/papers/multimodal-selfpaced-localitypreserving-learning-hao/6299577fc23e543e8a358956bdf3de4d/"
"A whole-process interpretable and multi-modal deep reinforcement learning for diagnosis and analysis of Alzheimer’s disease","This study developed an interpretable multimodal deep reinforcement learning model that accurately diagnoses Alzheimer's disease and analyzes potential biomarkers, improving clinical feasibility and interpretability.","Quan Zhang, Q. Du, Guohua Liu",2021,18,"Objective. Alzheimer’s disease (AD), a common disease of the elderly with unknown etiology, has been adversely affecting many people, especially with the aging of the population and the younger trend of this disease. Current artificial intelligence (AI) methods based on individual information or magnetic resonance imaging (MRI) can solve the problem of diagnostic sensitivity and specificity, but still face the challenges of interpretability and clinical feasibility. In this study, we propose an interpretable multimodal deep reinforcement learning model for inferring pathological features and the diagnosis of AD. Approach. First, for better clinical feasibility, the compressed-sensing MRI image is reconstructed using an interpretable deep reinforcement learning model. Then, the reconstructed MRI is input into the full convolution neural network to generate a pixel-level disease probability risk map (DPM) of the whole brain for AD. The DPM of important brain regions and individual information are then input into the attention-based fully deep neural network to obtain the diagnosis results and analyze the biomarkers. We used 1349 multi-center samples to construct and test the model. Main results. Finally, the model obtained 99.6% ± 0.2%, 97.9% ± 0.2%, and 96.1% ± 0.3% area under curve in ADNI, AIBL and NACC, respectively. The model also provides an effective analysis of multimodal pathology, predicts the imaging biomarkers in MRI and the weight of each individual item of information. In this study, a deep reinforcement learning model was designed, which can not only accurately diagnose AD, but analyze potential biomarkers. Significance. In this study, a deep reinforcement learning model was designed. The model builds a bridge between clinical practice and AI diagnosis and provides a viewpoint for the interpretability of AI technology.","","Journal of Neural Engineering","1","10.1088/1741-2552/ac37cc","https://consensus.app/papers/wholeprocess-reinforcement-learning-diagnosis-analysis-zhang/b711e57bb308590ab327d390d33b9e1c/"
"Multimodal Attention-based Deep Learning for Alzheimer's Disease Diagnosis","MADDi, a multimodal deep learning framework, accurately detects Alzheimer's disease and mild cognitive impairment from imaging, genetic, and clinical data, with 96.88% accuracy.","Michal Golovanevsky, C. Eickhoff, Ritambhara Singh",2022,14,"OBJECTIVE
Alzheimer's disease (AD) is the most common neurodegenerative disorder with one of the most complex pathogeneses, making effective and clinically actionable decision support difficult. The objective of this study was to develop a novel multimodal deep learning framework to aid medical professionals in AD diagnosis.


MATERIALS AND METHODS
We present a Multimodal Alzheimer's Disease Diagnosis framework (MADDi) to accurately detect the presence of AD and mild cognitive impairment (MCI) from imaging, genetic, and clinical data. MADDi is novel in that we use cross-modal attention, which captures interactions between modalities-a method not previously explored in this domain. We perform multi-class classification, a challenging task considering the strong similarities between MCI and AD. We compare with previous state-of-the-art models, evaluate the importance of attention, and examine the contribution of each modality to the model's performance.


RESULTS
MADDi classifies MCI, AD, and controls with 96.88% accuracy on a held-out test set. When examining the contribution of different attention schemes, we found that the combination of cross-modal attention with self-attention performed the best, and no attention layers in the model performed the worst, with a 7.9% difference in F1-scores.


DISCUSSION
Our experiments underlined the importance of structured clinical data to help machine learning models contextualize and interpret the remaining modalities. Extensive ablation studies showed that any multimodal mixture of input features without access to structured clinical information suffered marked performance losses.


CONCLUSION
This study demonstrates the merit of combining multiple input modalities via cross-modal attention to deliver highly accurate AD diagnostic decision support.","","Journal of the American Medical Informatics Association : JAMIA","1","10.1093/jamia/ocac168","https://consensus.app/papers/multimodal-attentionbased-deep-learning-alzheimers-golovanevsky/76681d62c5565fe180e9f3a515206f6e/"
