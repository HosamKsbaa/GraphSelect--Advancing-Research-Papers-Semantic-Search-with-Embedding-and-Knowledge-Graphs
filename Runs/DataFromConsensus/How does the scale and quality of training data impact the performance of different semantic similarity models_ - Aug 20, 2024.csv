Title,Takeaway,Authors,Year,Citations,Abstract,Study Type,Journal,Journal SJR Quartile,DOI,Consensus Link
"Large Scale Online Learning of Image Similarity Through Ranking","OASIS is a fast and accurate online algorithm for learning image similarity, achieving better results than existing methods while being orders of magnitude faster for large data sets.","Gal Chechik, Varun Sharma, Uri Shalit, Samy Bengio",2009,776,"Learning a measure of similarity between pairs of objects is an important generic problem in machine learning. It is particularly useful in large scale applications like searching for an image that is similar to a given image or finding videos that are relevant to a given video. In these tasks, users look for objects that are not only visually similar but also semantically related to a given object. Unfortunately, the approaches that exist today for learning such semantic similarity do not scale to large data sets. This is both because typically their CPU and storage requirements grow quadratically with the sample size, and because many methods impose complex positivity constraints on the space of learned similarity functions. 
 
The current paper presents OASIS, an Online Algorithm for Scalable Image Similarity learning that learns a bilinear similarity measure over sparse representations. OASIS is an online dual approach using the passive-aggressive family of learning algorithms with a large margin criterion and an efficient hinge loss cost. Our experiments show that OASIS is both fast and accurate at a wide range of scales: for a data set with thousands of images, it achieves better results than existing state-of-the-art methods, while being an order of magnitude faster. For large, web scale, data sets, OASIS can be trained on more than two million images from 150K text queries within 3 days on a single CPU. On this large scale data set, human evaluations showed that 35% of the ten nearest neighbors of a given test image, as found by OASIS, were semantically relevant to that image. This suggests that query independent similarity could be accurately learned even for large scale data sets that could not be handled before.","","","1","10.5555/1756006.1756042","https://consensus.app/papers/scale-online-learning-image-similarity-through-ranking-chechik/b80a4ae2229c5837bd97f9c3ecfdf2b3/"
"SemDeDup: Data-efficient learning at web-scale through semantic deduplication","SemDeDup reduces 50% of data in uncurated datasets like LAION with minimal performance loss, halving training time and improving performance in language models trained on C4.","Amro Abbas, Kushal Tirumala, Daniel Simig, S. Ganguli, Ari S. Morcos",2023,36,"Progress in machine learning has been driven in large part by massive increases in data. However, large web-scale datasets such as LAION are largely uncurated beyond searches for exact duplicates, potentially leaving much redundancy. Here, we introduce SemDeDup, a method which leverages embeddings from pre-trained models to identify and remove semantic duplicates: data pairs which are semantically similar, but not exactly identical. Removing semantic duplicates preserves performance and speeds up learning. Analyzing a subset of LAION, we show that SemDeDup can remove 50% of the data with minimal performance loss, effectively halving training time. Moreover, performance increases out of distribution. Also, analyzing language models trained on C4, a partially curated dataset, we show that SemDeDup improves over prior approaches while providing efficiency gains. SemDeDup provides an example of how simple ways of leveraging quality embeddings can be used to make models learn faster with less data.","","ArXiv","","10.48550/arXiv.2303.09540","https://consensus.app/papers/semdedup-dataefficient-learning-deduplication-abbas/6144c46385f15eb0bcd08531dd386062/"
"Contextualizing the Limits of Model & Evaluation Dataset Curation on Semantic Similarity Classification Tasks","The limitations of pre-trained models and open evaluation datasets impact the performance of semantic similarity classification tasks, highlighting the importance of understanding data collection, curation, and analysis.","Daniel Theron",2023,0,"This paper demonstrates how the limitations of pre-trained models and open evaluation datasets factor into assessing the performance of binary semantic similarity classification tasks. As (1) end-user-facing documentation around the curation of these datasets and pre-trained model training regimes is often not easily accessible and (2) given the lower friction and higher demand to quickly deploy such systems in real-world contexts, our study reinforces prior work showing performance disparities across datasets, embedding techniques and distance metrics, while highlighting the importance of understanding how data is collected, curated and analyzed in semantic similarity classification.","","ArXiv","","10.48550/arXiv.2311.04927","https://consensus.app/papers/contextualizing-limits-model-evaluation-dataset-theron/fddab218b8ef5847816ceff56957d4e0/"
"Sentence Similarity Based on Contexts","The proposed framework significantly improves sentence similarity measurement by defining meaning by contexts, achieving significant performance boosts over existing baselines in both supervised and unsupervised settings.","Xiaofei Sun, Yuxian Meng, Xiang Ao, Fei Wu, Tianwei Zhang, Jiwei Li, Chun Fan",2021,15,"Existing methods to measure sentence similarity are faced with two challenges: (1) labeled datasets are usually limited in size, making them insufficient to train supervised neural models; and (2) there is a training-test gap for unsupervised language modeling (LM) based models to compute semantic scores between sentences, since sentence-level semantics are not explicitly modeled at training. This results in inferior performances in this task. In this work, we propose a new framework to address these two issues. The proposed framework is based on the core idea that the meaning of a sentence should be defined by its contexts, and that sentence similarity can be measured by comparing the probabilities of generating two sentences given the same context. The proposed framework is able to generate high-quality, large-scale dataset with semantic similarity scores between two sentences in an unsupervised manner, with which the train-test gap can be largely bridged. Extensive experiments show that the proposed framework achieves significant performance boosts over existing baselines under both the supervised and unsupervised settings across different datasets.","","Transactions of the Association for Computational Linguistics","1","10.1162/tacl_a_00477","https://consensus.app/papers/sentence-similarity-based-contexts-sun/09c45faf4f2758f1be7bdbf428669cd3/"
"SimLex-999: Evaluating Semantic Models With (Genuine) Similarity Estimation","SimLex-999 improves semantic model evaluation by quantifying similarity, enabling a wider range of applications and guiding the development of next-generation representation-learning architectures.","Felix Hill, Roi Reichart, A. Korhonen",2014,1230,"We present SimLex-999, a gold standard resource for evaluating distributional semantic models that improves on existing resources in several important ways. First, in contrast to gold standards such as WordSim-353 and MEN, it explicitly quantifies similarity rather than association or relatedness so that pairs of entities that are associated but not actually similar (Freud, psychology) have a low rating. We show that, via this focus on similarity, SimLex-999 incentivizes the development of models with a different, and arguably wider, range of applications than those which reflect conceptual association. Second, SimLex-999 contains a range of concrete and abstract adjective, noun, and verb pairs, together with an independent rating of concreteness and (free) association strength for each pair. This diversity enables fine-grained analyses of the performance of models on concepts of different types, and consequently greater insight into how architectures can be improved. Further, unlike existing gold standard evaluations, for which automatic approaches have reached or surpassed the inter-annotator agreement ceiling, state-of-the-art models perform well below this ceiling on SimLex-999. There is therefore plenty of scope for SimLex-999 to quantify future improvements to distributional semantic models, guiding the development of the next generation of representation-learning architectures.","","Computational Linguistics","1","10.1162/COLI_a_00237","https://consensus.app/papers/simlex999-evaluating-semantic-models-with-genuine-hill/6d748770bf39500aa88f001e584e1caa/"
"Data set quality in Machine Learning: Consistency measure based on Group Decision Making","The proposed consistency measure based on Group Decision Making strongly correlates with the performance of deep neural networks implementing Learning to Rank methods.","G. Fenza, Mariacristina Gallo, V. Loia, F. Orciuoli, E. Herrera-Viedma",2021,19,"Abstract Performance of Machine Learning models heavily depends on the quality of the training dataset. Among others, the quality of training data relies on the consistency of the labels assigned to similar items. Indeed, the labels should be coherently assigned (or collected) by avoiding inconsistencies for increasing the performance of the machine learning model. This study focuses on evaluating training data consistency for machine learning algorithms dealing with ranking problems, i.e., the Learning to Rank methods (LTR). This work defines a training data consistency measure based on the consensus value introduced in Group Decision Making. It investigates the statistical relationship between the proposed consistency measure and the performance of a deep neural network implementing an LTR method. This measure could drive data filtering at the training stage and guide model update decisions. Experimentation reveals a strong correlation between the proposed consistency measure and the performance of the model.","","Appl. Soft Comput.","1","10.1016/J.ASOC.2021.107366","https://consensus.app/papers/data-quality-machine-learning-consistency-measure-based-fenza/1913f75ae2f45860aa37697600f8b085/"
"Investigating the impact of data normalization on classification performance","Data normalization methods improve machine learning performance, with z-Score and Pareto Scaling being the best performers for feature selection and weighting, and Mean Centered, Variable Stability Scaling, and Median and Median Absolute Deviation being the worst performers.","Dalwinder Singh, Birmohan Singh",2020,543,"Abstract Data normalization is one of the pre-processing approaches where the data is either scaled or transformed to make an equal contribution of each feature. The success of machine learning algorithms depends upon the quality of the data to obtain a generalized predictive model of the classification problem. The importance of data normalization for improving data quality and subsequently the performance of machine learning algorithms has been presented in many studies. But, the work lacks for the feature selection and feature weighting approaches, a current research trend in machine learning for improving performance. Therefore, this study aims to investigate the impact of fourteen data normalization methods on classification performance considering full feature set, feature selection, and feature weighting. In this paper, we also present a modified Ant Lion optimization that search feature subsets and the best feature weights along with the parameter of Nearest Neighbor Classifier. Experiments are performed on 21 publicly available real and synthetic datasets, and results are analyzed based on the accuracy, the percentage of feature reduced and runtime. It has been observed from the results that no single method outperforms others. Therefore, we have suggested a set of the best and the worst methods combining the normalization procedure and empirical analysis of results. The better performers are z -Score and Pareto Scaling for the full feature set and feature selection, and tanh and its variant for feature weighting. The worst performers are Mean Centered, Variable Stability Scaling and Median and Median Absolute Deviation methods along with un-normalized data.","","Appl. Soft Comput.","1","10.1016/J.ASOC.2019.105524","https://consensus.app/papers/investigating-impact-data-normalization-classification-singh/c1c2facc86a65590aca30197f0c06887/"
"Measuring semantic similarity of clinical trial outcomes using deep pre-trained language representations","Deep learning approaches using pre-trained language representations outperform other methods for assessing semantic similarity of clinical trial outcomes, without relying on domain-specific resources.","A. Koroleva, S. Kamath, P. Paroubek",2019,12,"BACKGROUND
Outcomes are variables monitored during a clinical trial to assess the impact of an intervention on humans' health.Automatic assessment of semantic similarity of trial outcomes is required for a number of tasks, such as detection of outcome switching (unjustified changes of pre-defined outcomes of a trial) and implementation of Core Outcome Sets (minimal sets of outcomes that should be reported in a particular medical domain).


OBJECTIVE
We aimed at building an algorithm for assessing semantic similarity of pairs of primary and reported outcomes.We focused on approaches that do not require manually curated domain-specific resources such as ontologies and thesauri.


METHODS
We tested several approaches, including single measures of similarity (based on strings, stems and lemmas, paths and distances in an ontology, and vector representations of phrases), classifiers using a combination of single measures as features, and a deep learning approach that consists in fine-tuning pre-trained deep language representations.We tested language models provided by BERT (trained on general-domain texts), BioBERT and SciBERT (trained on biomedical and scientific texts, respectively).We explored the possibility of improving the results by taking into account the variants for referring to an outcome (e.g.the use of a measurement tool name instead on the outcome name; the use of abbreviations).We release an open corpus with annotation for similarity of pairs of outcomes.


RESULTS
Classifiers using a combination of single measures as features outperformed the single measures, while deep learning algorithms using BioBERT and SciBERT models outperformed the classifiers.BioBERT reached the best F-measure of 89.75%.The addition of variants of outcomes did not improve the results for the best-performing single measures nor for the classifiers, but it improved the performance of deep learning algorithms: BioBERT achieved an F-measure of93.38%.


CONCLUSIONS
Deep learning approaches using pre-trained language representations outperformed other approaches for similarity assessment of trial outcomes, without relying on any manually curated domain-specific resources (ontologies and other lexical resources). Addition of variants of outcomes further improved the performance of deep learning algorithms.","","Journal of biomedical informatics","1","10.1016/j.yjbinx.2019.100058","https://consensus.app/papers/measuring-similarity-trial-outcomes-using-language-koroleva/feaed175597951fa86a9147c7de29a1d/"
"Data Augmentation by Guided Deep Interpolation","Guided Deep Interpolation (GDI) effectively augments low sample size and imbalanced data sets, improving machine learning models' predictive performance.","Gergely Szlobodnyik, L. Farkas",2021,9,"Abstract State-of-the-art machine learning algorithms require large amount of high quality data. In practice, however, the sample size is commonly low and data is imbalanced along different class labels. Low sample size and imbalanced class distribution can significantly deteriorate the predictive performance of machine learning models. In order to overcome data quality issues, we propose a novel data augmentation method, Guided Deep Interpolation (GDI). It is based on a convolutional auto-encoder network, which is equipped with an auxiliary linear self-expressive layer. The network is trained by minimizing a composite objective function so that to extract the underlying clustered structure of semantic similarities of data points while high reconstruction quality is also preserved. The trained network is used to define a sampling strategy and a synthetic data generation procedure. Making use of the weights of the self-expressive layer, we introduce a measure of semantic variability to quantify how similar a data point to other data points on average. Based on the proposed measure of semantic variability, a joint distribution is defined. Using the distribution we can draw pairs of similar data points so that one point is semantically underrepresented (isolated) while its pair possesses relatively high semantic variability. A sampled pair is interpolated in the deep feature space of the network so that to increase semantic variability while preserve class label of the semantically underrepresented data point. The trained decoder is used to determine pixel space representations of latent space interpolations. The resulting data augmentation procedure generates synthetic samples by increasing the semantic variability of semantically underrepresented instances in a class label preserving way. Our experimental results show that the proposed method outperforms traditional and generative model-based data augmentation methods on low sample size and imbalanced data sets.","","Appl. Soft Comput.","1","10.1016/J.ASOC.2021.107680","https://consensus.app/papers/data-augmentation-guided-deep-interpolation-szlobodnyik/9c5b08fe662a52f3a764fe76a94c29ea/"
"Measuring the Effects of Data Parallelism on Neural Network Training","Increasing batch size in neural network training does not degrade out-of-sample performance, as batch size varies greatly between workloads and training algorithms.","Christopher J. Shallue, Jaehoon Lee, J. Antognini, Jascha Narain Sohl-Dickstein, Roy Frostig, George E. Dahl",2018,337,"Recent hardware developments have dramatically increased the scale of data parallelism available for neural network training. Among the simplest ways to harness next-generation hardware is to increase the batch size in standard mini-batch neural network training algorithms. In this work, we aim to experimentally characterize the effects of increasing the batch size on training time, as measured by the number of steps necessary to reach a goal out-of-sample error. We study how this relationship varies with the training algorithm, model, and data set, and find extremely large variation between workloads. Along the way, we show that disagreements in the literature on how batch size affects model quality can largely be explained by differences in metaparameter tuning and compute budgets at different batch sizes. We find no evidence that larger batch sizes degrade out-of-sample performance. Finally, we discuss the implications of our results on efforts to train neural networks much faster in the future. Our experimental data is publicly available as a database of 71,638,836 loss measurements taken over the course of training for 168,160 individual models across 35 workloads.","","ArXiv","1","","https://consensus.app/papers/measuring-effects-data-parallelism-neural-network-shallue/91fea60e448c5c79bae4c14b394f7bd8/"
