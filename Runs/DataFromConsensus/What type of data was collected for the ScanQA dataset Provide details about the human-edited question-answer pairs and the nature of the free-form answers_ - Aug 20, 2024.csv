Title,Takeaway,Authors,Year,Citations,Abstract,Study Type,Journal,Journal SJR Quartile,DOI,Consensus Link
"ScanQA: 3D Question Answering for Spatial Scene Understanding","ScanQA model effectively performs object-grounded question answering in 3D environments, enabling accurate object localization and alignment in indoor scenes from textual questions.","Daich Azuma, Taiki Miyanishi, Shuhei Kurita, M. Kawanabe",2021,53,"We propose a new 3D spatial understanding task for 3D question answering (3D-QA). In the 3D-QA task, models receive visual information from the entire 3D scene of a rich RGB-D indoor scan and answer given textual questions about the 3D scene. Unlike the 2D-question answering of visual question answering, the conventional 2D-QA models suffer from problems with spatial understanding of object alignment and directions and fail in object localization from the textual questions in 3D-QA. We propose a baseline model for 3D-QA, called the ScanQA11https://github.com/ATR-DBI/ScanQA, which learns a fused descriptor from 3D object proposals and encoded sentence embeddings. This learned descriptor correlates language expressions with the underlying geometric features of the 3D scan and facilitates the regression of 3D bounding boxes to determine the described objects in textual questions. We collected human-edited question-answer pairs with free-form answers grounded in 3D objects in each 3D scene. Our new ScanQA dataset contains over 41k question-answer pairs from 800 indoor scenes obtained from the ScanNet dataset. To the best of our knowledge, ScanQA is the first large-scale effort to perform object-grounded question answering in 3D environments.","","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","","10.1109/CVPR52688.2022.01854","https://consensus.app/papers/scanqa-question-answering-spatial-scene-understanding-azuma/555f825f7ee55af8be7efac6bd4c5b50/"
"SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine","SearchQA is a large-scale dataset for machine comprehension, augmented with context from a search engine, showing a meaningful gap between human and machine performance.","Matthew Dunn, Levent Sagun, Mike Higgins, V. U. Güney, Volkan Cirik, Kyunghyun Cho",2017,392,"We publicly release a new large-scale dataset, called SearchQA, for machine comprehension, or question-answering. Unlike recently released datasets, such as DeepMind CNN/DailyMail and SQuAD, the proposed SearchQA was constructed to reflect a full pipeline of general question-answering. That is, we start not from an existing article and generate a question-answer pair, but start from an existing question-answer pair, crawled from J! Archive, and augment it with text snippets retrieved by Google. Following this approach, we built SearchQA, which consists of more than 140k question-answer pairs with each pair having 49.6 snippets on average. Each question-answer-context tuple of the SearchQA comes with additional meta-data such as the snippet's URL, which we believe will be valuable resources for future research. We conduct human evaluation as well as test two baseline methods, one simple word selection and the other deep learning based, on the SearchQA. We show that there is a meaningful gap between the human and machine performances. This suggests that the proposed dataset could well serve as a benchmark for question-answering.","","ArXiv","","","https://consensus.app/papers/searchqa-dataset-augmented-context-search-engine-dunn/a69a1f272a645a20afdbfff44f9b69fd/"
"FeTaQA: Free-form Table Question Answering","FeTaQA dataset challenges both semantic parsing-based QA systems and large pretrained text generation models in understanding, integrating, and generating advanced table-based answers from Wikipedia tables.","Linyong Nan, Chia-Hsuan Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang, Wojciech Kryscinski, Nick Schoelkopf, Riley Kong, Xiangru Tang, Murori Mutuma, Benjamin Rosand, Isabel Trindade, Renusree Bandaru, Jacob Cunningham, Caiming Xiong, Dragomir R. Radev",2021,59,"Existing table question answering datasets contain abundant factual questions that primarily evaluate a QA system’s comprehension of query and tabular data. However, restricted by their short-form answers, these datasets fail to include question–answer interactions that represent more advanced and naturally occurring information needs: questions that ask for reasoning and integration of information pieces retrieved from a structured knowledge source. To complement the existing datasets and to reveal the challenging nature of the table-based question answering task, we introduce FeTaQA, a new dataset with 10K Wikipedia-based table, question, free-form answer, supporting table cells pairs. FeTaQA is collected from noteworthy descriptions of Wikipedia tables that contain information people tend to seek; generation of these descriptions requires advanced processing that humans perform on a daily basis: Understand the question and table, retrieve, integrate, infer, and conduct text planning and surface realization to generate an answer. We provide two benchmark methods for the proposed task: a pipeline method based on semantic parsing-based QA systems and an end-to-end method based on large pretrained text generation models, and show that FeTaQA poses a challenge for both methods.","","Transactions of the Association for Computational Linguistics","1","10.1162/tacl_a_00446","https://consensus.app/papers/fetaqa-freeform-table-question-answering-nan/53719bcee72b52eeaa7db746d85880f7/"
"CoQA: A Conversational Question Answering Challenge","The best system on CoQA achieves 65.4% F1 score, 23.4 points behind human performance, indicating ample room for improvement in conversational question answering systems.","Siva Reddy, Danqi Chen, Christopher D. Manning",2018,898,"Humans gather information through conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets (e.g., coreference and pragmatic reasoning). We evaluate strong dialogue and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4%, which is 23.4 points behind human performance (88.8%), indicating that there is ample room for improvement. We present CoQA as a challenge to the community at https://stanfordnlp.github.io/coqa.","","Transactions of the Association for Computational Linguistics","1","10.1162/tacl_a_00266","https://consensus.app/papers/coqa-conversational-question-answering-challenge-reddy/256b363d94c8558a873ce4ec3f065069/"
"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering","Our balanced VQA dataset shows that state-of-the-art models perform significantly worse on it, suggesting they exploit language priors, and our novel interpretable model helps build trust among users.","Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh",2016,2006,"The problem of visual question answering (VQA) is of significant importance both as a challenging research question and for the rich set of applications it enables. In this context, however, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in VQA models that ignore visual information, leading to an inflated sense of their capability. We propose to counter these language priors for the task of VQA and make vision (the V in VQA) matter! Specifically, we balance the popular VQA dataset (Antol et al., in: ICCV, 2015) by collecting complementary images such that every question in our balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. Our dataset is by construction more balanced than the original VQA dataset and has approximately twice the number of image-question pairs. Our complete balanced dataset is publicly available at http://visualqa.org/ as part of the 2nd iteration of the VQA Dataset and Challenge (VQA v2.0). We further benchmark a number of state-of-art VQA models on our balanced dataset. All models perform significantly worse on our balanced dataset, suggesting that these models have indeed learned to exploit language priors. This finding provides the first concrete empirical evidence for what seems to be a qualitative sense among practitioners. We also present interesting insights from analysis of the participant entries in VQA Challenge 2017, organized by us on the proposed VQA v2.0 dataset. The results of the challenge were announced in the 2nd VQA Challenge Workshop at the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017. Finally, our data collection protocol for identifying complementary images enables us to develop a novel interpretable model, which in addition to providing an answer to the given (image, question) pair, also provides a counter-example based explanation. Specifically, it identifies an image that is similar to the original image, but it believes has a different answer to the same question. This can help in building trust for machines among their users.","","International Journal of Computer Vision","1","10.1007/s11263-018-1116-0","https://consensus.app/papers/making-matter-elevating-role-image-understanding-visual-goyal/bc57b53798e1513fbae379a2ebcd5a8c/"
"VQA: Visual Question Answering","Visual Question Answering (VQA) requires detailed understanding of images and complex reasoning, making it suitable for real-world scenarios like helping the visually impaired.","Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. L. Zitnick, Devi Parikh, Dhruv Batra",2015,4244,"We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing $$\sim $$∼0.25 M images, $$\sim $$∼0.76 M questions, and $$\sim $$∼10 M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines and methods for VQA are provided and compared with human performance. Our VQA demo is available on CloudCV (http://cloudcv.org/vqa).","","International Journal of Computer Vision","1","10.1007/s11263-016-0966-6","https://consensus.app/papers/question-answering-agrawal/162adb4f84185299818ee2e2c6b9eba8/"
"Discovering high quality answers in community question answering archives using a hierarchy of classifiers","Our hybrid hierarchy-of-classifiers framework effectively identifies high quality answers in community question answering services, while also accurately classifying low quality answers compared to single classifier approaches.","Hapnes Toba, Zhaoyan Ming, M. Adriani, Tat-Seng Chua",2014,114,"In community-based question answering (CQA) services where answers are generated by human, users may expect better answers than an automatic question answering system. However, in some cases, the user generated answers provided by CQA archives are not always of high quality. Most existing works on answer quality prediction use the same model for all answers, despite the fact that each answer is intrinsically different. However, modeling each individual QA pair differently is not feasible in practice. To balance between efficiency and accuracy, we propose a hybrid hierarchy-of-classifiers framework to model the QA pairs. First, we analyze the question type to guide the selection of the right answer quality model. Second, we use the information from question analysis to predict the expected answer features and train the type-based quality classifiers to hierarchically aggregate an overall answer quality score. We also propose a number of novel features that are effective in distinguishing the quality of answers. We tested the framework on a dataset of about 50 thousand QA pairs from Yahoo! Answer. The results show that our proposed framework is effective in identifying high quality answers. Moreover, further analysis reveals the ability of our framework to classify low quality answers more accurately than a single classifier approach.","","Inf. Sci.","1","10.1016/j.ins.2013.10.030","https://consensus.app/papers/discovering-quality-answers-community-question-toba/67e5485cd750548ab2e13e1f0ca48e9b/"
"Visual Question Answering","Visual Question Answering (VQA) requires a detailed understanding of an image and complex reasoning, making it suitable for real-world scenarios like helping the visually impaired.","A. Jabri, Armand Joulin, L. Maaten",2022,234,"We propose the task of free-form and open- ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open- ended answers contain only a few words or a closed set of answers that can be provided in a multiple choice format. We provide a dataset containing ~0.25M images, ~0.76M questions, and ~10M answers and discuss the information it provides. Numerous baseline for VQA are provided and compared with human performance. In this model, we have exclusively introduced a feature of voice to text using Speech recognition, Google Text-to-Speech and pygame module.","","International Journal of Advanced Research in Science, Communication and Technology","","10.48175/ijarsct-5763","https://consensus.app/papers/question-answering-jabri/9277897224885b98bdb8a788007ce9ad/"
"CodeQA: A Question Answering Dataset for Source Code Comprehension","CodeQA is a free-form question answering dataset for source code comprehension, providing a useful research benchmark for machine reading comprehension and question-answering.","Chenxiao Liu, Xiaojun Wan",2021,12,"We propose CodeQA, a free-form question answering dataset for the purpose of source code comprehension: given a code snippet and a question, a textual answer is required to be generated. CodeQA contains a Java dataset with 119,778 question-answer pairs and a Python dataset with 70,085 question-answer pairs. To obtain natural and faithful questions and answers, we implement syntactic rules and semantic analysis to transform code comments into question-answer pairs. We present the construction process and conduct systematic analysis of our dataset. Experiment results achieved by several neural baselines on our dataset are shown and discussed. While research on question-answering and machine reading comprehension develops rapidly, few prior work has drawn attention to code question answering. This new dataset can serve as a useful research benchmark for source code comprehension.","","ArXiv","","10.18653/v1/2021.findings-emnlp.223","https://consensus.app/papers/codeqa-question-answering-dataset-source-code-liu/8ee280197d005691be81f51f6a1ed0bd/"
"FreebaseQA: A New Factoid QA Data Set Matching Trivia-Style Question-Answer Pairs with Freebase","FreebaseQA is a new data set for factoid question answering tasks, providing more linguistically sophisticated questions for model training compared to other existing data sets.","Kelvin Jiang, Dekun Wu, Hui Jiang",2019,56,"In this paper, we present a new data set, named FreebaseQA, for open-domain factoid question answering (QA) tasks over structured knowledge bases, like Freebase. The data set is generated by matching trivia-type question-answer pairs with subject-predicate-object triples in Freebase. For each collected question-answer pair, we first tag all entities in each question and search for relevant predicates that bridge a tagged entity with the answer in Freebase. Finally, human annotation is used to remove any false positive in these matched triples. Using this method, we are able to efficiently generate over 54K matches from about 28K unique questions with minimal cost. Our analysis shows that this data set is suitable for model training in factoid QA tasks beyond simpler questions since FreebaseQA provides more linguistically sophisticated questions than other existing data sets.","","","","10.18653/v1/N19-1028","https://consensus.app/papers/freebaseqa-factoid-data-matching-triviastyle-jiang/a1533cac7b6c592b9ababd2f725b9482/"
